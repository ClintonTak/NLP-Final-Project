\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{enumitem}  %this permits the enumeration items to flow past sections
\newcommand\tab[1][1cm]{\hspace*{#1}}
\usepackage{amssymb}
\graphicspath{ {C:/images/}}
\usepackage{gensymb}
\usepackage{tabularx}



\title{Native Language Identification: Using Recurrent Neural Networks to Determine A Speaker's First Language}
\author{Lauren Becker, Daniel Brett, Isaiah Rawlinson, Clinton Tak}
\date{May 3rd, 2018}
\begin{document}
	\maketitle
\tableofcontents
\newpage
\section{Introduction}
\tab Native Language Identification (NLI) is the process by which language production in a learned language (i.e. a secondary language) is used to identify an individual's native tongue. Typically, this classification task is framed where the set of native languages (L1 groups) are known a priori and classifiers are (usually) based on language-usage patterns that are endemic to specific L1 groups. For our experiment, our group utilized a corpus of essays from the CAES institute which are annotated for POS tags \footnote{See Data section for more specifics on essays.}. Using this data, recurrent neural networks and a tensorflow implementation in python 3.6, our group attempted to create a classifier that could complete this NLI task with maximum accuracy.\\
\tab The tasks of Native Language Identification has a number of applications, especially as it pertains to pedagogy, language transfer and forensic linguistics. By identifying L1-specific features, we can develop better teaching materials and perform author profiling.\footnote{Especially useful when attempting to determine who wrote an anonymous text, etc.} The latter of these two has proven so useful that it has attracted the attention of certain intelligence agencies which wish to harness NLI to learn more about threats and who are responsible for them. In essence, NLI has broad and potentially impactful applications.\\
\tab Our groups has hypothesized that NLI is a classification task that is most accurate when using language POS tags rather than the text itself. We believe that we can create a (fairly) accurate classifier by using a recurrent neural networks that accepts a vector of POS tags as input. In approaching the problem in this manner, he hope to prove that this strategy is more effective than others.
\section{Background: Literature Review}
\subsection{Feature Analysis for Native Language Identification}
\tab With the goal of demonstrating individuals of similar cultural backgrounds undergo overlapping language learning processes, Nisioi (2015)$\cite{Review1}$ conducted multiple experiments with the EF Cambridge Open Language Database. Using 18 million tokens extracted from essays written by individuals from 29 separate countries, Nisioi extracted five distinct features for classification. These included POS n-grams, character n-grams, function words, shell nouns, and positional token frequency. Using an L2-Regularized L2-loss support vector classification machine, a log-entropy weighting scheme was used to construct feature vectors. 10-fold cross-validation experiments were then performed, yielding accuracy values like 99.89\% for character 4-grams, 97.42\% accuracy for positional tokens frequency, 96.22\% accuracy for function words and 93.65\% accuracy for shell nouns. POS tri-grams yielded a lower accuracy of 82.43\%. Regarding questions our group had, we were curious as to why the POS tri-grams yielded the lowest accuracy for the LB\_Ge dataset, but yielded the highest accuracy for the LB\_RuUk dataset. Is this a direct result of the structure of germanic languages versus English and Russian languages? Note that this article was helpful in helping us formulate our experiment because we decided to explore L-2 regularized SVM for our POS vectors and k-fold validation for testing. This was partially due to the success of Niosi's experiment and because we were familiar with these processes from class. 
 
\subsection{Native Language Identification Shared Task}
 \tab In an effort to automatically identify the native languages (L1) of individuals based on their language production in a learned language, previous experiments have relied on classification tasks where the set of L1s is known a priori. Previous shared tasks have used data in the form of either essays or spoken responses, however Malmasi's 2017$\cite{Review2}$ experiment combines both of these inputs to build on the results from previous shared tasks. A large dataset called the TOEFL 11 was used to supply essays, speech transcripts, and audio features for dialect identification as well as a corpus consisting of both written essays and orthographic transcriptions of spoken responses obtained from test takers in the context of a standardized assessment. Prior to the experiment, the highest text-based accuracy was achieved by using features such as n-grams of words, parts-of-speech, and lemmas. The NLI Shared Task 2017 split their tasks into three parts consisting of text-only, audio-only, and fusion phases. A variety of classifier systems including ensembles and meta-classifiers were used and were the most effective in all tasks, with traditional classifiers such as SVMs with lexical syntactic features also incorporated throughout the experiment. Some of the takeaways from this experiment were that multiple classifier systems are very effective, lexical n-grams are the best single feature type, speech transcript features did not perform well, feature weighting schemes are important, and most importantly, combining written and spoken responses yields a higher prediction accuracy than other methods. While the formal academic context used to obtain the test data works within the scope of our project, it would be interesting to note how accuracy is affected by the context of the conversation. In the classroom foreign languages are often taught in a very formal manner, whereas conversational language can differ slightly in grammar and word usage. It would be interesting to see if less rehearsed conversational topics yield a higher accuracy for natural language identification assuming they uncover more grammatical errors than in formal conversation. It is also important to note that the experiment did not use raw audio data, making its inclusion in the speech and fusion tasks an interesting task in future research. This article provided helpful in formulating how we would test the accuracy of our classifier, specifically by using a priori one-hot vectors for each of the speaker's native languages. In addition, it solidified the idea that we should explore an SVM as a potentially classifier as it produced high accuracy.
 \subsection{Native Language Identification: SVMs vs. NNs}
 \tab Somsehkar et. all (2017) set out with the goal of determining whether an SVM or Neural Net is more effective at speakers with 11 different native languages. Using a publicly available dataset from ETS, the authors were able to reference 13,200 oral transcriptions (11,000 used for training, 1100 for dev, and 1100 for test sets) to use with their models. Advanced n-grams (unigrams and bigrams), spacy library (POS tagging), and i-vectors (high dimensional representations of speech) were used as features for both the Linear SVM and NN. The biggest gains for the SVM were reported when going from unigram to bigrams and adding i-vector data. Factoring in POS tags (surprisingly) did not significantly improve performance. Regarding the Neural Nets, the authors originally explored using RNNs, but eventually decided upon GRU cells as they achieved equal performance to LSTM cells, but had much lower training times. Using GLoVe vectors, the authors compiled accuracies for NN models. Overall, the models with the highest accuracies for SVMs and NNs (respectively) were the Stemmed words and i-vectors (0.802) and the GRU with GloVe and i-vecotrs (0.612). As we can see from these models, the linear SVM proved to be far more accurate, and the authors note that this was likely due to the vector embeddings. This article had an effect in how we outlined our experiment because while we were committed to exploring a model that was predicated on neural nets (whether it be RNNs, CNNs, etc.), we also started to do research into linear SVMs. 
 \section{Data}
 \subsection{Data Corpus}
 \tab As mentioned in the introduction, our group utilized a corpus of essays from the CAES institute\footnote{URL: http://www.cervantes.es/default.htm}. These essays were annotated and converted into a set of language POS tags for all individuals in the database. All essays are written in Spanish. 
 \subsection{Metadata and Associated Information}
 \tab Data regarding the number of essays, languages and POS tags are outlined below in \textbf{Figure 1.}
 \begin{center}
 	\begin{tabular}{|c|c|}
 		\hline
 		Number of Respondents & 3878\\
 		\hline
 		Number of Essay Samples & 3878\\
		\hline
		Native Languages & French, Arabic,
		English, Russian\\
		  & Portuguese, Chinese\\
			\hline
		Total Numbrr of POS tags & 682172\\
			\hline
		Average Number of POS tags (per essay) & 175.9\\
			\hline
 	\end{tabular}\\
 	\textbf{Figure 1. Table With Basic Essay Information}
 \end{center}
While the above figure contains basic information about the POS tags contained within each of the essays, our group also set out to learn more about the average number of POS tags for each native language. The results can be found below in \textbf{Figure 2.}
 \begin{center}
 	\begin{tabular}{|c|c|c|}
 		\hline
 		\textbf{Language} & \textbf{Total Responses} & \textbf{Average Number of POS Tags}\\
 		\hline
 		Arabic & 1342 & 148.3\\
 		\hline
 		Chinese & 373 & 169.3\\
 		\hline
 		English & 615 & 204.7\\
 		\hline
 		French & 371 & 189.5\\
 		\hline
 		Russian & 176 & 140.9\\
 		\hline
 	\end{tabular}
 	\textbf{Figure 2. Metadata By Native Language}
 \end{center}

 \subsection{Preprocessing Methodology}
 \tab Most of the pre-processing that our team needed to complete for this experiment required turning POS tags into vectors that our RNN could accept. This was done in three distinct steps:\\
 \\
 1.) All distinct POS tags were scraped from the essays, put into a set, and then written to a file, entitled "CAESTags.txt.\\
 2.) Each essays was then converted into a vector with each POS tag replaced by its index number from the CAESTags file. This was then written to a new file.\\
 3.) One hot vectors were then created for each of the languages. For each of the essays, depending on the native language a  one-hot vector was assigned to that essay (there were six different one hot vectors, one for each native language).\\
 \\
 Note that these vectors were then used as input parameters for our RNN. We will discuss how they were used and associated results in later sections.
 \section{Methods}
 \subsection{Tools, Libraries, and Software}
 Outlined below are all the relevant electronic resources that were used to carry out our experiments. External resources (such as models or classifiers) are outlined in later sections.\\
 \\
 $\bullet$ Hardware: Dell XPS 13(Windows), 2.40 GHz Intel Core i5-6200U\\
$\bullet$ Software: Python 3.5\\
$\bullet$ Libraries: Tensorflow, numpy, sklearn\\
$\bullet$ Development Tools: Git, Slack\\
\\
Using slack as out main channel of communication, our group was able to outline project goals and directives, and informally collaborate on code. Git was used for overall development to push and pull new additions to the NLI Project. Python 3.5 was the language of choice, and specifically the tensorflow, numpy, and sklearn libraries were used to preprocess the information into vectors and create a viable RNN model. All of this was carried out on an XPS 13 machine (windows environment).
\subsection{External Resources}
\tab As mentioned in earlier sections, the main model that we used was a recurrent neural network, abbreviated RNN. 
\subsection{Experiments}
---------------------------------------------------------------------\\
WHEN EXPERIMENTS COMPLETED, PUT HERE. Talk about classifiers as well.\\
---------------------------------------------------------------------\\
\section{Results}
---------------------------------------------------------------------\\
DISCUSS RESULTS HERE WHEN COMPLETE.\\
---------------------------------------------------------------------\\
\section{Conclusions \& Future Work}
---------------------------------------------------------------------\\
DISCUSS CONCLUSIONS HERE.\\
---------------------------------------------------------------------\\

\addcontentsline{toc}{section}{References}
\begin{thebibliography}{9}
	\bibitem{Review2}
	Malmasi et. al. 2017. A Report on the 2017 Native Language Identiﬁcation Shared Task. Association for Computational Linguistics, pg. 62-66.
	
	\bibitem{Review1}
	Sergiu Nisioi. 2015. Feature Analysis for Native Language Identiﬁcation. Center for Computation Linguistics, pg. 646-653.
	
	\bibitem{Review3}
	Somshekar et. al. 2017. Native Language Identification. Stanford University Symbolic Systems, pg. 1-12.
\end{thebibliography}


\end{document}
