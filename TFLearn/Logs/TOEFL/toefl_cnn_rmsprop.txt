Training Step: 98  | total loss: 2.39847 | time: 12.384s
| RMSProp | epoch: 001 | loss: 2.39847 - acc: 0.0827 | val_loss: 2.39765 - val_acc: 0.1098 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 196  | total loss: 2.39741 | time: 11.377s
| RMSProp | epoch: 002 | loss: 2.39741 - acc: 0.0906 | val_loss: 2.39613 - val_acc: 0.0989 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 294  | total loss: 2.38487 | time: 11.354s
| RMSProp | epoch: 003 | loss: 2.38487 - acc: 0.1177 | val_loss: 2.38438 - val_acc: 0.1014 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 392  | total loss: 2.34578 | time: 11.327s
| RMSProp | epoch: 004 | loss: 2.34578 - acc: 0.1453 | val_loss: 2.33478 - val_acc: 0.1811 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 490  | total loss: 2.29005 | time: 11.332s
| RMSProp | epoch: 005 | loss: 2.29005 - acc: 0.1653 | val_loss: 2.27456 - val_acc: 0.1928 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 588  | total loss: 2.23410 | time: 11.342s
| RMSProp | epoch: 006 | loss: 2.23410 - acc: 0.1859 | val_loss: 2.21372 - val_acc: 0.2179 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 686  | total loss: 2.17209 | time: 11.344s
| RMSProp | epoch: 007 | loss: 2.17209 - acc: 0.2275 | val_loss: 2.15481 - val_acc: 0.2230 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 784  | total loss: 2.14533 | time: 11.350s
| RMSProp | epoch: 008 | loss: 2.14533 - acc: 0.2261 | val_loss: 2.11681 - val_acc: 0.2397 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 882  | total loss: 2.12559 | time: 11.319s
| RMSProp | epoch: 009 | loss: 2.12559 - acc: 0.2219 | val_loss: 2.08780 - val_acc: 0.2649 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 980  | total loss: 2.08797 | time: 11.349s
| RMSProp | epoch: 010 | loss: 2.08797 - acc: 0.2388 | val_loss: 2.05159 - val_acc: 0.2833 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1078  | total loss: 2.09181 | time: 11.329s
| RMSProp | epoch: 011 | loss: 2.09181 - acc: 0.2298 | val_loss: 2.02524 - val_acc: 0.2976 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1176  | total loss: 2.06865 | time: 11.344s
| RMSProp | epoch: 012 | loss: 2.06865 - acc: 0.2510 | val_loss: 2.00934 - val_acc: 0.2749 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1274  | total loss: 2.06231 | time: 11.382s
| RMSProp | epoch: 013 | loss: 2.06231 - acc: 0.2603 | val_loss: 1.99782 - val_acc: 0.2992 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1372  | total loss: 2.08076 | time: 11.299s
| RMSProp | epoch: 014 | loss: 2.08076 - acc: 0.2496 | val_loss: 1.99472 - val_acc: 0.3269 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1470  | total loss: 2.04966 | time: 11.320s
| RMSProp | epoch: 015 | loss: 2.04966 - acc: 0.2714 | val_loss: 1.97605 - val_acc: 0.3244 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1568  | total loss: 2.04934 | time: 11.367s
| RMSProp | epoch: 016 | loss: 2.04934 - acc: 0.2639 | val_loss: 1.98640 - val_acc: 0.3110 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1666  | total loss: 2.06145 | time: 11.302s
| RMSProp | epoch: 017 | loss: 2.06145 - acc: 0.2679 | val_loss: 1.95964 - val_acc: 0.3319 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1764  | total loss: 2.03844 | time: 11.307s
| RMSProp | epoch: 018 | loss: 2.03844 - acc: 0.2698 | val_loss: 1.95167 - val_acc: 0.3361 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1862  | total loss: 2.01141 | time: 11.333s
| RMSProp | epoch: 019 | loss: 2.01141 - acc: 0.2866 | val_loss: 1.94985 - val_acc: 0.3185 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1960  | total loss: 2.05132 | time: 11.328s
| RMSProp | epoch: 020 | loss: 2.05132 - acc: 0.2635 | val_loss: 1.94974 - val_acc: 0.3168 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2058  | total loss: 2.01485 | time: 11.344s
| RMSProp | epoch: 021 | loss: 2.01485 - acc: 0.2917 | val_loss: 1.93012 - val_acc: 0.3328 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2156  | total loss: 2.04034 | time: 11.351s
| RMSProp | epoch: 022 | loss: 2.04034 - acc: 0.2680 | val_loss: 1.93000 - val_acc: 0.3319 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2254  | total loss: 2.06128 | time: 11.344s
| RMSProp | epoch: 023 | loss: 2.06128 - acc: 0.2492 | val_loss: 1.92452 - val_acc: 0.3328 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2352  | total loss: 2.03382 | time: 11.351s
| RMSProp | epoch: 024 | loss: 2.03382 - acc: 0.2782 | val_loss: 1.90661 - val_acc: 0.3453 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2450  | total loss: 2.01759 | time: 11.333s
| RMSProp | epoch: 025 | loss: 2.01759 - acc: 0.2836 | val_loss: 1.93139 - val_acc: 0.3194 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2548  | total loss: 2.04560 | time: 11.348s
| RMSProp | epoch: 026 | loss: 2.04560 - acc: 0.2735 | val_loss: 1.90542 - val_acc: 0.3328 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2646  | total loss: 1.96342 | time: 11.335s
| RMSProp | epoch: 027 | loss: 1.96342 - acc: 0.2917 | val_loss: 1.88742 - val_acc: 0.3571 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2744  | total loss: 2.05338 | time: 11.337s
| RMSProp | epoch: 028 | loss: 2.05338 - acc: 0.2812 | val_loss: 1.90876 - val_acc: 0.3269 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2842  | total loss: 2.06222 | time: 11.336s
| RMSProp | epoch: 029 | loss: 2.06222 - acc: 0.2730 | val_loss: 1.91260 - val_acc: 0.3562 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2940  | total loss: 2.02786 | time: 11.335s
| RMSProp | epoch: 030 | loss: 2.02786 - acc: 0.2969 | val_loss: 1.91344 - val_acc: 0.3420 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3038  | total loss: 2.06012 | time: 11.321s
| RMSProp | epoch: 031 | loss: 2.06012 - acc: 0.2782 | val_loss: 1.90272 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3136  | total loss: 2.05774 | time: 11.343s
| RMSProp | epoch: 032 | loss: 2.05774 - acc: 0.2704 | val_loss: 1.91075 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3234  | total loss: 1.94263 | time: 11.338s
| RMSProp | epoch: 033 | loss: 1.94263 - acc: 0.3236 | val_loss: 1.91792 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3332  | total loss: 1.95958 | time: 11.353s
| RMSProp | epoch: 034 | loss: 1.95958 - acc: 0.2939 | val_loss: 1.86075 - val_acc: 0.3738 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3430  | total loss: 1.97997 | time: 11.338s
| RMSProp | epoch: 035 | loss: 1.97997 - acc: 0.3010 | val_loss: 1.86259 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3528  | total loss: 1.94710 | time: 11.340s
| RMSProp | epoch: 036 | loss: 1.94710 - acc: 0.3069 | val_loss: 1.86090 - val_acc: 0.3730 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3626  | total loss: 1.99508 | time: 11.342s
| RMSProp | epoch: 037 | loss: 1.99508 - acc: 0.2928 | val_loss: 1.86795 - val_acc: 0.3621 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3724  | total loss: 1.93830 | time: 11.301s
| RMSProp | epoch: 038 | loss: 1.93830 - acc: 0.2965 | val_loss: 1.85640 - val_acc: 0.3747 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3822  | total loss: 1.93789 | time: 11.336s
| RMSProp | epoch: 039 | loss: 1.93789 - acc: 0.2916 | val_loss: 1.83609 - val_acc: 0.3755 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3920  | total loss: 1.95682 | time: 11.320s
| RMSProp | epoch: 040 | loss: 1.95682 - acc: 0.2920 | val_loss: 1.84068 - val_acc: 0.3663 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4018  | total loss: 1.96937 | time: 11.319s
| RMSProp | epoch: 041 | loss: 1.96937 - acc: 0.2799 | val_loss: 1.85655 - val_acc: 0.3814 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4116  | total loss: 1.95954 | time: 11.321s
| RMSProp | epoch: 042 | loss: 1.95954 - acc: 0.2923 | val_loss: 1.85375 - val_acc: 0.3588 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4214  | total loss: 1.96626 | time: 11.322s
| RMSProp | epoch: 043 | loss: 1.96626 - acc: 0.2919 | val_loss: 1.84938 - val_acc: 0.3705 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4312  | total loss: 1.95947 | time: 11.311s
| RMSProp | epoch: 044 | loss: 1.95947 - acc: 0.2850 | val_loss: 1.86506 - val_acc: 0.3613 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4410  | total loss: 1.96845 | time: 11.347s
| RMSProp | epoch: 045 | loss: 1.96845 - acc: 0.2918 | val_loss: 1.85914 - val_acc: 0.3722 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4508  | total loss: 1.93471 | time: 11.327s
| RMSProp | epoch: 046 | loss: 1.93471 - acc: 0.3119 | val_loss: 1.84346 - val_acc: 0.3881 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4606  | total loss: 1.97127 | time: 11.334s
| RMSProp | epoch: 047 | loss: 1.97127 - acc: 0.3014 | val_loss: 1.85247 - val_acc: 0.3646 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4704  | total loss: 1.97143 | time: 11.344s
| RMSProp | epoch: 048 | loss: 1.97143 - acc: 0.2973 | val_loss: 1.84985 - val_acc: 0.3579 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4802  | total loss: 1.99704 | time: 11.338s
| RMSProp | epoch: 049 | loss: 1.99704 - acc: 0.2902 | val_loss: 1.83857 - val_acc: 0.3772 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4900  | total loss: 1.90801 | time: 11.362s
| RMSProp | epoch: 050 | loss: 1.90801 - acc: 0.3202 | val_loss: 1.84521 - val_acc: 0.3697 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4998  | total loss: 1.95858 | time: 11.317s
| RMSProp | epoch: 051 | loss: 1.95858 - acc: 0.3030 | val_loss: 1.83900 - val_acc: 0.3814 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5096  | total loss: 1.93022 | time: 11.306s
| RMSProp | epoch: 052 | loss: 1.93022 - acc: 0.3141 | val_loss: 1.82759 - val_acc: 0.3831 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5194  | total loss: 1.93930 | time: 11.324s
| RMSProp | epoch: 053 | loss: 1.93930 - acc: 0.2979 | val_loss: 1.81720 - val_acc: 0.3839 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5292  | total loss: 1.94195 | time: 11.651s
| RMSProp | epoch: 054 | loss: 1.94195 - acc: 0.3087 | val_loss: 1.81993 - val_acc: 0.3764 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5390  | total loss: 1.92884 | time: 11.487s
| RMSProp | epoch: 055 | loss: 1.92884 - acc: 0.3072 | val_loss: 1.84135 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5488  | total loss: 1.94559 | time: 11.727s
| RMSProp | epoch: 056 | loss: 1.94559 - acc: 0.2992 | val_loss: 1.83347 - val_acc: 0.3814 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5586  | total loss: 1.92038 | time: 11.330s
| RMSProp | epoch: 057 | loss: 1.92038 - acc: 0.3070 | val_loss: 1.87902 - val_acc: 0.3412 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5684  | total loss: 1.91794 | time: 11.318s
| RMSProp | epoch: 058 | loss: 1.91794 - acc: 0.2952 | val_loss: 1.84827 - val_acc: 0.3604 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5782  | total loss: 1.91958 | time: 11.318s
| RMSProp | epoch: 059 | loss: 1.91958 - acc: 0.3070 | val_loss: 1.81616 - val_acc: 0.3722 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5880  | total loss: 1.97133 | time: 11.310s
| RMSProp | epoch: 060 | loss: 1.97133 - acc: 0.3005 | val_loss: 1.81728 - val_acc: 0.3839 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5978  | total loss: 1.92497 | time: 11.332s
| RMSProp | epoch: 061 | loss: 1.92497 - acc: 0.3088 | val_loss: 1.81331 - val_acc: 0.3663 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6076  | total loss: 1.92794 | time: 11.313s
| RMSProp | epoch: 062 | loss: 1.92794 - acc: 0.2997 | val_loss: 1.85920 - val_acc: 0.3462 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6174  | total loss: 1.94483 | time: 11.304s
| RMSProp | epoch: 063 | loss: 1.94483 - acc: 0.2932 | val_loss: 1.82703 - val_acc: 0.3705 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6272  | total loss: 1.93791 | time: 11.310s
| RMSProp | epoch: 064 | loss: 1.93791 - acc: 0.3097 | val_loss: 1.82330 - val_acc: 0.3730 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6370  | total loss: 1.92402 | time: 11.312s
| RMSProp | epoch: 065 | loss: 1.92402 - acc: 0.3107 | val_loss: 1.84199 - val_acc: 0.3814 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6468  | total loss: 1.91225 | time: 11.313s
| RMSProp | epoch: 066 | loss: 1.91225 - acc: 0.3296 | val_loss: 1.81278 - val_acc: 0.3889 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6566  | total loss: 1.91310 | time: 11.300s
| RMSProp | epoch: 067 | loss: 1.91310 - acc: 0.3080 | val_loss: 1.81762 - val_acc: 0.3898 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6664  | total loss: 1.91234 | time: 11.326s
| RMSProp | epoch: 068 | loss: 1.91234 - acc: 0.3173 | val_loss: 1.88126 - val_acc: 0.3353 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6762  | total loss: 1.91043 | time: 11.305s
| RMSProp | epoch: 069 | loss: 1.91043 - acc: 0.3153 | val_loss: 1.83063 - val_acc: 0.3755 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6860  | total loss: 1.91481 | time: 11.295s
| RMSProp | epoch: 070 | loss: 1.91481 - acc: 0.3182 | val_loss: 1.80568 - val_acc: 0.3873 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6958  | total loss: 1.91156 | time: 11.311s
| RMSProp | epoch: 071 | loss: 1.91156 - acc: 0.3174 | val_loss: 1.80588 - val_acc: 0.3806 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7056  | total loss: 1.89818 | time: 11.321s
| RMSProp | epoch: 072 | loss: 1.89818 - acc: 0.3120 | val_loss: 1.81183 - val_acc: 0.3688 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7154  | total loss: 1.93888 | time: 11.323s
| RMSProp | epoch: 073 | loss: 1.93888 - acc: 0.2971 | val_loss: 1.81894 - val_acc: 0.3663 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7252  | total loss: 1.91404 | time: 11.334s
| RMSProp | epoch: 074 | loss: 1.91404 - acc: 0.3113 | val_loss: 1.83016 - val_acc: 0.3923 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7350  | total loss: 1.94091 | time: 11.293s
| RMSProp | epoch: 075 | loss: 1.94091 - acc: 0.3155 | val_loss: 1.80518 - val_acc: 0.3738 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7448  | total loss: 1.93282 | time: 11.357s
| RMSProp | epoch: 076 | loss: 1.93282 - acc: 0.3114 | val_loss: 1.82263 - val_acc: 0.3814 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7546  | total loss: 1.87782 | time: 11.296s
| RMSProp | epoch: 077 | loss: 1.87782 - acc: 0.3242 | val_loss: 1.84899 - val_acc: 0.3655 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7644  | total loss: 1.92990 | time: 11.360s
| RMSProp | epoch: 078 | loss: 1.92990 - acc: 0.3138 | val_loss: 1.83820 - val_acc: 0.3579 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7742  | total loss: 1.90139 | time: 11.302s
| RMSProp | epoch: 079 | loss: 1.90139 - acc: 0.3194 | val_loss: 1.81396 - val_acc: 0.3621 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7840  | total loss: 1.92785 | time: 11.319s
| RMSProp | epoch: 080 | loss: 1.92785 - acc: 0.3157 | val_loss: 1.82182 - val_acc: 0.3646 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7938  | total loss: 1.86922 | time: 11.292s
| RMSProp | epoch: 081 | loss: 1.86922 - acc: 0.3270 | val_loss: 1.79678 - val_acc: 0.3806 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8036  | total loss: 1.86825 | time: 11.344s
| RMSProp | epoch: 082 | loss: 1.86825 - acc: 0.3157 | val_loss: 1.83644 - val_acc: 0.3596 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8134  | total loss: 1.89652 | time: 11.281s
| RMSProp | epoch: 083 | loss: 1.89652 - acc: 0.3250 | val_loss: 1.79840 - val_acc: 0.3831 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8232  | total loss: 1.88191 | time: 11.312s
| RMSProp | epoch: 084 | loss: 1.88191 - acc: 0.3270 | val_loss: 1.83397 - val_acc: 0.3814 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8330  | total loss: 1.90313 | time: 11.317s
| RMSProp | epoch: 085 | loss: 1.90313 - acc: 0.3122 | val_loss: 1.83741 - val_acc: 0.3646 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8428  | total loss: 1.86815 | time: 11.323s
| RMSProp | epoch: 086 | loss: 1.86815 - acc: 0.3264 | val_loss: 1.84522 - val_acc: 0.3638 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8526  | total loss: 1.93206 | time: 11.300s
| RMSProp | epoch: 087 | loss: 1.93206 - acc: 0.2961 | val_loss: 1.82598 - val_acc: 0.3621 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8624  | total loss: 1.89695 | time: 11.331s
| RMSProp | epoch: 088 | loss: 1.89695 - acc: 0.3390 | val_loss: 1.81605 - val_acc: 0.3722 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8722  | total loss: 1.87924 | time: 11.338s
| RMSProp | epoch: 089 | loss: 1.87924 - acc: 0.3368 | val_loss: 1.79593 - val_acc: 0.3906 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8820  | total loss: 1.90832 | time: 11.326s
| RMSProp | epoch: 090 | loss: 1.90832 - acc: 0.3133 | val_loss: 1.84425 - val_acc: 0.3705 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8918  | total loss: 1.90652 | time: 11.325s
| RMSProp | epoch: 091 | loss: 1.90652 - acc: 0.3129 | val_loss: 1.81734 - val_acc: 0.3680 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9016  | total loss: 1.90794 | time: 11.342s
| RMSProp | epoch: 092 | loss: 1.90794 - acc: 0.3205 | val_loss: 1.81281 - val_acc: 0.3814 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9114  | total loss: 1.91075 | time: 11.392s
| RMSProp | epoch: 093 | loss: 1.91075 - acc: 0.3177 | val_loss: 1.82693 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9212  | total loss: 1.89657 | time: 11.333s
| RMSProp | epoch: 094 | loss: 1.89657 - acc: 0.3163 | val_loss: 1.84100 - val_acc: 0.3571 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9310  | total loss: 1.89006 | time: 11.348s
| RMSProp | epoch: 095 | loss: 1.89006 - acc: 0.3163 | val_loss: 1.81041 - val_acc: 0.3738 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9408  | total loss: 1.90204 | time: 11.353s
| RMSProp | epoch: 096 | loss: 1.90204 - acc: 0.3218 | val_loss: 1.79677 - val_acc: 0.3747 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9506  | total loss: 1.89678 | time: 11.357s
| RMSProp | epoch: 097 | loss: 1.89678 - acc: 0.3121 | val_loss: 1.81072 - val_acc: 0.3722 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9604  | total loss: 1.89809 | time: 11.335s
| RMSProp | epoch: 098 | loss: 1.89809 - acc: 0.3193 | val_loss: 1.80499 - val_acc: 0.3839 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9702  | total loss: 1.90784 | time: 11.333s
| RMSProp | epoch: 099 | loss: 1.90784 - acc: 0.3153 | val_loss: 1.83530 - val_acc: 0.3386 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9800  | total loss: 1.89667 | time: 11.348s
| RMSProp | epoch: 100 | loss: 1.89667 - acc: 0.3234 | val_loss: 1.78856 - val_acc: 0.3847 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

