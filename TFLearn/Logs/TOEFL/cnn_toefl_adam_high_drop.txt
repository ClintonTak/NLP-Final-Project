Training Step: 336  | total loss: 2.38657 | time: 13.164s
| Adam | epoch: 001 | loss: 2.38657 - acc: 0.1217 | val_loss: 2.38653 - val_acc: 0.1098 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 672  | total loss: 2.32789 | time: 12.345s
| Adam | epoch: 002 | loss: 2.32789 - acc: 0.1514 | val_loss: 2.29382 - val_acc: 0.2096 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1008  | total loss: 2.27287 | time: 12.337s
| Adam | epoch: 003 | loss: 2.27287 - acc: 0.1699 | val_loss: 2.21732 - val_acc: 0.2129 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1344  | total loss: 2.25297 | time: 12.322s
| Adam | epoch: 004 | loss: 2.25297 - acc: 0.2012 | val_loss: 2.16736 - val_acc: 0.2397 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1680  | total loss: 2.19177 | time: 12.300s
| Adam | epoch: 005 | loss: 2.19177 - acc: 0.1637 | val_loss: 2.12376 - val_acc: 0.2657 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2016  | total loss: 2.19230 | time: 12.353s
| Adam | epoch: 006 | loss: 2.19230 - acc: 0.2186 | val_loss: 2.13317 - val_acc: 0.2490 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2352  | total loss: 2.16314 | time: 12.282s
| Adam | epoch: 007 | loss: 2.16314 - acc: 0.2149 | val_loss: 2.09507 - val_acc: 0.2456 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2688  | total loss: 2.14673 | time: 12.316s
| Adam | epoch: 008 | loss: 2.14673 - acc: 0.2221 | val_loss: 2.08358 - val_acc: 0.2506 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3024  | total loss: 2.13987 | time: 12.345s
| Adam | epoch: 009 | loss: 2.13987 - acc: 0.2226 | val_loss: 2.07753 - val_acc: 0.2867 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3360  | total loss: 2.15887 | time: 12.351s
| Adam | epoch: 010 | loss: 2.15887 - acc: 0.2308 | val_loss: 2.07207 - val_acc: 0.2942 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3696  | total loss: 2.11349 | time: 12.292s
| Adam | epoch: 011 | loss: 2.11349 - acc: 0.2407 | val_loss: 2.06314 - val_acc: 0.2791 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4032  | total loss: 2.10738 | time: 12.336s
| Adam | epoch: 012 | loss: 2.10738 - acc: 0.2292 | val_loss: 2.04198 - val_acc: 0.2967 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4368  | total loss: 2.10585 | time: 12.365s
| Adam | epoch: 013 | loss: 2.10585 - acc: 0.2433 | val_loss: 2.04800 - val_acc: 0.2934 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4704  | total loss: 2.14129 | time: 12.325s
| Adam | epoch: 014 | loss: 2.14129 - acc: 0.2283 | val_loss: 2.05322 - val_acc: 0.2816 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5040  | total loss: 2.10127 | time: 12.305s
| Adam | epoch: 015 | loss: 2.10127 - acc: 0.2406 | val_loss: 2.03789 - val_acc: 0.2925 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5376  | total loss: 2.12524 | time: 12.318s
| Adam | epoch: 016 | loss: 2.12524 - acc: 0.2103 | val_loss: 2.04053 - val_acc: 0.2850 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5712  | total loss: 2.15751 | time: 12.331s
| Adam | epoch: 017 | loss: 2.15751 - acc: 0.2030 | val_loss: 2.04801 - val_acc: 0.2967 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6048  | total loss: 2.12298 | time: 12.323s
| Adam | epoch: 018 | loss: 2.12298 - acc: 0.2431 | val_loss: 2.01638 - val_acc: 0.3177 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6384  | total loss: 2.15882 | time: 12.309s
| Adam | epoch: 019 | loss: 2.15882 - acc: 0.2409 | val_loss: 2.02506 - val_acc: 0.3068 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6720  | total loss: 2.10929 | time: 12.352s
| Adam | epoch: 020 | loss: 2.10929 - acc: 0.2408 | val_loss: 2.02453 - val_acc: 0.3110 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7056  | total loss: 2.13517 | time: 12.280s
| Adam | epoch: 021 | loss: 2.13517 - acc: 0.2615 | val_loss: 2.01453 - val_acc: 0.3185 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7392  | total loss: 2.08347 | time: 12.313s
| Adam | epoch: 022 | loss: 2.08347 - acc: 0.2489 | val_loss: 2.02843 - val_acc: 0.2976 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7728  | total loss: 2.11988 | time: 12.335s
| Adam | epoch: 023 | loss: 2.11988 - acc: 0.2517 | val_loss: 2.02084 - val_acc: 0.3143 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8064  | total loss: 2.05332 | time: 12.335s
| Adam | epoch: 024 | loss: 2.05332 - acc: 0.2795 | val_loss: 1.99862 - val_acc: 0.3210 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8400  | total loss: 2.11791 | time: 12.343s
| Adam | epoch: 025 | loss: 2.11791 - acc: 0.2193 | val_loss: 2.00791 - val_acc: 0.3135 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8736  | total loss: 2.10070 | time: 12.466s
| Adam | epoch: 026 | loss: 2.10070 - acc: 0.2467 | val_loss: 1.99644 - val_acc: 0.3269 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9072  | total loss: 2.08265 | time: 12.332s
| Adam | epoch: 027 | loss: 2.08265 - acc: 0.2519 | val_loss: 1.99837 - val_acc: 0.3143 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9408  | total loss: 2.11108 | time: 12.328s
| Adam | epoch: 028 | loss: 2.11108 - acc: 0.2361 | val_loss: 2.00779 - val_acc: 0.3236 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9744  | total loss: 2.10872 | time: 12.267s
| Adam | epoch: 029 | loss: 2.10872 - acc: 0.2746 | val_loss: 1.98010 - val_acc: 0.3185 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 10080  | total loss: 2.13902 | time: 12.364s
| Adam | epoch: 030 | loss: 2.13902 - acc: 0.2569 | val_loss: 1.99817 - val_acc: 0.3345 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 10416  | total loss: 2.21372 | time: 12.687s
| Adam | epoch: 031 | loss: 2.21372 - acc: 0.2063 | val_loss: 1.99906 - val_acc: 0.3303 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 10752  | total loss: 2.11104 | time: 12.283s
| Adam | epoch: 032 | loss: 2.11104 - acc: 0.2514 | val_loss: 1.97900 - val_acc: 0.3437 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 11088  | total loss: 2.13088 | time: 12.352s
| Adam | epoch: 033 | loss: 2.13088 - acc: 0.2446 | val_loss: 1.97695 - val_acc: 0.3269 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 11424  | total loss: 2.11441 | time: 12.311s
| Adam | epoch: 034 | loss: 2.11441 - acc: 0.2269 | val_loss: 1.99678 - val_acc: 0.3135 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 11760  | total loss: 2.05505 | time: 12.334s
| Adam | epoch: 035 | loss: 2.05505 - acc: 0.2616 | val_loss: 1.97644 - val_acc: 0.3378 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 12096  | total loss: 2.10451 | time: 12.296s
| Adam | epoch: 036 | loss: 2.10451 - acc: 0.2707 | val_loss: 1.98735 - val_acc: 0.3319 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 12432  | total loss: 2.08303 | time: 12.304s
| Adam | epoch: 037 | loss: 2.08303 - acc: 0.2344 | val_loss: 1.98014 - val_acc: 0.3219 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 12768  | total loss: 2.11527 | time: 12.523s
| Adam | epoch: 038 | loss: 2.11527 - acc: 0.2347 | val_loss: 1.97966 - val_acc: 0.3227 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 13104  | total loss: 2.04876 | time: 12.302s
| Adam | epoch: 039 | loss: 2.04876 - acc: 0.2846 | val_loss: 1.95493 - val_acc: 0.3512 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 13440  | total loss: 2.14090 | time: 12.360s
| Adam | epoch: 040 | loss: 2.14090 - acc: 0.2258 | val_loss: 1.97614 - val_acc: 0.3462 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 13776  | total loss: 2.09834 | time: 12.349s
| Adam | epoch: 041 | loss: 2.09834 - acc: 0.2481 | val_loss: 1.97563 - val_acc: 0.3143 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 14112  | total loss: 2.08958 | time: 12.309s
| Adam | epoch: 042 | loss: 2.08958 - acc: 0.2531 | val_loss: 1.96443 - val_acc: 0.3453 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 14448  | total loss: 2.07222 | time: 12.297s
| Adam | epoch: 043 | loss: 2.07222 - acc: 0.2567 | val_loss: 1.97994 - val_acc: 0.3336 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 14784  | total loss: 2.05631 | time: 12.318s
| Adam | epoch: 044 | loss: 2.05631 - acc: 0.2600 | val_loss: 1.96339 - val_acc: 0.3219 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 15120  | total loss: 2.12972 | time: 12.301s
| Adam | epoch: 045 | loss: 2.12972 - acc: 0.2379 | val_loss: 1.96691 - val_acc: 0.3328 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 15456  | total loss: 2.03813 | time: 12.298s
| Adam | epoch: 046 | loss: 2.03813 - acc: 0.2805 | val_loss: 1.96669 - val_acc: 0.3453 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 15792  | total loss: 2.05852 | time: 12.303s
| Adam | epoch: 047 | loss: 2.05852 - acc: 0.2549 | val_loss: 1.97555 - val_acc: 0.3286 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 16128  | total loss: 2.12875 | time: 12.356s
| Adam | epoch: 048 | loss: 2.12875 - acc: 0.2348 | val_loss: 1.96897 - val_acc: 0.3453 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 16464  | total loss: 2.08012 | time: 12.336s
| Adam | epoch: 049 | loss: 2.08012 - acc: 0.2598 | val_loss: 1.96324 - val_acc: 0.3311 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 16800  | total loss: 2.05782 | time: 12.341s
| Adam | epoch: 050 | loss: 2.05782 - acc: 0.2863 | val_loss: 1.96677 - val_acc: 0.3386 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 17136  | total loss: 2.07358 | time: 12.319s
| Adam | epoch: 051 | loss: 2.07358 - acc: 0.2520 | val_loss: 1.97234 - val_acc: 0.3353 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 17472  | total loss: 2.04590 | time: 12.322s
| Adam | epoch: 052 | loss: 2.04590 - acc: 0.2662 | val_loss: 1.96497 - val_acc: 0.3546 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 17808  | total loss: 2.04955 | time: 12.323s
| Adam | epoch: 053 | loss: 2.04955 - acc: 0.2886 | val_loss: 1.95819 - val_acc: 0.3537 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 18144  | total loss: 2.03729 | time: 12.292s
| Adam | epoch: 054 | loss: 2.03729 - acc: 0.2699 | val_loss: 1.95162 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 18480  | total loss: 2.08250 | time: 12.313s
| Adam | epoch: 055 | loss: 2.08250 - acc: 0.2693 | val_loss: 1.95879 - val_acc: 0.3311 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 18816  | total loss: 2.07531 | time: 12.303s
| Adam | epoch: 056 | loss: 2.07531 - acc: 0.2725 | val_loss: 1.96694 - val_acc: 0.3294 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 19152  | total loss: 2.03263 | time: 12.321s
| Adam | epoch: 057 | loss: 2.03263 - acc: 0.2877 | val_loss: 1.95446 - val_acc: 0.3210 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 19488  | total loss: 2.06748 | time: 12.345s
| Adam | epoch: 058 | loss: 2.06748 - acc: 0.2619 | val_loss: 1.97851 - val_acc: 0.3261 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 19824  | total loss: 2.05089 | time: 15.888s
| Adam | epoch: 059 | loss: 2.05089 - acc: 0.2454 | val_loss: 1.97222 - val_acc: 0.3328 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 20160  | total loss: 1.99324 | time: 13.104s
| Adam | epoch: 060 | loss: 1.99324 - acc: 0.2724 | val_loss: 1.94266 - val_acc: 0.3269 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 20496  | total loss: 2.07698 | time: 13.733s
| Adam | epoch: 061 | loss: 2.07698 - acc: 0.2610 | val_loss: 1.95281 - val_acc: 0.3328 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 20832  | total loss: 2.07770 | time: 13.080s
| Adam | epoch: 062 | loss: 2.07770 - acc: 0.2431 | val_loss: 1.95144 - val_acc: 0.3277 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 21168  | total loss: 2.03727 | time: 13.064s
| Adam | epoch: 063 | loss: 2.03727 - acc: 0.2649 | val_loss: 1.93884 - val_acc: 0.3135 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 21504  | total loss: 2.08945 | time: 13.066s
| Adam | epoch: 064 | loss: 2.08945 - acc: 0.2276 | val_loss: 1.95824 - val_acc: 0.3386 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 21840  | total loss: 2.03350 | time: 12.800s
| Adam | epoch: 065 | loss: 2.03350 - acc: 0.2596 | val_loss: 1.94956 - val_acc: 0.3345 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 22176  | total loss: 2.03793 | time: 12.887s
| Adam | epoch: 066 | loss: 2.03793 - acc: 0.2663 | val_loss: 1.93418 - val_acc: 0.3202 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 22512  | total loss: 2.02782 | time: 12.866s
| Adam | epoch: 067 | loss: 2.02782 - acc: 0.2831 | val_loss: 1.95925 - val_acc: 0.3319 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 22848  | total loss: 2.06705 | time: 12.580s
| Adam | epoch: 068 | loss: 2.06705 - acc: 0.2605 | val_loss: 1.95443 - val_acc: 0.3495 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 23184  | total loss: 2.02440 | time: 12.784s
| Adam | epoch: 069 | loss: 2.02440 - acc: 0.2683 | val_loss: 1.93081 - val_acc: 0.3311 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 23520  | total loss: 2.09568 | time: 12.711s
| Adam | epoch: 070 | loss: 2.09568 - acc: 0.2239 | val_loss: 1.95042 - val_acc: 0.3403 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 23856  | total loss: 2.09630 | time: 12.909s
| Adam | epoch: 071 | loss: 2.09630 - acc: 0.2316 | val_loss: 1.94269 - val_acc: 0.3412 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 24192  | total loss: 2.09468 | time: 12.765s
| Adam | epoch: 072 | loss: 2.09468 - acc: 0.2410 | val_loss: 1.94815 - val_acc: 0.3336 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 24528  | total loss: 2.06961 | time: 12.740s
| Adam | epoch: 073 | loss: 2.06961 - acc: 0.2516 | val_loss: 1.94672 - val_acc: 0.3328 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 24864  | total loss: 2.08400 | time: 12.783s
| Adam | epoch: 074 | loss: 2.08400 - acc: 0.2280 | val_loss: 1.96193 - val_acc: 0.3152 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 25200  | total loss: 2.06520 | time: 12.647s
| Adam | epoch: 075 | loss: 2.06520 - acc: 0.2431 | val_loss: 1.93286 - val_acc: 0.3428 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 25536  | total loss: 2.06720 | time: 12.493s
| Adam | epoch: 076 | loss: 2.06720 - acc: 0.2740 | val_loss: 1.95213 - val_acc: 0.3252 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 25872  | total loss: 2.04620 | time: 12.982s
| Adam | epoch: 077 | loss: 2.04620 - acc: 0.2556 | val_loss: 1.95745 - val_acc: 0.3219 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 26208  | total loss: 2.00702 | time: 12.640s
| Adam | epoch: 078 | loss: 2.00702 - acc: 0.2812 | val_loss: 1.94953 - val_acc: 0.3227 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 26544  | total loss: 2.01821 | time: 12.686s
| Adam | epoch: 079 | loss: 2.01821 - acc: 0.2722 | val_loss: 1.93743 - val_acc: 0.3512 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 26880  | total loss: 2.04871 | time: 12.631s
| Adam | epoch: 080 | loss: 2.04871 - acc: 0.2592 | val_loss: 1.94474 - val_acc: 0.3345 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 27216  | total loss: 2.08430 | time: 13.185s
| Adam | epoch: 081 | loss: 2.08430 - acc: 0.2747 | val_loss: 1.96044 - val_acc: 0.3244 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 27552  | total loss: 2.03435 | time: 12.951s
| Adam | epoch: 082 | loss: 2.03435 - acc: 0.2506 | val_loss: 1.94726 - val_acc: 0.3495 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 27888  | total loss: 2.11882 | time: 12.590s
| Adam | epoch: 083 | loss: 2.11882 - acc: 0.2584 | val_loss: 1.95266 - val_acc: 0.3395 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 28224  | total loss: 2.10933 | time: 14.420s
| Adam | epoch: 084 | loss: 2.10933 - acc: 0.2064 | val_loss: 1.94284 - val_acc: 0.3303 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 28560  | total loss: 2.03544 | time: 13.276s
| Adam | epoch: 085 | loss: 2.03544 - acc: 0.2386 | val_loss: 1.93432 - val_acc: 0.3453 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 28896  | total loss: 2.13057 | time: 12.857s
| Adam | epoch: 086 | loss: 2.13057 - acc: 0.2316 | val_loss: 1.96021 - val_acc: 0.3236 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 29232  | total loss: 2.01322 | time: 12.769s
| Adam | epoch: 087 | loss: 2.01322 - acc: 0.2797 | val_loss: 1.92758 - val_acc: 0.3269 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 29568  | total loss: 2.10009 | time: 12.933s
| Adam | epoch: 088 | loss: 2.10009 - acc: 0.2557 | val_loss: 1.93468 - val_acc: 0.3462 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 29904  | total loss: 2.05607 | time: 12.793s
| Adam | epoch: 089 | loss: 2.05607 - acc: 0.2759 | val_loss: 1.94604 - val_acc: 0.3512 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 30240  | total loss: 2.03716 | time: 12.743s
| Adam | epoch: 090 | loss: 2.03716 - acc: 0.2789 | val_loss: 1.93542 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 30576  | total loss: 2.04074 | time: 13.614s
| Adam | epoch: 091 | loss: 2.04074 - acc: 0.2380 | val_loss: 1.93782 - val_acc: 0.3345 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 30912  | total loss: 2.10139 | time: 13.137s
| Adam | epoch: 092 | loss: 2.10139 - acc: 0.2591 | val_loss: 1.94553 - val_acc: 0.3479 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 31248  | total loss: 2.02988 | time: 12.838s
| Adam | epoch: 093 | loss: 2.02988 - acc: 0.2688 | val_loss: 1.94623 - val_acc: 0.3495 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 31584  | total loss: 2.04909 | time: 13.364s
| Adam | epoch: 094 | loss: 2.04909 - acc: 0.2584 | val_loss: 1.94777 - val_acc: 0.3403 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 31920  | total loss: 2.06876 | time: 12.931s
| Adam | epoch: 095 | loss: 2.06876 - acc: 0.2701 | val_loss: 1.94014 - val_acc: 0.3412 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 32256  | total loss: 2.03918 | time: 13.541s
| Adam | epoch: 096 | loss: 2.03918 - acc: 0.2583 | val_loss: 1.94516 - val_acc: 0.3512 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 32592  | total loss: 2.02711 | time: 12.856s
| Adam | epoch: 097 | loss: 2.02711 - acc: 0.2949 | val_loss: 1.93882 - val_acc: 0.3487 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 32928  | total loss: 2.01945 | time: 12.756s
| Adam | epoch: 098 | loss: 2.01945 - acc: 0.3123 | val_loss: 1.93856 - val_acc: 0.3546 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 33264  | total loss: 2.04076 | time: 13.591s
| Adam | epoch: 099 | loss: 2.04076 - acc: 0.2217 | val_loss: 1.91790 - val_acc: 0.3537 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 33600  | total loss: 2.07773 | time: 13.560s
| Adam | epoch: 100 | loss: 2.07773 - acc: 0.2523 | val_loss: 1.93302 - val_acc: 0.3277 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

