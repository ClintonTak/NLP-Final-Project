Training Step: 98  | total loss: 2.39602 | time: 12.900s
| Adam | epoch: 001 | loss: 2.39602 - acc: 0.0990 | val_loss: 2.39409 - val_acc: 0.1023 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 196  | total loss: 2.36019 | time: 11.323s
| Adam | epoch: 002 | loss: 2.36019 - acc: 0.1232 | val_loss: 2.33598 - val_acc: 0.1668 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 294  | total loss: 2.28235 | time: 11.303s
| Adam | epoch: 003 | loss: 2.28235 - acc: 0.1770 | val_loss: 2.25055 - val_acc: 0.2037 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 392  | total loss: 2.23143 | time: 11.292s
| Adam | epoch: 004 | loss: 2.23143 - acc: 0.1833 | val_loss: 2.18978 - val_acc: 0.2246 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 490  | total loss: 2.18373 | time: 11.361s
| Adam | epoch: 005 | loss: 2.18373 - acc: 0.2139 | val_loss: 2.14312 - val_acc: 0.2330 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 588  | total loss: 2.15649 | time: 11.271s
| Adam | epoch: 006 | loss: 2.15649 - acc: 0.2179 | val_loss: 2.08974 - val_acc: 0.2624 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 686  | total loss: 2.10145 | time: 11.285s
| Adam | epoch: 007 | loss: 2.10145 - acc: 0.2387 | val_loss: 2.06123 - val_acc: 0.2741 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 784  | total loss: 2.09847 | time: 11.287s
| Adam | epoch: 008 | loss: 2.09847 - acc: 0.2319 | val_loss: 2.03972 - val_acc: 0.2816 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 882  | total loss: 2.06850 | time: 11.310s
| Adam | epoch: 009 | loss: 2.06850 - acc: 0.2541 | val_loss: 2.01049 - val_acc: 0.2808 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 980  | total loss: 2.06441 | time: 11.298s
| Adam | epoch: 010 | loss: 2.06441 - acc: 0.2566 | val_loss: 2.00354 - val_acc: 0.2909 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1078  | total loss: 2.05151 | time: 11.307s
| Adam | epoch: 011 | loss: 2.05151 - acc: 0.2647 | val_loss: 1.98864 - val_acc: 0.2959 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1176  | total loss: 2.06895 | time: 11.288s
| Adam | epoch: 012 | loss: 2.06895 - acc: 0.2377 | val_loss: 1.98328 - val_acc: 0.2992 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1274  | total loss: 2.02369 | time: 11.298s
| Adam | epoch: 013 | loss: 2.02369 - acc: 0.2692 | val_loss: 1.96008 - val_acc: 0.3160 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1372  | total loss: 2.02295 | time: 11.301s
| Adam | epoch: 014 | loss: 2.02295 - acc: 0.2696 | val_loss: 1.96340 - val_acc: 0.3101 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1470  | total loss: 2.01323 | time: 11.293s
| Adam | epoch: 015 | loss: 2.01323 - acc: 0.2840 | val_loss: 1.95926 - val_acc: 0.3085 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1568  | total loss: 2.03270 | time: 11.294s
| Adam | epoch: 016 | loss: 2.03270 - acc: 0.2742 | val_loss: 1.95016 - val_acc: 0.3110 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1666  | total loss: 2.00806 | time: 11.266s
| Adam | epoch: 017 | loss: 2.00806 - acc: 0.2783 | val_loss: 1.94113 - val_acc: 0.3219 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1764  | total loss: 1.99617 | time: 11.287s
| Adam | epoch: 018 | loss: 1.99617 - acc: 0.2866 | val_loss: 1.93890 - val_acc: 0.3252 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1862  | total loss: 2.03077 | time: 11.372s
| Adam | epoch: 019 | loss: 2.03077 - acc: 0.2722 | val_loss: 1.94387 - val_acc: 0.3168 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1960  | total loss: 2.04121 | time: 11.691s
| Adam | epoch: 020 | loss: 2.04121 - acc: 0.2813 | val_loss: 1.93614 - val_acc: 0.3269 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2058  | total loss: 2.02892 | time: 11.303s
| Adam | epoch: 021 | loss: 2.02892 - acc: 0.2766 | val_loss: 1.93371 - val_acc: 0.3311 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2156  | total loss: 2.01367 | time: 11.320s
| Adam | epoch: 022 | loss: 2.01367 - acc: 0.2769 | val_loss: 1.94310 - val_acc: 0.3143 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2254  | total loss: 2.02371 | time: 11.313s
| Adam | epoch: 023 | loss: 2.02371 - acc: 0.2727 | val_loss: 1.93913 - val_acc: 0.3210 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2352  | total loss: 1.99437 | time: 11.303s
| Adam | epoch: 024 | loss: 1.99437 - acc: 0.2916 | val_loss: 1.90897 - val_acc: 0.3378 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2450  | total loss: 2.01090 | time: 11.291s
| Adam | epoch: 025 | loss: 2.01090 - acc: 0.2862 | val_loss: 1.90821 - val_acc: 0.3294 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2548  | total loss: 2.04774 | time: 11.302s
| Adam | epoch: 026 | loss: 2.04774 - acc: 0.2724 | val_loss: 1.91042 - val_acc: 0.3462 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2646  | total loss: 2.03752 | time: 11.324s
| Adam | epoch: 027 | loss: 2.03752 - acc: 0.2863 | val_loss: 1.91189 - val_acc: 0.3244 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2744  | total loss: 2.02783 | time: 11.303s
| Adam | epoch: 028 | loss: 2.02783 - acc: 0.2817 | val_loss: 1.91049 - val_acc: 0.3453 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2842  | total loss: 2.02199 | time: 11.299s
| Adam | epoch: 029 | loss: 2.02199 - acc: 0.2784 | val_loss: 1.89916 - val_acc: 0.3420 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2940  | total loss: 2.03872 | time: 11.314s
| Adam | epoch: 030 | loss: 2.03872 - acc: 0.2751 | val_loss: 1.89282 - val_acc: 0.3353 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3038  | total loss: 1.96344 | time: 11.293s
| Adam | epoch: 031 | loss: 1.96344 - acc: 0.2978 | val_loss: 1.89273 - val_acc: 0.3663 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3136  | total loss: 2.05556 | time: 11.288s
| Adam | epoch: 032 | loss: 2.05556 - acc: 0.2800 | val_loss: 1.88099 - val_acc: 0.3462 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3234  | total loss: 1.95732 | time: 11.308s
| Adam | epoch: 033 | loss: 1.95732 - acc: 0.2910 | val_loss: 1.87881 - val_acc: 0.3487 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3332  | total loss: 1.96776 | time: 11.322s
| Adam | epoch: 034 | loss: 1.96776 - acc: 0.2751 | val_loss: 1.86852 - val_acc: 0.3495 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3430  | total loss: 1.93389 | time: 11.314s
| Adam | epoch: 035 | loss: 1.93389 - acc: 0.3184 | val_loss: 1.87265 - val_acc: 0.3588 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3528  | total loss: 1.91577 | time: 11.307s
| Adam | epoch: 036 | loss: 1.91577 - acc: 0.3171 | val_loss: 1.85781 - val_acc: 0.3512 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3626  | total loss: 1.95431 | time: 11.303s
| Adam | epoch: 037 | loss: 1.95431 - acc: 0.3063 | val_loss: 1.88196 - val_acc: 0.3294 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3724  | total loss: 1.93994 | time: 11.319s
| Adam | epoch: 038 | loss: 1.93994 - acc: 0.3039 | val_loss: 1.85448 - val_acc: 0.3663 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3822  | total loss: 1.92942 | time: 11.299s
| Adam | epoch: 039 | loss: 1.92942 - acc: 0.3063 | val_loss: 1.85254 - val_acc: 0.3537 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3920  | total loss: 1.94573 | time: 11.299s
| Adam | epoch: 040 | loss: 1.94573 - acc: 0.2910 | val_loss: 1.85555 - val_acc: 0.3328 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4018  | total loss: 1.91532 | time: 11.320s
| Adam | epoch: 041 | loss: 1.91532 - acc: 0.3034 | val_loss: 1.84342 - val_acc: 0.3479 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4116  | total loss: 1.92676 | time: 11.337s
| Adam | epoch: 042 | loss: 1.92676 - acc: 0.3021 | val_loss: 1.85272 - val_acc: 0.3546 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4214  | total loss: 1.93627 | time: 11.326s
| Adam | epoch: 043 | loss: 1.93627 - acc: 0.3119 | val_loss: 1.84249 - val_acc: 0.3470 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4312  | total loss: 1.92286 | time: 11.303s
| Adam | epoch: 044 | loss: 1.92286 - acc: 0.2940 | val_loss: 1.83881 - val_acc: 0.3512 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4410  | total loss: 1.93824 | time: 11.312s
| Adam | epoch: 045 | loss: 1.93824 - acc: 0.3134 | val_loss: 1.85119 - val_acc: 0.3730 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4508  | total loss: 1.90625 | time: 11.319s
| Adam | epoch: 046 | loss: 1.90625 - acc: 0.3112 | val_loss: 1.83272 - val_acc: 0.3646 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4606  | total loss: 1.93332 | time: 11.305s
| Adam | epoch: 047 | loss: 1.93332 - acc: 0.3171 | val_loss: 1.84010 - val_acc: 0.3621 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4704  | total loss: 1.93633 | time: 11.290s
| Adam | epoch: 048 | loss: 1.93633 - acc: 0.3124 | val_loss: 1.84838 - val_acc: 0.3562 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4802  | total loss: 1.92655 | time: 11.289s
| Adam | epoch: 049 | loss: 1.92655 - acc: 0.3144 | val_loss: 1.83192 - val_acc: 0.3705 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4900  | total loss: 1.88796 | time: 11.319s
| Adam | epoch: 050 | loss: 1.88796 - acc: 0.3266 | val_loss: 1.83432 - val_acc: 0.3562 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4998  | total loss: 1.92473 | time: 11.291s
| Adam | epoch: 051 | loss: 1.92473 - acc: 0.3060 | val_loss: 1.83447 - val_acc: 0.3638 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5096  | total loss: 1.92426 | time: 11.299s
| Adam | epoch: 052 | loss: 1.92426 - acc: 0.3265 | val_loss: 1.83678 - val_acc: 0.3630 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5194  | total loss: 1.92582 | time: 11.300s
| Adam | epoch: 053 | loss: 1.92582 - acc: 0.3119 | val_loss: 1.83474 - val_acc: 0.3822 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5292  | total loss: 1.93329 | time: 11.289s
| Adam | epoch: 054 | loss: 1.93329 - acc: 0.3041 | val_loss: 1.82853 - val_acc: 0.3680 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5390  | total loss: 1.90156 | time: 11.274s
| Adam | epoch: 055 | loss: 1.90156 - acc: 0.3140 | val_loss: 1.83604 - val_acc: 0.3571 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5488  | total loss: 1.89338 | time: 11.300s
| Adam | epoch: 056 | loss: 1.89338 - acc: 0.3171 | val_loss: 1.82524 - val_acc: 0.3529 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5586  | total loss: 1.88715 | time: 11.285s
| Adam | epoch: 057 | loss: 1.88715 - acc: 0.3207 | val_loss: 1.81394 - val_acc: 0.3604 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5684  | total loss: 1.88841 | time: 11.304s
| Adam | epoch: 058 | loss: 1.88841 - acc: 0.3200 | val_loss: 1.81773 - val_acc: 0.3713 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5782  | total loss: 1.89861 | time: 11.300s
| Adam | epoch: 059 | loss: 1.89861 - acc: 0.3314 | val_loss: 1.82451 - val_acc: 0.3621 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5880  | total loss: 1.88289 | time: 11.313s
| Adam | epoch: 060 | loss: 1.88289 - acc: 0.3193 | val_loss: 1.82770 - val_acc: 0.3747 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5978  | total loss: 1.86386 | time: 11.307s
| Adam | epoch: 061 | loss: 1.86386 - acc: 0.3300 | val_loss: 1.82012 - val_acc: 0.3571 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6076  | total loss: 1.89073 | time: 11.317s
| Adam | epoch: 062 | loss: 1.89073 - acc: 0.3336 | val_loss: 1.81758 - val_acc: 0.3797 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6174  | total loss: 1.91761 | time: 11.296s
| Adam | epoch: 063 | loss: 1.91761 - acc: 0.3208 | val_loss: 1.81161 - val_acc: 0.3697 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6272  | total loss: 1.90215 | time: 11.319s
| Adam | epoch: 064 | loss: 1.90215 - acc: 0.3220 | val_loss: 1.81914 - val_acc: 0.3747 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6370  | total loss: 1.89539 | time: 11.294s
| Adam | epoch: 065 | loss: 1.89539 - acc: 0.3244 | val_loss: 1.80894 - val_acc: 0.3663 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6468  | total loss: 1.85127 | time: 11.317s
| Adam | epoch: 066 | loss: 1.85127 - acc: 0.3465 | val_loss: 1.81624 - val_acc: 0.3630 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6566  | total loss: 1.90210 | time: 11.327s
| Adam | epoch: 067 | loss: 1.90210 - acc: 0.3139 | val_loss: 1.80886 - val_acc: 0.3571 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6664  | total loss: 1.91992 | time: 11.297s
| Adam | epoch: 068 | loss: 1.91992 - acc: 0.3147 | val_loss: 1.82101 - val_acc: 0.3571 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6762  | total loss: 1.89148 | time: 11.287s
| Adam | epoch: 069 | loss: 1.89148 - acc: 0.3237 | val_loss: 1.81071 - val_acc: 0.3722 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6860  | total loss: 1.88918 | time: 11.310s
| Adam | epoch: 070 | loss: 1.88918 - acc: 0.3200 | val_loss: 1.81630 - val_acc: 0.3780 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6958  | total loss: 1.89575 | time: 11.284s
| Adam | epoch: 071 | loss: 1.89575 - acc: 0.3286 | val_loss: 1.81543 - val_acc: 0.3554 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7056  | total loss: 1.91053 | time: 11.304s
| Adam | epoch: 072 | loss: 1.91053 - acc: 0.3157 | val_loss: 1.82614 - val_acc: 0.3663 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7154  | total loss: 1.89348 | time: 11.318s
| Adam | epoch: 073 | loss: 1.89348 - acc: 0.3296 | val_loss: 1.81219 - val_acc: 0.3596 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7252  | total loss: 1.90178 | time: 11.308s
| Adam | epoch: 074 | loss: 1.90178 - acc: 0.3285 | val_loss: 1.80367 - val_acc: 0.3697 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7350  | total loss: 1.91203 | time: 11.335s
| Adam | epoch: 075 | loss: 1.91203 - acc: 0.3218 | val_loss: 1.81309 - val_acc: 0.3713 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7448  | total loss: 1.87344 | time: 11.294s
| Adam | epoch: 076 | loss: 1.87344 - acc: 0.3308 | val_loss: 1.80553 - val_acc: 0.3604 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7546  | total loss: 1.89376 | time: 11.298s
| Adam | epoch: 077 | loss: 1.89376 - acc: 0.3349 | val_loss: 1.80128 - val_acc: 0.3680 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7644  | total loss: 1.87211 | time: 11.313s
| Adam | epoch: 078 | loss: 1.87211 - acc: 0.3508 | val_loss: 1.79682 - val_acc: 0.3822 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7742  | total loss: 1.90956 | time: 11.326s
| Adam | epoch: 079 | loss: 1.90956 - acc: 0.3236 | val_loss: 1.79747 - val_acc: 0.3814 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7840  | total loss: 1.89047 | time: 11.294s
| Adam | epoch: 080 | loss: 1.89047 - acc: 0.3185 | val_loss: 1.79313 - val_acc: 0.3655 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7938  | total loss: 1.89668 | time: 11.314s
| Adam | epoch: 081 | loss: 1.89668 - acc: 0.3253 | val_loss: 1.79617 - val_acc: 0.3873 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8036  | total loss: 1.90466 | time: 11.263s
| Adam | epoch: 082 | loss: 1.90466 - acc: 0.3241 | val_loss: 1.80133 - val_acc: 0.3814 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8134  | total loss: 1.84038 | time: 11.320s
| Adam | epoch: 083 | loss: 1.84038 - acc: 0.3453 | val_loss: 1.79310 - val_acc: 0.3738 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8232  | total loss: 1.86369 | time: 11.292s
| Adam | epoch: 084 | loss: 1.86369 - acc: 0.3270 | val_loss: 1.79006 - val_acc: 0.3755 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8330  | total loss: 1.87499 | time: 11.310s
| Adam | epoch: 085 | loss: 1.87499 - acc: 0.3284 | val_loss: 1.79244 - val_acc: 0.3655 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8428  | total loss: 1.86225 | time: 11.305s
| Adam | epoch: 086 | loss: 1.86225 - acc: 0.3445 | val_loss: 1.78971 - val_acc: 0.3730 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8526  | total loss: 1.87351 | time: 11.303s
| Adam | epoch: 087 | loss: 1.87351 - acc: 0.3265 | val_loss: 1.79489 - val_acc: 0.3789 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8624  | total loss: 1.87915 | time: 11.317s
| Adam | epoch: 088 | loss: 1.87915 - acc: 0.3362 | val_loss: 1.80490 - val_acc: 0.3655 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8722  | total loss: 1.89490 | time: 11.312s
| Adam | epoch: 089 | loss: 1.89490 - acc: 0.3271 | val_loss: 1.79369 - val_acc: 0.3747 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8820  | total loss: 1.89023 | time: 11.331s
| Adam | epoch: 090 | loss: 1.89023 - acc: 0.3362 | val_loss: 1.78703 - val_acc: 0.3806 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8918  | total loss: 1.87155 | time: 11.308s
| Adam | epoch: 091 | loss: 1.87155 - acc: 0.3322 | val_loss: 1.78180 - val_acc: 0.3764 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9016  | total loss: 1.87797 | time: 11.294s
| Adam | epoch: 092 | loss: 1.87797 - acc: 0.3369 | val_loss: 1.78510 - val_acc: 0.3806 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9114  | total loss: 1.87324 | time: 11.317s
| Adam | epoch: 093 | loss: 1.87324 - acc: 0.3285 | val_loss: 1.80048 - val_acc: 0.3847 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9212  | total loss: 1.85811 | time: 11.271s
| Adam | epoch: 094 | loss: 1.85811 - acc: 0.3289 | val_loss: 1.77580 - val_acc: 0.3797 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9310  | total loss: 1.85753 | time: 11.313s
| Adam | epoch: 095 | loss: 1.85753 - acc: 0.3255 | val_loss: 1.78912 - val_acc: 0.3822 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9408  | total loss: 1.88522 | time: 11.290s
| Adam | epoch: 096 | loss: 1.88522 - acc: 0.3379 | val_loss: 1.80785 - val_acc: 0.3772 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9506  | total loss: 1.86137 | time: 11.291s
| Adam | epoch: 097 | loss: 1.86137 - acc: 0.3514 | val_loss: 1.79050 - val_acc: 0.3596 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9604  | total loss: 1.87023 | time: 11.353s
| Adam | epoch: 098 | loss: 1.87023 - acc: 0.3397 | val_loss: 1.79582 - val_acc: 0.3630 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9702  | total loss: 1.86549 | time: 11.317s
| Adam | epoch: 099 | loss: 1.86549 - acc: 0.3524 | val_loss: 1.78882 - val_acc: 0.3806 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9800  | total loss: 1.83384 | time: 11.340s
| Adam | epoch: 100 | loss: 1.83384 - acc: 0.3610 | val_loss: 1.78489 - val_acc: 0.3780 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

