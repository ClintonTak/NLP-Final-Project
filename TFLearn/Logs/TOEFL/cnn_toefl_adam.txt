Training Step: 336  | total loss: 2.37607 | time: 13.323s
| Adam | epoch: 001 | loss: 2.37607 - acc: 0.1264 | val_loss: 2.37957 - val_acc: 0.0855 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 672  | total loss: 2.26436 | time: 12.369s
| Adam | epoch: 002 | loss: 2.26436 - acc: 0.1683 | val_loss: 2.25399 - val_acc: 0.1953 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1008  | total loss: 2.28042 | time: 12.352s
| Adam | epoch: 003 | loss: 2.28042 - acc: 0.1482 | val_loss: 2.21666 - val_acc: 0.2137 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1344  | total loss: 2.21598 | time: 12.378s
| Adam | epoch: 004 | loss: 2.21598 - acc: 0.1911 | val_loss: 2.16156 - val_acc: 0.2322 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1680  | total loss: 2.23108 | time: 12.375s
| Adam | epoch: 005 | loss: 2.23108 - acc: 0.1801 | val_loss: 2.13495 - val_acc: 0.2573 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2016  | total loss: 2.17319 | time: 12.305s
| Adam | epoch: 006 | loss: 2.17319 - acc: 0.2049 | val_loss: 2.09670 - val_acc: 0.2540 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2352  | total loss: 2.16102 | time: 12.348s
| Adam | epoch: 007 | loss: 2.16102 - acc: 0.2106 | val_loss: 2.07500 - val_acc: 0.2808 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2688  | total loss: 2.19070 | time: 12.400s
| Adam | epoch: 008 | loss: 2.19070 - acc: 0.2085 | val_loss: 2.07537 - val_acc: 0.3034 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3024  | total loss: 2.15260 | time: 12.349s
| Adam | epoch: 009 | loss: 2.15260 - acc: 0.2395 | val_loss: 2.06152 - val_acc: 0.2808 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3360  | total loss: 2.15400 | time: 12.345s
| Adam | epoch: 010 | loss: 2.15400 - acc: 0.2248 | val_loss: 2.04324 - val_acc: 0.2867 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3696  | total loss: 2.17679 | time: 12.321s
| Adam | epoch: 011 | loss: 2.17679 - acc: 0.2090 | val_loss: 2.04811 - val_acc: 0.3135 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4032  | total loss: 2.13616 | time: 12.357s
| Adam | epoch: 012 | loss: 2.13616 - acc: 0.2369 | val_loss: 2.03600 - val_acc: 0.3043 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4368  | total loss: 2.14924 | time: 12.382s
| Adam | epoch: 013 | loss: 2.14924 - acc: 0.2339 | val_loss: 2.02941 - val_acc: 0.3370 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4704  | total loss: 2.13762 | time: 12.297s
| Adam | epoch: 014 | loss: 2.13762 - acc: 0.2058 | val_loss: 1.99712 - val_acc: 0.3319 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5040  | total loss: 2.14562 | time: 12.337s
| Adam | epoch: 015 | loss: 2.14562 - acc: 0.2213 | val_loss: 1.98979 - val_acc: 0.3403 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5376  | total loss: 2.09873 | time: 12.390s
| Adam | epoch: 016 | loss: 2.09873 - acc: 0.2258 | val_loss: 1.96787 - val_acc: 0.3286 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5712  | total loss: 2.10804 | time: 12.375s
| Adam | epoch: 017 | loss: 2.10804 - acc: 0.2286 | val_loss: 1.98311 - val_acc: 0.3294 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6048  | total loss: 2.12232 | time: 12.348s
| Adam | epoch: 018 | loss: 2.12232 - acc: 0.2268 | val_loss: 1.97675 - val_acc: 0.3319 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6384  | total loss: 2.11961 | time: 12.321s
| Adam | epoch: 019 | loss: 2.11961 - acc: 0.2364 | val_loss: 1.98518 - val_acc: 0.3202 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6720  | total loss: 2.14229 | time: 12.354s
| Adam | epoch: 020 | loss: 2.14229 - acc: 0.2398 | val_loss: 1.97754 - val_acc: 0.3303 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7056  | total loss: 2.12284 | time: 12.366s
| Adam | epoch: 021 | loss: 2.12284 - acc: 0.2275 | val_loss: 1.98420 - val_acc: 0.3244 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7392  | total loss: 2.12775 | time: 12.323s
| Adam | epoch: 022 | loss: 2.12775 - acc: 0.2505 | val_loss: 1.96534 - val_acc: 0.3579 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7728  | total loss: 2.11267 | time: 12.366s
| Adam | epoch: 023 | loss: 2.11267 - acc: 0.2496 | val_loss: 1.96352 - val_acc: 0.3428 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8064  | total loss: 2.07631 | time: 12.358s
| Adam | epoch: 024 | loss: 2.07631 - acc: 0.2558 | val_loss: 1.95515 - val_acc: 0.3453 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8400  | total loss: 2.08586 | time: 12.381s
| Adam | epoch: 025 | loss: 2.08586 - acc: 0.2485 | val_loss: 1.96108 - val_acc: 0.3328 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8736  | total loss: 2.10007 | time: 12.389s
| Adam | epoch: 026 | loss: 2.10007 - acc: 0.2532 | val_loss: 1.93550 - val_acc: 0.3487 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9072  | total loss: 2.14584 | time: 12.368s
| Adam | epoch: 027 | loss: 2.14584 - acc: 0.2213 | val_loss: 1.96989 - val_acc: 0.3210 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9408  | total loss: 2.04824 | time: 12.379s
| Adam | epoch: 028 | loss: 2.04824 - acc: 0.2480 | val_loss: 1.95733 - val_acc: 0.3386 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9744  | total loss: 2.10692 | time: 12.345s
| Adam | epoch: 029 | loss: 2.10692 - acc: 0.2459 | val_loss: 1.94409 - val_acc: 0.3604 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 10080  | total loss: 2.14555 | time: 12.364s
| Adam | epoch: 030 | loss: 2.14555 - acc: 0.2248 | val_loss: 1.95744 - val_acc: 0.3386 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 10416  | total loss: 2.09931 | time: 12.378s
| Adam | epoch: 031 | loss: 2.09931 - acc: 0.2442 | val_loss: 1.95991 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 10752  | total loss: 2.06656 | time: 12.357s
| Adam | epoch: 032 | loss: 2.06656 - acc: 0.2697 | val_loss: 1.94442 - val_acc: 0.3437 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 11088  | total loss: 2.08329 | time: 12.374s
| Adam | epoch: 033 | loss: 2.08329 - acc: 0.2536 | val_loss: 1.94235 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 11424  | total loss: 2.06110 | time: 12.376s
| Adam | epoch: 034 | loss: 2.06110 - acc: 0.2483 | val_loss: 1.94166 - val_acc: 0.3680 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 11760  | total loss: 2.11081 | time: 12.334s
| Adam | epoch: 035 | loss: 2.11081 - acc: 0.2319 | val_loss: 1.94720 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 12096  | total loss: 2.06436 | time: 12.313s
| Adam | epoch: 036 | loss: 2.06436 - acc: 0.2764 | val_loss: 1.95008 - val_acc: 0.3219 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 12432  | total loss: 2.08990 | time: 12.361s
| Adam | epoch: 037 | loss: 2.08990 - acc: 0.2512 | val_loss: 1.92639 - val_acc: 0.3470 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 12768  | total loss: 2.05563 | time: 12.342s
| Adam | epoch: 038 | loss: 2.05563 - acc: 0.2729 | val_loss: 1.94669 - val_acc: 0.3370 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 13104  | total loss: 2.07431 | time: 12.387s
| Adam | epoch: 039 | loss: 2.07431 - acc: 0.2517 | val_loss: 1.93812 - val_acc: 0.3386 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 13440  | total loss: 2.08674 | time: 12.375s
| Adam | epoch: 040 | loss: 2.08674 - acc: 0.2594 | val_loss: 1.93476 - val_acc: 0.3470 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 13776  | total loss: 2.08195 | time: 12.360s
| Adam | epoch: 041 | loss: 2.08195 - acc: 0.2527 | val_loss: 1.95094 - val_acc: 0.3395 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 14112  | total loss: 2.06857 | time: 12.376s
| Adam | epoch: 042 | loss: 2.06857 - acc: 0.2531 | val_loss: 1.94399 - val_acc: 0.3495 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 14448  | total loss: 2.11187 | time: 12.324s
| Adam | epoch: 043 | loss: 2.11187 - acc: 0.2373 | val_loss: 1.93474 - val_acc: 0.3345 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 14784  | total loss: 2.09288 | time: 12.316s
| Adam | epoch: 044 | loss: 2.09288 - acc: 0.2463 | val_loss: 1.94606 - val_acc: 0.3487 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 15120  | total loss: 2.02608 | time: 12.365s
| Adam | epoch: 045 | loss: 2.02608 - acc: 0.2797 | val_loss: 1.92571 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 15456  | total loss: 2.07708 | time: 12.331s
| Adam | epoch: 046 | loss: 2.07708 - acc: 0.2600 | val_loss: 1.92082 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 15792  | total loss: 2.06900 | time: 12.315s
| Adam | epoch: 047 | loss: 2.06900 - acc: 0.2637 | val_loss: 1.93573 - val_acc: 0.3445 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 16128  | total loss: 2.11091 | time: 12.380s
| Adam | epoch: 048 | loss: 2.11091 - acc: 0.2224 | val_loss: 1.92724 - val_acc: 0.3562 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 16464  | total loss: 2.09297 | time: 12.382s
| Adam | epoch: 049 | loss: 2.09297 - acc: 0.2517 | val_loss: 1.92044 - val_acc: 0.3630 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 16800  | total loss: 2.13251 | time: 12.414s
| Adam | epoch: 050 | loss: 2.13251 - acc: 0.2358 | val_loss: 1.94472 - val_acc: 0.3420 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 17136  | total loss: 2.12340 | time: 12.367s
| Adam | epoch: 051 | loss: 2.12340 - acc: 0.2300 | val_loss: 1.93039 - val_acc: 0.3529 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 17472  | total loss: 2.14097 | time: 12.421s
| Adam | epoch: 052 | loss: 2.14097 - acc: 0.2340 | val_loss: 1.93420 - val_acc: 0.3453 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 17808  | total loss: 2.08417 | time: 12.383s
| Adam | epoch: 053 | loss: 2.08417 - acc: 0.2576 | val_loss: 1.91933 - val_acc: 0.3403 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 18144  | total loss: 2.04124 | time: 12.364s
| Adam | epoch: 054 | loss: 2.04124 - acc: 0.2727 | val_loss: 1.92347 - val_acc: 0.3462 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 18480  | total loss: 2.06629 | time: 12.365s
| Adam | epoch: 055 | loss: 2.06629 - acc: 0.2287 | val_loss: 1.91248 - val_acc: 0.3571 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 18816  | total loss: 2.06585 | time: 12.369s
| Adam | epoch: 056 | loss: 2.06585 - acc: 0.2452 | val_loss: 1.93349 - val_acc: 0.3495 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 19152  | total loss: 2.04674 | time: 12.336s
| Adam | epoch: 057 | loss: 2.04674 - acc: 0.2799 | val_loss: 1.92788 - val_acc: 0.3630 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 19488  | total loss: 2.02604 | time: 12.348s
| Adam | epoch: 058 | loss: 2.02604 - acc: 0.2504 | val_loss: 1.93080 - val_acc: 0.3529 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 19824  | total loss: 2.08159 | time: 12.371s
| Adam | epoch: 059 | loss: 2.08159 - acc: 0.2650 | val_loss: 1.92287 - val_acc: 0.3546 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 20160  | total loss: 2.11184 | time: 12.363s
| Adam | epoch: 060 | loss: 2.11184 - acc: 0.2666 | val_loss: 1.94266 - val_acc: 0.3345 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 20496  | total loss: 2.07937 | time: 12.400s
| Adam | epoch: 061 | loss: 2.07937 - acc: 0.2731 | val_loss: 1.91201 - val_acc: 0.3529 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 20832  | total loss: 2.08600 | time: 12.669s
| Adam | epoch: 062 | loss: 2.08600 - acc: 0.2521 | val_loss: 1.93479 - val_acc: 0.3529 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 21168  | total loss: 2.02230 | time: 12.384s
| Adam | epoch: 063 | loss: 2.02230 - acc: 0.2663 | val_loss: 1.88883 - val_acc: 0.3546 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 21504  | total loss: 2.03601 | time: 12.394s
| Adam | epoch: 064 | loss: 2.03601 - acc: 0.2924 | val_loss: 1.90884 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 21840  | total loss: 2.05588 | time: 12.373s
| Adam | epoch: 065 | loss: 2.05588 - acc: 0.2727 | val_loss: 1.90935 - val_acc: 0.3554 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 22176  | total loss: 2.03950 | time: 12.317s
| Adam | epoch: 066 | loss: 2.03950 - acc: 0.2710 | val_loss: 1.91716 - val_acc: 0.3470 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 22512  | total loss: 2.10647 | time: 12.325s
| Adam | epoch: 067 | loss: 2.10647 - acc: 0.2323 | val_loss: 1.92106 - val_acc: 0.3529 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 22848  | total loss: 2.01513 | time: 12.373s
| Adam | epoch: 068 | loss: 2.01513 - acc: 0.2705 | val_loss: 1.89539 - val_acc: 0.3504 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 23184  | total loss: 2.10789 | time: 12.315s
| Adam | epoch: 069 | loss: 2.10789 - acc: 0.2314 | val_loss: 1.92436 - val_acc: 0.3378 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 23520  | total loss: 2.06870 | time: 12.375s
| Adam | epoch: 070 | loss: 2.06870 - acc: 0.2550 | val_loss: 1.92011 - val_acc: 0.3495 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 23856  | total loss: 2.08182 | time: 12.345s
| Adam | epoch: 071 | loss: 2.08182 - acc: 0.2543 | val_loss: 1.93036 - val_acc: 0.3361 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 24192  | total loss: 2.03324 | time: 12.359s
| Adam | epoch: 072 | loss: 2.03324 - acc: 0.2601 | val_loss: 1.91236 - val_acc: 0.3462 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 24528  | total loss: 2.04831 | time: 12.374s
| Adam | epoch: 073 | loss: 2.04831 - acc: 0.2626 | val_loss: 1.90529 - val_acc: 0.3495 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 24864  | total loss: 2.08179 | time: 12.338s
| Adam | epoch: 074 | loss: 2.08179 - acc: 0.2476 | val_loss: 1.91382 - val_acc: 0.3521 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 25200  | total loss: 2.06991 | time: 12.366s
| Adam | epoch: 075 | loss: 2.06991 - acc: 0.2952 | val_loss: 1.89526 - val_acc: 0.3588 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 25536  | total loss: 2.09305 | time: 12.350s
| Adam | epoch: 076 | loss: 2.09305 - acc: 0.2515 | val_loss: 1.91499 - val_acc: 0.3487 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 25872  | total loss: 2.06135 | time: 12.310s
| Adam | epoch: 077 | loss: 2.06135 - acc: 0.2710 | val_loss: 1.90403 - val_acc: 0.3495 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 26208  | total loss: 2.10173 | time: 12.408s
| Adam | epoch: 078 | loss: 2.10173 - acc: 0.2577 | val_loss: 1.93539 - val_acc: 0.3420 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 26544  | total loss: 2.04977 | time: 12.352s
| Adam | epoch: 079 | loss: 2.04977 - acc: 0.2652 | val_loss: 1.90058 - val_acc: 0.3537 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 26880  | total loss: 2.07307 | time: 12.348s
| Adam | epoch: 080 | loss: 2.07307 - acc: 0.2709 | val_loss: 1.90711 - val_acc: 0.3655 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 27216  | total loss: 2.07900 | time: 12.386s
| Adam | epoch: 081 | loss: 2.07900 - acc: 0.2384 | val_loss: 1.90293 - val_acc: 0.3554 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 27552  | total loss: 2.04106 | time: 12.345s
| Adam | epoch: 082 | loss: 2.04106 - acc: 0.2549 | val_loss: 1.89014 - val_acc: 0.3437 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 27888  | total loss: 2.10399 | time: 12.353s
| Adam | epoch: 083 | loss: 2.10399 - acc: 0.2325 | val_loss: 1.90880 - val_acc: 0.3621 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 28224  | total loss: 2.06959 | time: 12.367s
| Adam | epoch: 084 | loss: 2.06959 - acc: 0.2437 | val_loss: 1.90831 - val_acc: 0.3579 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 28560  | total loss: 2.05227 | time: 12.321s
| Adam | epoch: 085 | loss: 2.05227 - acc: 0.2573 | val_loss: 1.91335 - val_acc: 0.3571 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 28896  | total loss: 2.05985 | time: 12.369s
| Adam | epoch: 086 | loss: 2.05985 - acc: 0.2840 | val_loss: 1.89086 - val_acc: 0.3680 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 29232  | total loss: 2.01755 | time: 12.409s
| Adam | epoch: 087 | loss: 2.01755 - acc: 0.2527 | val_loss: 1.89487 - val_acc: 0.3638 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 29568  | total loss: 2.05429 | time: 12.373s
| Adam | epoch: 088 | loss: 2.05429 - acc: 0.2571 | val_loss: 1.88717 - val_acc: 0.3747 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 29904  | total loss: 2.04231 | time: 12.373s
| Adam | epoch: 089 | loss: 2.04231 - acc: 0.2542 | val_loss: 1.88916 - val_acc: 0.3621 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 30240  | total loss: 2.08188 | time: 12.328s
| Adam | epoch: 090 | loss: 2.08188 - acc: 0.2634 | val_loss: 1.90995 - val_acc: 0.3604 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 30576  | total loss: 2.07491 | time: 12.388s
| Adam | epoch: 091 | loss: 2.07491 - acc: 0.2512 | val_loss: 1.88548 - val_acc: 0.3655 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 30912  | total loss: 2.06817 | time: 12.370s
| Adam | epoch: 092 | loss: 2.06817 - acc: 0.2745 | val_loss: 1.89217 - val_acc: 0.3738 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 31248  | total loss: 2.08365 | time: 12.371s
| Adam | epoch: 093 | loss: 2.08365 - acc: 0.2723 | val_loss: 1.89608 - val_acc: 0.3428 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 31584  | total loss: 2.07454 | time: 12.366s
| Adam | epoch: 094 | loss: 2.07454 - acc: 0.2764 | val_loss: 1.90920 - val_acc: 0.3705 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 31920  | total loss: 2.06337 | time: 12.380s
| Adam | epoch: 095 | loss: 2.06337 - acc: 0.2482 | val_loss: 1.90811 - val_acc: 0.3638 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 32256  | total loss: 1.98943 | time: 12.340s
| Adam | epoch: 096 | loss: 1.98943 - acc: 0.2720 | val_loss: 1.88037 - val_acc: 0.3596 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 32592  | total loss: 2.04633 | time: 12.353s
| Adam | epoch: 097 | loss: 2.04633 - acc: 0.2483 | val_loss: 1.90343 - val_acc: 0.3462 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 32928  | total loss: 2.02559 | time: 12.314s
| Adam | epoch: 098 | loss: 2.02559 - acc: 0.2859 | val_loss: 1.89650 - val_acc: 0.3638 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 33264  | total loss: 2.04541 | time: 12.354s
| Adam | epoch: 099 | loss: 2.04541 - acc: 0.2644 | val_loss: 1.87880 - val_acc: 0.3671 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 33600  | total loss: 1.99261 | time: 12.352s
| Adam | epoch: 100 | loss: 1.99261 - acc: 0.3073 | val_loss: 1.87835 - val_acc: 0.3764 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

