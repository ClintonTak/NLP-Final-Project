Training Step: 98  | total loss: 2.42416 | time: 88.782s
| Adam | epoch: 001 | loss: 2.42416 - acc: 0.0948 | val_loss: 2.40631 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 196  | total loss: 2.42122 | time: 27.923s
| Adam | epoch: 002 | loss: 2.42122 - acc: 0.0844 | val_loss: 2.40310 - val_acc: 0.1014 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 294  | total loss: 2.41648 | time: 27.736s
| Adam | epoch: 003 | loss: 2.41648 - acc: 0.0838 | val_loss: 2.40054 - val_acc: 0.1014 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 392  | total loss: 2.41447 | time: 27.899s
| Adam | epoch: 004 | loss: 2.41447 - acc: 0.0894 | val_loss: 2.40349 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 490  | total loss: 2.40783 | time: 27.793s
| Adam | epoch: 005 | loss: 2.40783 - acc: 0.0826 | val_loss: 2.40336 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 588  | total loss: 2.40713 | time: 27.859s
| Adam | epoch: 006 | loss: 2.40713 - acc: 0.0993 | val_loss: 2.40485 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 686  | total loss: 2.40588 | time: 27.727s
| Adam | epoch: 007 | loss: 2.40588 - acc: 0.1077 | val_loss: 2.40183 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 784  | total loss: 2.40660 | time: 27.751s
| Adam | epoch: 008 | loss: 2.40660 - acc: 0.0798 | val_loss: 2.40294 - val_acc: 0.0863 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 882  | total loss: 2.40413 | time: 27.789s
| Adam | epoch: 009 | loss: 2.40413 - acc: 0.0838 | val_loss: 2.40311 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 980  | total loss: 2.40863 | time: 27.853s
| Adam | epoch: 010 | loss: 2.40863 - acc: 0.0813 | val_loss: 2.40143 - val_acc: 0.0964 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1078  | total loss: 2.40555 | time: 27.755s
| Adam | epoch: 011 | loss: 2.40555 - acc: 0.0848 | val_loss: 2.40153 - val_acc: 0.1014 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1176  | total loss: 2.40398 | time: 28.030s
| Adam | epoch: 012 | loss: 2.40398 - acc: 0.1013 | val_loss: 2.40126 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1274  | total loss: 2.40301 | time: 27.883s
| Adam | epoch: 013 | loss: 2.40301 - acc: 0.0937 | val_loss: 2.40139 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1372  | total loss: 2.40666 | time: 27.706s
| Adam | epoch: 014 | loss: 2.40666 - acc: 0.0809 | val_loss: 2.39865 - val_acc: 0.1014 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1470  | total loss: 2.40641 | time: 27.829s
| Adam | epoch: 015 | loss: 2.40641 - acc: 0.0928 | val_loss: 2.40152 - val_acc: 0.0863 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1568  | total loss: 2.40758 | time: 27.764s
| Adam | epoch: 016 | loss: 2.40758 - acc: 0.0837 | val_loss: 2.40233 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1666  | total loss: 2.40408 | time: 27.875s
| Adam | epoch: 017 | loss: 2.40408 - acc: 0.0935 | val_loss: 2.39931 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1764  | total loss: 2.40392 | time: 27.755s
| Adam | epoch: 018 | loss: 2.40392 - acc: 0.0938 | val_loss: 2.40097 - val_acc: 0.0863 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1862  | total loss: 2.40740 | time: 27.730s
| Adam | epoch: 019 | loss: 2.40740 - acc: 0.0760 | val_loss: 2.39821 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 1960  | total loss: 2.40378 | time: 28.073s
| Adam | epoch: 020 | loss: 2.40378 - acc: 0.0926 | val_loss: 2.40009 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2058  | total loss: 2.40326 | time: 27.805s
| Adam | epoch: 021 | loss: 2.40326 - acc: 0.0826 | val_loss: 2.40057 - val_acc: 0.1014 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2156  | total loss: 2.40164 | time: 27.772s
| Adam | epoch: 022 | loss: 2.40164 - acc: 0.0966 | val_loss: 2.40321 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2254  | total loss: 2.39991 | time: 27.745s
| Adam | epoch: 023 | loss: 2.39991 - acc: 0.0925 | val_loss: 2.39950 - val_acc: 0.0964 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2352  | total loss: 2.40323 | time: 27.771s
| Adam | epoch: 024 | loss: 2.40323 - acc: 0.0970 | val_loss: 2.40308 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2450  | total loss: 2.40244 | time: 27.713s
| Adam | epoch: 025 | loss: 2.40244 - acc: 0.0840 | val_loss: 2.39872 - val_acc: 0.0930 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2548  | total loss: 2.40284 | time: 27.770s
| Adam | epoch: 026 | loss: 2.40284 - acc: 0.0903 | val_loss: 2.40033 - val_acc: 0.0930 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2646  | total loss: 2.40209 | time: 27.860s
| Adam | epoch: 027 | loss: 2.40209 - acc: 0.0889 | val_loss: 2.39921 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2744  | total loss: 2.40204 | time: 27.862s
| Adam | epoch: 028 | loss: 2.40204 - acc: 0.0988 | val_loss: 2.39996 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2842  | total loss: 2.40902 | time: 27.770s
| Adam | epoch: 029 | loss: 2.40902 - acc: 0.0891 | val_loss: 2.39876 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 2940  | total loss: 2.40037 | time: 27.899s
| Adam | epoch: 030 | loss: 2.40037 - acc: 0.0946 | val_loss: 2.39986 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3038  | total loss: 2.40235 | time: 27.848s
| Adam | epoch: 031 | loss: 2.40235 - acc: 0.1017 | val_loss: 2.40108 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3136  | total loss: 2.40102 | time: 27.764s
| Adam | epoch: 032 | loss: 2.40102 - acc: 0.0896 | val_loss: 2.39891 - val_acc: 0.0964 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3234  | total loss: 2.40515 | time: 27.932s
| Adam | epoch: 033 | loss: 2.40515 - acc: 0.0835 | val_loss: 2.40027 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3332  | total loss: 2.40347 | time: 27.737s
| Adam | epoch: 034 | loss: 2.40347 - acc: 0.0883 | val_loss: 2.39980 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3430  | total loss: 2.40158 | time: 27.805s
| Adam | epoch: 035 | loss: 2.40158 - acc: 0.0930 | val_loss: 2.39944 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3528  | total loss: 2.40098 | time: 27.647s
| Adam | epoch: 036 | loss: 2.40098 - acc: 0.0856 | val_loss: 2.39991 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3626  | total loss: 2.40024 | time: 27.673s
| Adam | epoch: 037 | loss: 2.40024 - acc: 0.0931 | val_loss: 2.39793 - val_acc: 0.0964 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3724  | total loss: 2.40188 | time: 27.889s
| Adam | epoch: 038 | loss: 2.40188 - acc: 0.0867 | val_loss: 2.40064 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3822  | total loss: 2.40258 | time: 27.901s
| Adam | epoch: 039 | loss: 2.40258 - acc: 0.0852 | val_loss: 2.39951 - val_acc: 0.0863 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 3920  | total loss: 2.39996 | time: 27.837s
| Adam | epoch: 040 | loss: 2.39996 - acc: 0.0927 | val_loss: 2.39952 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4018  | total loss: 2.40136 | time: 27.692s
| Adam | epoch: 041 | loss: 2.40136 - acc: 0.0908 | val_loss: 2.40139 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4116  | total loss: 2.40174 | time: 27.694s
| Adam | epoch: 042 | loss: 2.40174 - acc: 0.0903 | val_loss: 2.40169 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4214  | total loss: 2.39915 | time: 27.676s
| Adam | epoch: 043 | loss: 2.39915 - acc: 0.1007 | val_loss: 2.39972 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4312  | total loss: 2.40071 | time: 27.685s
| Adam | epoch: 044 | loss: 2.40071 - acc: 0.0934 | val_loss: 2.40064 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4410  | total loss: 2.40004 | time: 27.669s
| Adam | epoch: 045 | loss: 2.40004 - acc: 0.1019 | val_loss: 2.39918 - val_acc: 0.0863 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4508  | total loss: 2.40336 | time: 27.798s
| Adam | epoch: 046 | loss: 2.40336 - acc: 0.0859 | val_loss: 2.39946 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4606  | total loss: 2.40079 | time: 27.696s
| Adam | epoch: 047 | loss: 2.40079 - acc: 0.0854 | val_loss: 2.39831 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4704  | total loss: 2.40176 | time: 27.845s
| Adam | epoch: 048 | loss: 2.40176 - acc: 0.0974 | val_loss: 2.39863 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4802  | total loss: 2.40201 | time: 27.692s
| Adam | epoch: 049 | loss: 2.40201 - acc: 0.0852 | val_loss: 2.39889 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4900  | total loss: 2.40226 | time: 27.826s
| Adam | epoch: 050 | loss: 2.40226 - acc: 0.0808 | val_loss: 2.39828 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 4998  | total loss: 2.40173 | time: 27.780s
| Adam | epoch: 051 | loss: 2.40173 - acc: 0.0947 | val_loss: 2.39951 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5096  | total loss: 2.40111 | time: 27.820s
| Adam | epoch: 052 | loss: 2.40111 - acc: 0.0921 | val_loss: 2.39817 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5194  | total loss: 2.40034 | time: 27.809s
| Adam | epoch: 053 | loss: 2.40034 - acc: 0.0866 | val_loss: 2.39864 - val_acc: 0.0863 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5292  | total loss: 2.40179 | time: 27.768s
| Adam | epoch: 054 | loss: 2.40179 - acc: 0.0809 | val_loss: 2.39880 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5390  | total loss: 2.40283 | time: 27.662s
| Adam | epoch: 055 | loss: 2.40283 - acc: 0.0855 | val_loss: 2.39948 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5488  | total loss: 2.40152 | time: 27.718s
| Adam | epoch: 056 | loss: 2.40152 - acc: 0.0900 | val_loss: 2.39933 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5586  | total loss: 2.39968 | time: 27.557s
| Adam | epoch: 057 | loss: 2.39968 - acc: 0.0922 | val_loss: 2.39883 - val_acc: 0.1014 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5684  | total loss: 2.40118 | time: 27.850s
| Adam | epoch: 058 | loss: 2.40118 - acc: 0.0908 | val_loss: 2.39967 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5782  | total loss: 2.39956 | time: 27.664s
| Adam | epoch: 059 | loss: 2.39956 - acc: 0.0961 | val_loss: 2.39791 - val_acc: 0.0964 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5880  | total loss: 2.40200 | time: 27.720s
| Adam | epoch: 060 | loss: 2.40200 - acc: 0.0811 | val_loss: 2.40031 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 5978  | total loss: 2.40288 | time: 27.923s
| Adam | epoch: 061 | loss: 2.40288 - acc: 0.0823 | val_loss: 2.39726 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6076  | total loss: 2.40192 | time: 27.761s
| Adam | epoch: 062 | loss: 2.40192 - acc: 0.0870 | val_loss: 2.40083 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6174  | total loss: 2.40319 | time: 27.774s
| Adam | epoch: 063 | loss: 2.40319 - acc: 0.0812 | val_loss: 2.39946 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6272  | total loss: 2.40139 | time: 27.711s
| Adam | epoch: 064 | loss: 2.40139 - acc: 0.0875 | val_loss: 2.39906 - val_acc: 0.0930 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6370  | total loss: 2.40423 | time: 27.808s
| Adam | epoch: 065 | loss: 2.40423 - acc: 0.0755 | val_loss: 2.39869 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6468  | total loss: 2.40197 | time: 27.736s
| Adam | epoch: 066 | loss: 2.40197 - acc: 0.0876 | val_loss: 2.39846 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6566  | total loss: 2.40159 | time: 27.667s
| Adam | epoch: 067 | loss: 2.40159 - acc: 0.0712 | val_loss: 2.39892 - val_acc: 0.0771 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6664  | total loss: 2.39972 | time: 27.733s
| Adam | epoch: 068 | loss: 2.39972 - acc: 0.0928 | val_loss: 2.39817 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6762  | total loss: 2.39967 | time: 27.777s
| Adam | epoch: 069 | loss: 2.39967 - acc: 0.0892 | val_loss: 2.39866 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6860  | total loss: 2.40089 | time: 27.741s
| Adam | epoch: 070 | loss: 2.40089 - acc: 0.0875 | val_loss: 2.39821 - val_acc: 0.0930 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 6958  | total loss: 2.39939 | time: 27.840s
| Adam | epoch: 071 | loss: 2.39939 - acc: 0.0981 | val_loss: 2.39871 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7056  | total loss: 2.40018 | time: 28.253s
| Adam | epoch: 072 | loss: 2.40018 - acc: 0.0840 | val_loss: 2.39898 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7154  | total loss: 2.40036 | time: 28.144s
| Adam | epoch: 073 | loss: 2.40036 - acc: 0.0818 | val_loss: 2.39878 - val_acc: 0.0863 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7252  | total loss: 2.40121 | time: 27.774s
| Adam | epoch: 074 | loss: 2.40121 - acc: 0.0925 | val_loss: 2.39800 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7350  | total loss: 2.40169 | time: 27.813s
| Adam | epoch: 075 | loss: 2.40169 - acc: 0.0862 | val_loss: 2.39921 - val_acc: 0.0863 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7448  | total loss: 2.40013 | time: 28.050s
| Adam | epoch: 076 | loss: 2.40013 - acc: 0.0873 | val_loss: 2.39862 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7546  | total loss: 2.40001 | time: 27.775s
| Adam | epoch: 077 | loss: 2.40001 - acc: 0.0931 | val_loss: 2.39824 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7644  | total loss: 2.39927 | time: 27.669s
| Adam | epoch: 078 | loss: 2.39927 - acc: 0.0888 | val_loss: 2.39869 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7742  | total loss: 2.40094 | time: 27.735s
| Adam | epoch: 079 | loss: 2.40094 - acc: 0.0825 | val_loss: 2.39866 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7840  | total loss: 2.39992 | time: 27.844s
| Adam | epoch: 080 | loss: 2.39992 - acc: 0.0936 | val_loss: 2.39901 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 7938  | total loss: 2.40072 | time: 27.666s
| Adam | epoch: 081 | loss: 2.40072 - acc: 0.0832 | val_loss: 2.39783 - val_acc: 0.0922 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8036  | total loss: 2.39921 | time: 27.726s
| Adam | epoch: 082 | loss: 2.39921 - acc: 0.0871 | val_loss: 2.39968 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8134  | total loss: 2.39949 | time: 27.708s
| Adam | epoch: 083 | loss: 2.39949 - acc: 0.0928 | val_loss: 2.39913 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8232  | total loss: 2.40019 | time: 27.816s
| Adam | epoch: 084 | loss: 2.40019 - acc: 0.0918 | val_loss: 2.39893 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8330  | total loss: 2.40003 | time: 27.725s
| Adam | epoch: 085 | loss: 2.40003 - acc: 0.0859 | val_loss: 2.39769 - val_acc: 0.0930 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8428  | total loss: 2.39985 | time: 27.840s
| Adam | epoch: 086 | loss: 2.39985 - acc: 0.0843 | val_loss: 2.39937 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8526  | total loss: 2.39868 | time: 27.643s
| Adam | epoch: 087 | loss: 2.39868 - acc: 0.0942 | val_loss: 2.39839 - val_acc: 0.1014 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8624  | total loss: 2.40031 | time: 27.868s
| Adam | epoch: 088 | loss: 2.40031 - acc: 0.0918 | val_loss: 2.39896 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8722  | total loss: 2.39891 | time: 27.677s
| Adam | epoch: 089 | loss: 2.39891 - acc: 0.0910 | val_loss: 2.40059 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8820  | total loss: 2.39862 | time: 27.809s
| Adam | epoch: 090 | loss: 2.39862 - acc: 0.0928 | val_loss: 2.39963 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 8918  | total loss: 2.40122 | time: 27.879s
| Adam | epoch: 091 | loss: 2.40122 - acc: 0.0876 | val_loss: 2.39834 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9016  | total loss: 2.40108 | time: 27.943s
| Adam | epoch: 092 | loss: 2.40108 - acc: 0.0858 | val_loss: 2.39715 - val_acc: 0.0964 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9114  | total loss: 2.39841 | time: 27.690s
| Adam | epoch: 093 | loss: 2.39841 - acc: 0.0937 | val_loss: 2.39907 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9212  | total loss: 2.39994 | time: 27.831s
| Adam | epoch: 094 | loss: 2.39994 - acc: 0.0900 | val_loss: 2.39849 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9310  | total loss: 2.39925 | time: 27.753s
| Adam | epoch: 095 | loss: 2.39925 - acc: 0.0800 | val_loss: 2.40011 - val_acc: 0.0763 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9408  | total loss: 2.39855 | time: 27.839s
| Adam | epoch: 096 | loss: 2.39855 - acc: 0.0879 | val_loss: 2.39895 - val_acc: 0.0872 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9506  | total loss: 2.40039 | time: 27.675s
| Adam | epoch: 097 | loss: 2.40039 - acc: 0.0829 | val_loss: 2.39807 - val_acc: 0.0863 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9604  | total loss: 2.40005 | time: 27.774s
| Adam | epoch: 098 | loss: 2.40005 - acc: 0.0811 | val_loss: 2.39892 - val_acc: 0.0930 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9702  | total loss: 2.40048 | time: 27.588s
| Adam | epoch: 099 | loss: 2.40048 - acc: 0.0916 | val_loss: 2.39897 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

Training Step: 9800  | total loss: 2.40041 | time: 27.701s
| Adam | epoch: 100 | loss: 2.40041 - acc: 0.0911 | val_loss: 2.39927 - val_acc: 0.0939 -- iter: 10742/10742 

 -------------------------------------------------------------------------------- 

