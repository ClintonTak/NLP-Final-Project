Training Step: 98  | total loss: 2.39523 | time: 33.863s
| Adam | epoch: 001 | loss: 2.39523 - acc: 0.0980 | val_loss: 2.39569 - val_acc: 0.1263 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 196  | total loss: 2.38239 | time: 32.330s
| Adam | epoch: 002 | loss: 2.38239 - acc: 0.1034 | val_loss: 2.37223 - val_acc: 0.1608 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 294  | total loss: 2.33483 | time: 31.653s
| Adam | epoch: 003 | loss: 2.33483 - acc: 0.1403 | val_loss: 2.31162 - val_acc: 0.1557 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 392  | total loss: 2.27185 | time: 31.624s
| Adam | epoch: 004 | loss: 2.27185 - acc: 0.1762 | val_loss: 2.22738 - val_acc: 0.2180 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 490  | total loss: 2.22299 | time: 31.621s
| Adam | epoch: 005 | loss: 2.22299 - acc: 0.1860 | val_loss: 2.15609 - val_acc: 0.2778 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 588  | total loss: 2.18914 | time: 31.634s
| Adam | epoch: 006 | loss: 2.18914 - acc: 0.2097 | val_loss: 2.11753 - val_acc: 0.2643 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 686  | total loss: 2.17235 | time: 31.608s
| Adam | epoch: 007 | loss: 2.17235 - acc: 0.2077 | val_loss: 2.10615 - val_acc: 0.2769 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 784  | total loss: 2.17863 | time: 31.674s
| Adam | epoch: 008 | loss: 2.17863 - acc: 0.2087 | val_loss: 2.10022 - val_acc: 0.2576 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 882  | total loss: 2.14296 | time: 31.659s
| Adam | epoch: 009 | loss: 2.14296 - acc: 0.2286 | val_loss: 2.07473 - val_acc: 0.2753 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 980  | total loss: 2.16691 | time: 31.620s
| Adam | epoch: 010 | loss: 2.16691 - acc: 0.2217 | val_loss: 2.07853 - val_acc: 0.2811 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1078  | total loss: 2.15209 | time: 31.637s
| Adam | epoch: 011 | loss: 2.15209 - acc: 0.2394 | val_loss: 2.05109 - val_acc: 0.3039 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1176  | total loss: 2.13859 | time: 31.652s
| Adam | epoch: 012 | loss: 2.13859 - acc: 0.2252 | val_loss: 2.06198 - val_acc: 0.3013 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1274  | total loss: 2.15925 | time: 31.687s
| Adam | epoch: 013 | loss: 2.15925 - acc: 0.2232 | val_loss: 2.04328 - val_acc: 0.3106 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1372  | total loss: 2.14267 | time: 31.630s
| Adam | epoch: 014 | loss: 2.14267 - acc: 0.2287 | val_loss: 2.03392 - val_acc: 0.3047 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1470  | total loss: 2.11158 | time: 31.653s
| Adam | epoch: 015 | loss: 2.11158 - acc: 0.2439 | val_loss: 2.01751 - val_acc: 0.3157 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1568  | total loss: 2.12064 | time: 31.604s
| Adam | epoch: 016 | loss: 2.12064 - acc: 0.2458 | val_loss: 2.01822 - val_acc: 0.3157 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1666  | total loss: 2.09966 | time: 31.635s
| Adam | epoch: 017 | loss: 2.09966 - acc: 0.2605 | val_loss: 2.01461 - val_acc: 0.3123 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1764  | total loss: 2.08882 | time: 31.610s
| Adam | epoch: 018 | loss: 2.08882 - acc: 0.2509 | val_loss: 2.00545 - val_acc: 0.3173 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1862  | total loss: 2.11895 | time: 31.722s
| Adam | epoch: 019 | loss: 2.11895 - acc: 0.2475 | val_loss: 2.01913 - val_acc: 0.3157 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1960  | total loss: 2.13379 | time: 31.675s
| Adam | epoch: 020 | loss: 2.13379 - acc: 0.2288 | val_loss: 2.03157 - val_acc: 0.3215 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2058  | total loss: 2.13270 | time: 31.684s
| Adam | epoch: 021 | loss: 2.13270 - acc: 0.2260 | val_loss: 2.01459 - val_acc: 0.3241 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2156  | total loss: 2.12992 | time: 31.618s
| Adam | epoch: 022 | loss: 2.12992 - acc: 0.2452 | val_loss: 2.02375 - val_acc: 0.2938 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2254  | total loss: 2.09996 | time: 31.686s
| Adam | epoch: 023 | loss: 2.09996 - acc: 0.2466 | val_loss: 2.00156 - val_acc: 0.3148 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2352  | total loss: 2.12742 | time: 31.605s
| Adam | epoch: 024 | loss: 2.12742 - acc: 0.2419 | val_loss: 2.01503 - val_acc: 0.3190 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2450  | total loss: 2.11954 | time: 31.674s
| Adam | epoch: 025 | loss: 2.11954 - acc: 0.2329 | val_loss: 2.00057 - val_acc: 0.3165 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2548  | total loss: 2.11906 | time: 31.634s
| Adam | epoch: 026 | loss: 2.11906 - acc: 0.2449 | val_loss: 1.99622 - val_acc: 0.3325 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2646  | total loss: 2.13039 | time: 31.645s
| Adam | epoch: 027 | loss: 2.13039 - acc: 0.2361 | val_loss: 1.99952 - val_acc: 0.3199 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2744  | total loss: 2.11451 | time: 31.605s
| Adam | epoch: 028 | loss: 2.11451 - acc: 0.2527 | val_loss: 1.98189 - val_acc: 0.3350 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2842  | total loss: 2.13788 | time: 31.639s
| Adam | epoch: 029 | loss: 2.13788 - acc: 0.2473 | val_loss: 2.00065 - val_acc: 0.3123 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2940  | total loss: 2.15547 | time: 31.652s
| Adam | epoch: 030 | loss: 2.15547 - acc: 0.2311 | val_loss: 1.99989 - val_acc: 0.3249 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3038  | total loss: 2.15296 | time: 31.616s
| Adam | epoch: 031 | loss: 2.15296 - acc: 0.2363 | val_loss: 1.98445 - val_acc: 0.3274 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3136  | total loss: 2.14415 | time: 31.683s
| Adam | epoch: 032 | loss: 2.14415 - acc: 0.2262 | val_loss: 1.98212 - val_acc: 0.3300 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3234  | total loss: 2.06453 | time: 31.611s
| Adam | epoch: 033 | loss: 2.06453 - acc: 0.2648 | val_loss: 1.96768 - val_acc: 0.3384 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3332  | total loss: 2.06247 | time: 31.670s
| Adam | epoch: 034 | loss: 2.06247 - acc: 0.2569 | val_loss: 1.97269 - val_acc: 0.3300 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3430  | total loss: 2.06041 | time: 31.644s
| Adam | epoch: 035 | loss: 2.06041 - acc: 0.2676 | val_loss: 1.96100 - val_acc: 0.3173 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3528  | total loss: 2.08940 | time: 31.626s
| Adam | epoch: 036 | loss: 2.08940 - acc: 0.2644 | val_loss: 1.97692 - val_acc: 0.3325 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3626  | total loss: 2.06009 | time: 31.620s
| Adam | epoch: 037 | loss: 2.06009 - acc: 0.2591 | val_loss: 1.96067 - val_acc: 0.3342 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3724  | total loss: 2.06148 | time: 31.666s
| Adam | epoch: 038 | loss: 2.06148 - acc: 0.2595 | val_loss: 1.96375 - val_acc: 0.3274 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3822  | total loss: 2.06729 | time: 31.592s
| Adam | epoch: 039 | loss: 2.06729 - acc: 0.2550 | val_loss: 1.96668 - val_acc: 0.3232 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3920  | total loss: 2.09569 | time: 31.627s
| Adam | epoch: 040 | loss: 2.09569 - acc: 0.2475 | val_loss: 1.97372 - val_acc: 0.3215 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4018  | total loss: 2.09066 | time: 31.621s
| Adam | epoch: 041 | loss: 2.09066 - acc: 0.2543 | val_loss: 1.97221 - val_acc: 0.3476 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4116  | total loss: 2.06995 | time: 31.615s
| Adam | epoch: 042 | loss: 2.06995 - acc: 0.2592 | val_loss: 1.95647 - val_acc: 0.3418 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4214  | total loss: 2.06847 | time: 31.680s
| Adam | epoch: 043 | loss: 2.06847 - acc: 0.2516 | val_loss: 1.96679 - val_acc: 0.3409 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4312  | total loss: 2.06733 | time: 31.608s
| Adam | epoch: 044 | loss: 2.06733 - acc: 0.2576 | val_loss: 1.96213 - val_acc: 0.3409 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4410  | total loss: 2.06433 | time: 31.655s
| Adam | epoch: 045 | loss: 2.06433 - acc: 0.2577 | val_loss: 1.95237 - val_acc: 0.3384 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4508  | total loss: 2.09322 | time: 31.640s
| Adam | epoch: 046 | loss: 2.09322 - acc: 0.2570 | val_loss: 1.97944 - val_acc: 0.3316 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4606  | total loss: 2.06863 | time: 31.660s
| Adam | epoch: 047 | loss: 2.06863 - acc: 0.2546 | val_loss: 1.96011 - val_acc: 0.3451 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4704  | total loss: 2.05276 | time: 31.636s
| Adam | epoch: 048 | loss: 2.05276 - acc: 0.2586 | val_loss: 1.95136 - val_acc: 0.3359 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4802  | total loss: 2.06659 | time: 31.623s
| Adam | epoch: 049 | loss: 2.06659 - acc: 0.2533 | val_loss: 1.95796 - val_acc: 0.3418 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4900  | total loss: 2.08193 | time: 31.636s
| Adam | epoch: 050 | loss: 2.08193 - acc: 0.2644 | val_loss: 1.96560 - val_acc: 0.3316 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4998  | total loss: 2.05323 | time: 31.684s
| Adam | epoch: 051 | loss: 2.05323 - acc: 0.2616 | val_loss: 1.95650 - val_acc: 0.3333 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5096  | total loss: 2.07798 | time: 31.588s
| Adam | epoch: 052 | loss: 2.07798 - acc: 0.2529 | val_loss: 1.95760 - val_acc: 0.3476 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5194  | total loss: 2.07151 | time: 31.628s
| Adam | epoch: 053 | loss: 2.07151 - acc: 0.2556 | val_loss: 1.96144 - val_acc: 0.3367 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5292  | total loss: 2.04938 | time: 31.644s
| Adam | epoch: 054 | loss: 2.04938 - acc: 0.2592 | val_loss: 1.95383 - val_acc: 0.3392 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5390  | total loss: 2.07905 | time: 31.614s
| Adam | epoch: 055 | loss: 2.07905 - acc: 0.2516 | val_loss: 1.96436 - val_acc: 0.3519 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5488  | total loss: 2.04935 | time: 31.633s
| Adam | epoch: 056 | loss: 2.04935 - acc: 0.2753 | val_loss: 1.94810 - val_acc: 0.3502 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5586  | total loss: 2.04261 | time: 31.675s
| Adam | epoch: 057 | loss: 2.04261 - acc: 0.2500 | val_loss: 1.94676 - val_acc: 0.3418 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5684  | total loss: 2.04381 | time: 31.645s
| Adam | epoch: 058 | loss: 2.04381 - acc: 0.2780 | val_loss: 1.95339 - val_acc: 0.3367 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5782  | total loss: 2.05874 | time: 32.483s
| Adam | epoch: 059 | loss: 2.05874 - acc: 0.2515 | val_loss: 1.94997 - val_acc: 0.3418 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5880  | total loss: 2.06195 | time: 31.649s
| Adam | epoch: 060 | loss: 2.06195 - acc: 0.2640 | val_loss: 1.94744 - val_acc: 0.3460 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5978  | total loss: 2.05445 | time: 31.663s
| Adam | epoch: 061 | loss: 2.05445 - acc: 0.2472 | val_loss: 1.95116 - val_acc: 0.3561 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6076  | total loss: 2.08322 | time: 31.625s
| Adam | epoch: 062 | loss: 2.08322 - acc: 0.2637 | val_loss: 1.94790 - val_acc: 0.3527 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6174  | total loss: 2.05546 | time: 31.608s
| Adam | epoch: 063 | loss: 2.05546 - acc: 0.2738 | val_loss: 1.95426 - val_acc: 0.3493 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6272  | total loss: 2.07342 | time: 31.646s
| Adam | epoch: 064 | loss: 2.07342 - acc: 0.2537 | val_loss: 1.95747 - val_acc: 0.3401 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6370  | total loss: 2.07384 | time: 31.641s
| Adam | epoch: 065 | loss: 2.07384 - acc: 0.2586 | val_loss: 1.95413 - val_acc: 0.3510 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6468  | total loss: 2.04681 | time: 31.682s
| Adam | epoch: 066 | loss: 2.04681 - acc: 0.2623 | val_loss: 1.94108 - val_acc: 0.3392 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6566  | total loss: 2.05385 | time: 31.639s
| Adam | epoch: 067 | loss: 2.05385 - acc: 0.2618 | val_loss: 1.94920 - val_acc: 0.3291 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6664  | total loss: 2.03921 | time: 31.646s
| Adam | epoch: 068 | loss: 2.03921 - acc: 0.2675 | val_loss: 1.94170 - val_acc: 0.3409 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6762  | total loss: 2.05110 | time: 31.638s
| Adam | epoch: 069 | loss: 2.05110 - acc: 0.2742 | val_loss: 1.94876 - val_acc: 0.3418 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6860  | total loss: 2.05359 | time: 31.694s
| Adam | epoch: 070 | loss: 2.05359 - acc: 0.2706 | val_loss: 1.95337 - val_acc: 0.3367 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6958  | total loss: 2.06760 | time: 31.597s
| Adam | epoch: 071 | loss: 2.06760 - acc: 0.2499 | val_loss: 1.94200 - val_acc: 0.3367 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7056  | total loss: 2.07871 | time: 31.610s
| Adam | epoch: 072 | loss: 2.07871 - acc: 0.2536 | val_loss: 1.95062 - val_acc: 0.3409 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7154  | total loss: 2.04567 | time: 31.630s
| Adam | epoch: 073 | loss: 2.04567 - acc: 0.2429 | val_loss: 1.94581 - val_acc: 0.3367 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7252  | total loss: 2.04830 | time: 31.609s
| Adam | epoch: 074 | loss: 2.04830 - acc: 0.2663 | val_loss: 1.95700 - val_acc: 0.3342 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7350  | total loss: 2.04352 | time: 31.576s
| Adam | epoch: 075 | loss: 2.04352 - acc: 0.2660 | val_loss: 1.93851 - val_acc: 0.3409 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7448  | total loss: 2.03804 | time: 31.649s
| Adam | epoch: 076 | loss: 2.03804 - acc: 0.2760 | val_loss: 1.93995 - val_acc: 0.3333 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7546  | total loss: 2.06804 | time: 31.599s
| Adam | epoch: 077 | loss: 2.06804 - acc: 0.2762 | val_loss: 1.94497 - val_acc: 0.3460 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7644  | total loss: 2.05827 | time: 31.651s
| Adam | epoch: 078 | loss: 2.05827 - acc: 0.2592 | val_loss: 1.93317 - val_acc: 0.3552 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7742  | total loss: 2.05316 | time: 31.631s
| Adam | epoch: 079 | loss: 2.05316 - acc: 0.2815 | val_loss: 1.95132 - val_acc: 0.3527 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7840  | total loss: 2.05255 | time: 31.621s
| Adam | epoch: 080 | loss: 2.05255 - acc: 0.2712 | val_loss: 1.94392 - val_acc: 0.3375 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7938  | total loss: 2.02327 | time: 31.644s
| Adam | epoch: 081 | loss: 2.02327 - acc: 0.2887 | val_loss: 1.94278 - val_acc: 0.3350 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8036  | total loss: 2.04086 | time: 31.615s
| Adam | epoch: 082 | loss: 2.04086 - acc: 0.2775 | val_loss: 1.93803 - val_acc: 0.3291 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8134  | total loss: 2.04461 | time: 31.627s
| Adam | epoch: 083 | loss: 2.04461 - acc: 0.2596 | val_loss: 1.94193 - val_acc: 0.3443 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8232  | total loss: 2.04480 | time: 31.656s
| Adam | epoch: 084 | loss: 2.04480 - acc: 0.2644 | val_loss: 1.94869 - val_acc: 0.3241 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8330  | total loss: 2.05522 | time: 31.640s
| Adam | epoch: 085 | loss: 2.05522 - acc: 0.2654 | val_loss: 1.94107 - val_acc: 0.3367 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8428  | total loss: 2.03793 | time: 31.623s
| Adam | epoch: 086 | loss: 2.03793 - acc: 0.2684 | val_loss: 1.93518 - val_acc: 0.3443 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8526  | total loss: 2.04442 | time: 31.686s
| Adam | epoch: 087 | loss: 2.04442 - acc: 0.2720 | val_loss: 1.94801 - val_acc: 0.3460 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8624  | total loss: 2.04353 | time: 31.602s
| Adam | epoch: 088 | loss: 2.04353 - acc: 0.2716 | val_loss: 1.93702 - val_acc: 0.3443 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8722  | total loss: 2.02037 | time: 31.570s
| Adam | epoch: 089 | loss: 2.02037 - acc: 0.2748 | val_loss: 1.93208 - val_acc: 0.3502 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8820  | total loss: 2.08395 | time: 31.654s
| Adam | epoch: 090 | loss: 2.08395 - acc: 0.2653 | val_loss: 1.95220 - val_acc: 0.3325 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8918  | total loss: 2.02285 | time: 31.657s
| Adam | epoch: 091 | loss: 2.02285 - acc: 0.2701 | val_loss: 1.93477 - val_acc: 0.3409 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9016  | total loss: 2.05301 | time: 31.633s
| Adam | epoch: 092 | loss: 2.05301 - acc: 0.2636 | val_loss: 1.93459 - val_acc: 0.3359 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9114  | total loss: 2.06766 | time: 32.106s
| Adam | epoch: 093 | loss: 2.06766 - acc: 0.2662 | val_loss: 1.94666 - val_acc: 0.3426 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9212  | total loss: 2.01955 | time: 31.604s
| Adam | epoch: 094 | loss: 2.01955 - acc: 0.2796 | val_loss: 1.93566 - val_acc: 0.3325 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9310  | total loss: 2.03553 | time: 31.646s
| Adam | epoch: 095 | loss: 2.03553 - acc: 0.2817 | val_loss: 1.93456 - val_acc: 0.3367 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9408  | total loss: 2.01726 | time: 31.596s
| Adam | epoch: 096 | loss: 2.01726 - acc: 0.2887 | val_loss: 1.93802 - val_acc: 0.3316 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9506  | total loss: 2.00410 | time: 31.650s
| Adam | epoch: 097 | loss: 2.00410 - acc: 0.2810 | val_loss: 1.92809 - val_acc: 0.3325 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9604  | total loss: 2.04471 | time: 31.589s
| Adam | epoch: 098 | loss: 2.04471 - acc: 0.2722 | val_loss: 1.92772 - val_acc: 0.3392 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9702  | total loss: 1.98338 | time: 31.628s
| Adam | epoch: 099 | loss: 1.98338 - acc: 0.2606 | val_loss: 1.91839 - val_acc: 0.3367 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9800  | total loss: 2.02496 | time: 31.856s
| Adam | epoch: 100 | loss: 2.02496 - acc: 0.2729 | val_loss: 1.93472 - val_acc: 0.3409 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

