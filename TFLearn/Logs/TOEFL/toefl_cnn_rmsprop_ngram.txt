Training Step: 98  | total loss: 2.39819 | time: 32.989s
| RMSProp | epoch: 001 | loss: 2.39819 - acc: 0.0883 | val_loss: 2.39854 - val_acc: 0.0842 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 196  | total loss: 2.39800 | time: 31.864s
| RMSProp | epoch: 002 | loss: 2.39800 - acc: 0.0917 | val_loss: 2.39701 - val_acc: 0.1094 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 294  | total loss: 2.38878 | time: 31.638s
| RMSProp | epoch: 003 | loss: 2.38878 - acc: 0.1134 | val_loss: 2.38544 - val_acc: 0.1355 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 392  | total loss: 2.34509 | time: 31.634s
| RMSProp | epoch: 004 | loss: 2.34509 - acc: 0.1444 | val_loss: 2.33074 - val_acc: 0.1793 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 490  | total loss: 2.30256 | time: 31.681s
| RMSProp | epoch: 005 | loss: 2.30256 - acc: 0.1627 | val_loss: 2.28549 - val_acc: 0.1835 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 588  | total loss: 2.25948 | time: 31.628s
| RMSProp | epoch: 006 | loss: 2.25948 - acc: 0.1807 | val_loss: 2.23054 - val_acc: 0.2079 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 686  | total loss: 2.21885 | time: 31.708s
| RMSProp | epoch: 007 | loss: 2.21885 - acc: 0.1999 | val_loss: 2.18567 - val_acc: 0.2273 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 784  | total loss: 2.19550 | time: 31.660s
| RMSProp | epoch: 008 | loss: 2.19550 - acc: 0.2063 | val_loss: 2.14843 - val_acc: 0.2382 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 882  | total loss: 2.18722 | time: 31.654s
| RMSProp | epoch: 009 | loss: 2.18722 - acc: 0.2062 | val_loss: 2.13106 - val_acc: 0.2441 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 980  | total loss: 2.16296 | time: 31.641s
| RMSProp | epoch: 010 | loss: 2.16296 - acc: 0.2293 | val_loss: 2.11000 - val_acc: 0.2475 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1078  | total loss: 2.16012 | time: 31.654s
| RMSProp | epoch: 011 | loss: 2.16012 - acc: 0.2232 | val_loss: 2.10061 - val_acc: 0.2618 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1176  | total loss: 2.16179 | time: 31.677s
| RMSProp | epoch: 012 | loss: 2.16179 - acc: 0.2246 | val_loss: 2.09644 - val_acc: 0.2551 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1274  | total loss: 2.15181 | time: 31.662s
| RMSProp | epoch: 013 | loss: 2.15181 - acc: 0.2336 | val_loss: 2.07982 - val_acc: 0.2677 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1372  | total loss: 2.14080 | time: 31.635s
| RMSProp | epoch: 014 | loss: 2.14080 - acc: 0.2404 | val_loss: 2.06870 - val_acc: 0.2719 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1470  | total loss: 2.12449 | time: 31.631s
| RMSProp | epoch: 015 | loss: 2.12449 - acc: 0.2512 | val_loss: 2.07072 - val_acc: 0.2508 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1568  | total loss: 2.11205 | time: 31.623s
| RMSProp | epoch: 016 | loss: 2.11205 - acc: 0.2387 | val_loss: 2.05174 - val_acc: 0.2736 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1666  | total loss: 2.15065 | time: 31.606s
| RMSProp | epoch: 017 | loss: 2.15065 - acc: 0.2315 | val_loss: 2.05570 - val_acc: 0.2769 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1764  | total loss: 2.13386 | time: 31.932s
| RMSProp | epoch: 018 | loss: 2.13386 - acc: 0.2461 | val_loss: 2.04751 - val_acc: 0.2753 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1862  | total loss: 2.10851 | time: 35.272s
| RMSProp | epoch: 019 | loss: 2.10851 - acc: 0.2520 | val_loss: 2.04710 - val_acc: 0.2685 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 1960  | total loss: 2.14821 | time: 32.020s
| RMSProp | epoch: 020 | loss: 2.14821 - acc: 0.2261 | val_loss: 2.04614 - val_acc: 0.2769 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2058  | total loss: 2.09863 | time: 31.890s
| RMSProp | epoch: 021 | loss: 2.09863 - acc: 0.2457 | val_loss: 2.02624 - val_acc: 0.2845 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2156  | total loss: 2.13298 | time: 32.243s
| RMSProp | epoch: 022 | loss: 2.13298 - acc: 0.2369 | val_loss: 2.02839 - val_acc: 0.2719 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2254  | total loss: 2.12298 | time: 32.433s
| RMSProp | epoch: 023 | loss: 2.12298 - acc: 0.2486 | val_loss: 2.02708 - val_acc: 0.2921 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2352  | total loss: 2.13023 | time: 32.238s
| RMSProp | epoch: 024 | loss: 2.13023 - acc: 0.2489 | val_loss: 2.01515 - val_acc: 0.2955 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2450  | total loss: 2.12765 | time: 32.065s
| RMSProp | epoch: 025 | loss: 2.12765 - acc: 0.2450 | val_loss: 2.01742 - val_acc: 0.3013 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2548  | total loss: 2.11156 | time: 32.038s
| RMSProp | epoch: 026 | loss: 2.11156 - acc: 0.2454 | val_loss: 2.01967 - val_acc: 0.2929 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2646  | total loss: 2.13741 | time: 31.835s
| RMSProp | epoch: 027 | loss: 2.13741 - acc: 0.2361 | val_loss: 2.01938 - val_acc: 0.3056 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2744  | total loss: 2.13424 | time: 31.998s
| RMSProp | epoch: 028 | loss: 2.13424 - acc: 0.2491 | val_loss: 2.01912 - val_acc: 0.3022 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2842  | total loss: 2.13356 | time: 31.864s
| RMSProp | epoch: 029 | loss: 2.13356 - acc: 0.2444 | val_loss: 2.02098 - val_acc: 0.2955 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 2940  | total loss: 2.14811 | time: 32.472s
| RMSProp | epoch: 030 | loss: 2.14811 - acc: 0.2388 | val_loss: 2.02038 - val_acc: 0.2837 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3038  | total loss: 2.17188 | time: 32.462s
| RMSProp | epoch: 031 | loss: 2.17188 - acc: 0.2220 | val_loss: 2.04123 - val_acc: 0.2997 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3136  | total loss: 2.13708 | time: 32.779s
| RMSProp | epoch: 032 | loss: 2.13708 - acc: 0.2427 | val_loss: 2.02414 - val_acc: 0.2870 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3234  | total loss: 2.11186 | time: 32.368s
| RMSProp | epoch: 033 | loss: 2.11186 - acc: 0.2371 | val_loss: 2.04043 - val_acc: 0.3022 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3332  | total loss: 2.07317 | time: 32.383s
| RMSProp | epoch: 034 | loss: 2.07317 - acc: 0.2473 | val_loss: 2.00279 - val_acc: 0.2896 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3430  | total loss: 2.07282 | time: 33.140s
| RMSProp | epoch: 035 | loss: 2.07282 - acc: 0.2575 | val_loss: 1.98933 - val_acc: 0.2946 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3528  | total loss: 2.09524 | time: 33.779s
| RMSProp | epoch: 036 | loss: 2.09524 - acc: 0.2562 | val_loss: 1.99526 - val_acc: 0.2845 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3626  | total loss: 2.06111 | time: 34.464s
| RMSProp | epoch: 037 | loss: 2.06111 - acc: 0.2636 | val_loss: 1.97894 - val_acc: 0.2845 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3724  | total loss: 2.06716 | time: 33.134s
| RMSProp | epoch: 038 | loss: 2.06716 - acc: 0.2523 | val_loss: 1.99564 - val_acc: 0.2845 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3822  | total loss: 2.09449 | time: 34.420s
| RMSProp | epoch: 039 | loss: 2.09449 - acc: 0.2574 | val_loss: 1.97944 - val_acc: 0.2963 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 3920  | total loss: 2.10533 | time: 33.322s
| RMSProp | epoch: 040 | loss: 2.10533 - acc: 0.2549 | val_loss: 2.00097 - val_acc: 0.3072 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4018  | total loss: 2.08534 | time: 32.346s
| RMSProp | epoch: 041 | loss: 2.08534 - acc: 0.2585 | val_loss: 1.98647 - val_acc: 0.2980 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4116  | total loss: 2.10192 | time: 32.508s
| RMSProp | epoch: 042 | loss: 2.10192 - acc: 0.2445 | val_loss: 1.99291 - val_acc: 0.3039 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4214  | total loss: 2.08800 | time: 32.860s
| RMSProp | epoch: 043 | loss: 2.08800 - acc: 0.2431 | val_loss: 1.99077 - val_acc: 0.2963 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4312  | total loss: 2.08829 | time: 32.857s
| RMSProp | epoch: 044 | loss: 2.08829 - acc: 0.2499 | val_loss: 1.98806 - val_acc: 0.2879 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4410  | total loss: 2.10253 | time: 32.662s
| RMSProp | epoch: 045 | loss: 2.10253 - acc: 0.2439 | val_loss: 1.98927 - val_acc: 0.2921 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4508  | total loss: 2.08108 | time: 32.666s
| RMSProp | epoch: 046 | loss: 2.08108 - acc: 0.2555 | val_loss: 1.97204 - val_acc: 0.3140 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4606  | total loss: 2.09438 | time: 33.321s
| RMSProp | epoch: 047 | loss: 2.09438 - acc: 0.2520 | val_loss: 2.01041 - val_acc: 0.2896 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4704  | total loss: 2.05564 | time: 32.070s
| RMSProp | epoch: 048 | loss: 2.05564 - acc: 0.2674 | val_loss: 1.97355 - val_acc: 0.3106 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4802  | total loss: 2.06446 | time: 32.101s
| RMSProp | epoch: 049 | loss: 2.06446 - acc: 0.2683 | val_loss: 1.98721 - val_acc: 0.2997 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4900  | total loss: 2.08753 | time: 32.178s
| RMSProp | epoch: 050 | loss: 2.08753 - acc: 0.2323 | val_loss: 1.98040 - val_acc: 0.3199 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 4998  | total loss: 2.04507 | time: 32.097s
| RMSProp | epoch: 051 | loss: 2.04507 - acc: 0.2671 | val_loss: 1.97281 - val_acc: 0.3047 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5096  | total loss: 2.09672 | time: 32.089s
| RMSProp | epoch: 052 | loss: 2.09672 - acc: 0.2547 | val_loss: 1.97438 - val_acc: 0.3106 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5194  | total loss: 2.03862 | time: 32.448s
| RMSProp | epoch: 053 | loss: 2.03862 - acc: 0.2718 | val_loss: 1.98540 - val_acc: 0.3022 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5292  | total loss: 2.06999 | time: 32.217s
| RMSProp | epoch: 054 | loss: 2.06999 - acc: 0.2547 | val_loss: 1.96787 - val_acc: 0.3072 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5390  | total loss: 2.05270 | time: 32.585s
| RMSProp | epoch: 055 | loss: 2.05270 - acc: 0.2773 | val_loss: 1.96910 - val_acc: 0.3182 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5488  | total loss: 2.06359 | time: 32.675s
| RMSProp | epoch: 056 | loss: 2.06359 - acc: 0.2701 | val_loss: 1.96857 - val_acc: 0.3056 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5586  | total loss: 2.08785 | time: 32.182s
| RMSProp | epoch: 057 | loss: 2.08785 - acc: 0.2552 | val_loss: 1.97405 - val_acc: 0.3072 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5684  | total loss: 2.08296 | time: 32.834s
| RMSProp | epoch: 058 | loss: 2.08296 - acc: 0.2515 | val_loss: 1.98216 - val_acc: 0.2980 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5782  | total loss: 2.07824 | time: 32.446s
| RMSProp | epoch: 059 | loss: 2.07824 - acc: 0.2560 | val_loss: 1.97049 - val_acc: 0.3081 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5880  | total loss: 2.05848 | time: 33.349s
| RMSProp | epoch: 060 | loss: 2.05848 - acc: 0.2559 | val_loss: 1.97102 - val_acc: 0.3089 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 5978  | total loss: 2.04853 | time: 33.837s
| RMSProp | epoch: 061 | loss: 2.04853 - acc: 0.2591 | val_loss: 1.96778 - val_acc: 0.3190 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6076  | total loss: 2.07882 | time: 33.965s
| RMSProp | epoch: 062 | loss: 2.07882 - acc: 0.2491 | val_loss: 1.97158 - val_acc: 0.3140 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6174  | total loss: 2.07539 | time: 33.831s
| RMSProp | epoch: 063 | loss: 2.07539 - acc: 0.2670 | val_loss: 1.97237 - val_acc: 0.3207 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6272  | total loss: 2.06063 | time: 34.443s
| RMSProp | epoch: 064 | loss: 2.06063 - acc: 0.2695 | val_loss: 1.97173 - val_acc: 0.3199 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6370  | total loss: 2.05402 | time: 32.739s
| RMSProp | epoch: 065 | loss: 2.05402 - acc: 0.2576 | val_loss: 1.97042 - val_acc: 0.3224 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6468  | total loss: 2.07659 | time: 31.779s
| RMSProp | epoch: 066 | loss: 2.07659 - acc: 0.2598 | val_loss: 1.96575 - val_acc: 0.2963 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6566  | total loss: 2.06866 | time: 32.572s
| RMSProp | epoch: 067 | loss: 2.06866 - acc: 0.2484 | val_loss: 1.96499 - val_acc: 0.3013 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6664  | total loss: 2.09513 | time: 32.918s
| RMSProp | epoch: 068 | loss: 2.09513 - acc: 0.2512 | val_loss: 1.98342 - val_acc: 0.3123 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6762  | total loss: 2.06928 | time: 32.711s
| RMSProp | epoch: 069 | loss: 2.06928 - acc: 0.2628 | val_loss: 1.96743 - val_acc: 0.3241 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6860  | total loss: 2.06746 | time: 32.466s
| RMSProp | epoch: 070 | loss: 2.06746 - acc: 0.2736 | val_loss: 1.96472 - val_acc: 0.3224 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 6958  | total loss: 2.04254 | time: 32.488s
| RMSProp | epoch: 071 | loss: 2.04254 - acc: 0.2616 | val_loss: 1.95013 - val_acc: 0.3165 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7056  | total loss: 2.05415 | time: 31.865s
| RMSProp | epoch: 072 | loss: 2.05415 - acc: 0.2734 | val_loss: 1.96218 - val_acc: 0.3013 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7154  | total loss: 2.07890 | time: 32.228s
| RMSProp | epoch: 073 | loss: 2.07890 - acc: 0.2557 | val_loss: 1.96132 - val_acc: 0.3232 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7252  | total loss: 2.05154 | time: 33.319s
| RMSProp | epoch: 074 | loss: 2.05154 - acc: 0.2614 | val_loss: 1.96177 - val_acc: 0.3047 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7350  | total loss: 2.06414 | time: 32.426s
| RMSProp | epoch: 075 | loss: 2.06414 - acc: 0.2738 | val_loss: 1.96077 - val_acc: 0.3098 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7448  | total loss: 2.06952 | time: 31.806s
| RMSProp | epoch: 076 | loss: 2.06952 - acc: 0.2615 | val_loss: 1.95050 - val_acc: 0.3123 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7546  | total loss: 2.04856 | time: 31.791s
| RMSProp | epoch: 077 | loss: 2.04856 - acc: 0.2754 | val_loss: 1.95040 - val_acc: 0.3291 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7644  | total loss: 2.04048 | time: 31.832s
| RMSProp | epoch: 078 | loss: 2.04048 - acc: 0.2704 | val_loss: 1.94915 - val_acc: 0.3283 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7742  | total loss: 2.05653 | time: 32.879s
| RMSProp | epoch: 079 | loss: 2.05653 - acc: 0.2712 | val_loss: 1.96043 - val_acc: 0.3241 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7840  | total loss: 2.06527 | time: 33.038s
| RMSProp | epoch: 080 | loss: 2.06527 - acc: 0.2567 | val_loss: 1.96006 - val_acc: 0.3157 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 7938  | total loss: 2.08265 | time: 32.300s
| RMSProp | epoch: 081 | loss: 2.08265 - acc: 0.2511 | val_loss: 1.95699 - val_acc: 0.3047 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8036  | total loss: 2.06019 | time: 32.836s
| RMSProp | epoch: 082 | loss: 2.06019 - acc: 0.2733 | val_loss: 1.95499 - val_acc: 0.3384 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8134  | total loss: 2.08414 | time: 32.860s
| RMSProp | epoch: 083 | loss: 2.08414 - acc: 0.2553 | val_loss: 1.97597 - val_acc: 0.3258 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8232  | total loss: 2.06548 | time: 32.678s
| RMSProp | epoch: 084 | loss: 2.06548 - acc: 0.2669 | val_loss: 1.96375 - val_acc: 0.3308 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8330  | total loss: 2.04395 | time: 32.887s
| RMSProp | epoch: 085 | loss: 2.04395 - acc: 0.2632 | val_loss: 1.95260 - val_acc: 0.3081 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8428  | total loss: 2.06688 | time: 33.728s
| RMSProp | epoch: 086 | loss: 2.06688 - acc: 0.2650 | val_loss: 1.95883 - val_acc: 0.3258 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8526  | total loss: 2.00478 | time: 32.667s
| RMSProp | epoch: 087 | loss: 2.00478 - acc: 0.2860 | val_loss: 1.95633 - val_acc: 0.3232 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8624  | total loss: 2.09292 | time: 31.660s
| RMSProp | epoch: 088 | loss: 2.09292 - acc: 0.2531 | val_loss: 1.96870 - val_acc: 0.3064 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8722  | total loss: 2.05235 | time: 31.646s
| RMSProp | epoch: 089 | loss: 2.05235 - acc: 0.2742 | val_loss: 1.95278 - val_acc: 0.3157 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8820  | total loss: 2.07829 | time: 31.670s
| RMSProp | epoch: 090 | loss: 2.07829 - acc: 0.2621 | val_loss: 1.95402 - val_acc: 0.3291 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 8918  | total loss: 2.07525 | time: 31.621s
| RMSProp | epoch: 091 | loss: 2.07525 - acc: 0.2516 | val_loss: 1.95745 - val_acc: 0.3199 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9016  | total loss: 2.07403 | time: 31.643s
| RMSProp | epoch: 092 | loss: 2.07403 - acc: 0.2545 | val_loss: 1.96003 - val_acc: 0.3316 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9114  | total loss: 2.09563 | time: 31.639s
| RMSProp | epoch: 093 | loss: 2.09563 - acc: 0.2782 | val_loss: 1.96652 - val_acc: 0.3173 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9212  | total loss: 2.07180 | time: 31.607s
| RMSProp | epoch: 094 | loss: 2.07180 - acc: 0.2614 | val_loss: 1.96337 - val_acc: 0.3072 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9310  | total loss: 2.06268 | time: 31.722s
| RMSProp | epoch: 095 | loss: 2.06268 - acc: 0.2750 | val_loss: 1.95754 - val_acc: 0.3308 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9408  | total loss: 2.03061 | time: 31.684s
| RMSProp | epoch: 096 | loss: 2.03061 - acc: 0.2692 | val_loss: 1.96739 - val_acc: 0.3165 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9506  | total loss: 2.09722 | time: 31.728s
| RMSProp | epoch: 097 | loss: 2.09722 - acc: 0.2272 | val_loss: 1.97936 - val_acc: 0.3173 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9604  | total loss: 2.07302 | time: 31.682s
| RMSProp | epoch: 098 | loss: 2.07302 - acc: 0.2680 | val_loss: 1.98952 - val_acc: 0.3098 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9702  | total loss: 2.09964 | time: 31.626s
| RMSProp | epoch: 099 | loss: 2.09964 - acc: 0.2527 | val_loss: 1.95325 - val_acc: 0.3190 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

Training Step: 9800  | total loss: 2.06271 | time: 31.939s
| RMSProp | epoch: 100 | loss: 2.06271 - acc: 0.2509 | val_loss: 1.95925 - val_acc: 0.3316 -- iter: 10698/10698 

 -------------------------------------------------------------------------------- 

