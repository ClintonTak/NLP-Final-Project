Training Step: 58  | total loss: 1.63871 | time: 57.768s
| Adam | epoch: 001 | loss: 1.63871 - acc: 0.3200 | val_loss: 1.60787 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 116  | total loss: 1.61736 | time: 12.957s
| Adam | epoch: 002 | loss: 1.61736 - acc: 0.3435 | val_loss: 1.60588 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 174  | total loss: 1.58309 | time: 12.925s
| Adam | epoch: 003 | loss: 1.58309 - acc: 0.3698 | val_loss: 1.60603 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 232  | total loss: 1.61752 | time: 12.924s
| Adam | epoch: 004 | loss: 1.61752 - acc: 0.3366 | val_loss: 1.60915 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 290  | total loss: 1.60996 | time: 12.937s
| Adam | epoch: 005 | loss: 1.60996 - acc: 0.3458 | val_loss: 1.60735 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 348  | total loss: 1.60927 | time: 12.959s
| Adam | epoch: 006 | loss: 1.60927 - acc: 0.3513 | val_loss: 1.60747 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 406  | total loss: 1.59452 | time: 12.887s
| Adam | epoch: 007 | loss: 1.59452 - acc: 0.3495 | val_loss: 1.60744 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 464  | total loss: 1.60098 | time: 12.939s
| Adam | epoch: 008 | loss: 1.60098 - acc: 0.3469 | val_loss: 1.60688 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 522  | total loss: 1.58627 | time: 12.935s
| Adam | epoch: 009 | loss: 1.58627 - acc: 0.3385 | val_loss: 1.60734 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 580  | total loss: 1.58924 | time: 12.954s
| Adam | epoch: 010 | loss: 1.58924 - acc: 0.3533 | val_loss: 1.60886 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 638  | total loss: 1.59055 | time: 12.956s
| Adam | epoch: 011 | loss: 1.59055 - acc: 0.3519 | val_loss: 1.60600 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 696  | total loss: 1.61088 | time: 12.970s
| Adam | epoch: 012 | loss: 1.61088 - acc: 0.3521 | val_loss: 1.60962 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 754  | total loss: 1.62094 | time: 12.966s
| Adam | epoch: 013 | loss: 1.62094 - acc: 0.3316 | val_loss: 1.60909 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 812  | total loss: 1.59522 | time: 12.870s
| Adam | epoch: 014 | loss: 1.59522 - acc: 0.3524 | val_loss: 1.60747 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 870  | total loss: 1.60312 | time: 12.961s
| Adam | epoch: 015 | loss: 1.60312 - acc: 0.3427 | val_loss: 1.60751 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 928  | total loss: 1.60610 | time: 12.982s
| Adam | epoch: 016 | loss: 1.60610 - acc: 0.3660 | val_loss: 1.61103 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 986  | total loss: 1.57332 | time: 12.979s
| Adam | epoch: 017 | loss: 1.57332 - acc: 0.3600 | val_loss: 1.60818 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1044  | total loss: 1.61621 | time: 12.921s
| Adam | epoch: 018 | loss: 1.61621 - acc: 0.3393 | val_loss: 1.60903 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1102  | total loss: 1.63011 | time: 12.996s
| Adam | epoch: 019 | loss: 1.63011 - acc: 0.3380 | val_loss: 1.60785 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1160  | total loss: 1.61022 | time: 12.950s
| Adam | epoch: 020 | loss: 1.61022 - acc: 0.3414 | val_loss: 1.60597 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1218  | total loss: 1.61439 | time: 12.953s
| Adam | epoch: 021 | loss: 1.61439 - acc: 0.3485 | val_loss: 1.60887 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1276  | total loss: 1.60287 | time: 12.977s
| Adam | epoch: 022 | loss: 1.60287 - acc: 0.3708 | val_loss: 1.60573 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1334  | total loss: 1.63269 | time: 12.961s
| Adam | epoch: 023 | loss: 1.63269 - acc: 0.3317 | val_loss: 1.60774 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1392  | total loss: 1.63039 | time: 12.949s
| Adam | epoch: 024 | loss: 1.63039 - acc: 0.3356 | val_loss: 1.60623 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1450  | total loss: 1.59360 | time: 12.967s
| Adam | epoch: 025 | loss: 1.59360 - acc: 0.3447 | val_loss: 1.60790 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1508  | total loss: 1.60217 | time: 12.968s
| Adam | epoch: 026 | loss: 1.60217 - acc: 0.3469 | val_loss: 1.60492 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1566  | total loss: 1.61065 | time: 12.971s
| Adam | epoch: 027 | loss: 1.61065 - acc: 0.3386 | val_loss: 1.60547 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1624  | total loss: 1.61456 | time: 12.976s
| Adam | epoch: 028 | loss: 1.61456 - acc: 0.3479 | val_loss: 1.60819 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1682  | total loss: 1.59001 | time: 12.967s
| Adam | epoch: 029 | loss: 1.59001 - acc: 0.3645 | val_loss: 1.60683 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1740  | total loss: 1.62854 | time: 12.954s
| Adam | epoch: 030 | loss: 1.62854 - acc: 0.3210 | val_loss: 1.60723 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1798  | total loss: 1.60907 | time: 12.928s
| Adam | epoch: 031 | loss: 1.60907 - acc: 0.3434 | val_loss: 1.60681 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1856  | total loss: 1.60688 | time: 12.918s
| Adam | epoch: 032 | loss: 1.60688 - acc: 0.3421 | val_loss: 1.60890 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1914  | total loss: 1.61722 | time: 12.935s
| Adam | epoch: 033 | loss: 1.61722 - acc: 0.3287 | val_loss: 1.60585 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1972  | total loss: 1.58292 | time: 12.913s
| Adam | epoch: 034 | loss: 1.58292 - acc: 0.3623 | val_loss: 1.61071 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2030  | total loss: 1.61205 | time: 12.985s
| Adam | epoch: 035 | loss: 1.61205 - acc: 0.3317 | val_loss: 1.60573 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2088  | total loss: 1.60708 | time: 12.960s
| Adam | epoch: 036 | loss: 1.60708 - acc: 0.3483 | val_loss: 1.60664 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2146  | total loss: 1.60270 | time: 12.997s
| Adam | epoch: 037 | loss: 1.60270 - acc: 0.3445 | val_loss: 1.60769 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2204  | total loss: 1.60770 | time: 12.969s
| Adam | epoch: 038 | loss: 1.60770 - acc: 0.3415 | val_loss: 1.60605 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2262  | total loss: 1.60107 | time: 12.958s
| Adam | epoch: 039 | loss: 1.60107 - acc: 0.3430 | val_loss: 1.60754 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2320  | total loss: 1.61612 | time: 12.954s
| Adam | epoch: 040 | loss: 1.61612 - acc: 0.3437 | val_loss: 1.60862 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2378  | total loss: 1.59827 | time: 12.928s
| Adam | epoch: 041 | loss: 1.59827 - acc: 0.3575 | val_loss: 1.60833 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2436  | total loss: 1.58515 | time: 12.946s
| Adam | epoch: 042 | loss: 1.58515 - acc: 0.3658 | val_loss: 1.60964 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2494  | total loss: 1.61156 | time: 13.002s
| Adam | epoch: 043 | loss: 1.61156 - acc: 0.3349 | val_loss: 1.60837 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2552  | total loss: 1.57434 | time: 12.948s
| Adam | epoch: 044 | loss: 1.57434 - acc: 0.3654 | val_loss: 1.60718 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2610  | total loss: 1.60310 | time: 12.910s
| Adam | epoch: 045 | loss: 1.60310 - acc: 0.3476 | val_loss: 1.60731 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2668  | total loss: 1.61751 | time: 13.039s
| Adam | epoch: 046 | loss: 1.61751 - acc: 0.3445 | val_loss: 1.61338 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2726  | total loss: 1.60935 | time: 12.973s
| Adam | epoch: 047 | loss: 1.60935 - acc: 0.3441 | val_loss: 1.60576 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2784  | total loss: 1.60358 | time: 13.113s
| Adam | epoch: 048 | loss: 1.60358 - acc: 0.3488 | val_loss: 1.60733 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2842  | total loss: 1.59622 | time: 13.049s
| Adam | epoch: 049 | loss: 1.59622 - acc: 0.3468 | val_loss: 1.60516 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2900  | total loss: 1.59425 | time: 12.966s
| Adam | epoch: 050 | loss: 1.59425 - acc: 0.3478 | val_loss: 1.60953 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2958  | total loss: 1.61049 | time: 12.894s
| Adam | epoch: 051 | loss: 1.61049 - acc: 0.3647 | val_loss: 1.60614 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3016  | total loss: 1.58814 | time: 12.883s
| Adam | epoch: 052 | loss: 1.58814 - acc: 0.3788 | val_loss: 1.60951 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3074  | total loss: 1.56075 | time: 12.867s
| Adam | epoch: 053 | loss: 1.56075 - acc: 0.3916 | val_loss: 1.61448 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3132  | total loss: 1.57345 | time: 12.892s
| Adam | epoch: 054 | loss: 1.57345 - acc: 0.3623 | val_loss: 1.61011 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3190  | total loss: 1.65159 | time: 12.892s
| Adam | epoch: 055 | loss: 1.65159 - acc: 0.3236 | val_loss: 1.60824 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3248  | total loss: 1.57614 | time: 12.894s
| Adam | epoch: 056 | loss: 1.57614 - acc: 0.3397 | val_loss: 1.60495 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3306  | total loss: 1.63002 | time: 12.884s
| Adam | epoch: 057 | loss: 1.63002 - acc: 0.3336 | val_loss: 1.60924 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3364  | total loss: 1.59040 | time: 13.026s
| Adam | epoch: 058 | loss: 1.59040 - acc: 0.3518 | val_loss: 1.60684 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3422  | total loss: 1.61008 | time: 12.892s
| Adam | epoch: 059 | loss: 1.61008 - acc: 0.3457 | val_loss: 1.60476 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3480  | total loss: 1.61698 | time: 12.918s
| Adam | epoch: 060 | loss: 1.61698 - acc: 0.3460 | val_loss: 1.60762 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3538  | total loss: 1.62473 | time: 12.937s
| Adam | epoch: 061 | loss: 1.62473 - acc: 0.3540 | val_loss: 1.60586 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3596  | total loss: 1.58220 | time: 12.902s
| Adam | epoch: 062 | loss: 1.58220 - acc: 0.3322 | val_loss: 1.60809 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3654  | total loss: 1.61146 | time: 12.858s
| Adam | epoch: 063 | loss: 1.61146 - acc: 0.3435 | val_loss: 1.60669 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3712  | total loss: 1.61604 | time: 12.878s
| Adam | epoch: 064 | loss: 1.61604 - acc: 0.3401 | val_loss: 1.60744 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3770  | total loss: 1.60161 | time: 12.833s
| Adam | epoch: 065 | loss: 1.60161 - acc: 0.3556 | val_loss: 1.60892 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3828  | total loss: 1.61466 | time: 12.908s
| Adam | epoch: 066 | loss: 1.61466 - acc: 0.3439 | val_loss: 1.60531 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3886  | total loss: 1.57139 | time: 12.868s
| Adam | epoch: 067 | loss: 1.57139 - acc: 0.3618 | val_loss: 1.60898 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3944  | total loss: 1.61764 | time: 12.857s
| Adam | epoch: 068 | loss: 1.61764 - acc: 0.3372 | val_loss: 1.60748 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4002  | total loss: 1.56455 | time: 12.866s
| Adam | epoch: 069 | loss: 1.56455 - acc: 0.3613 | val_loss: 1.60583 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4060  | total loss: 1.58987 | time: 12.885s
| Adam | epoch: 070 | loss: 1.58987 - acc: 0.3509 | val_loss: 1.60785 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4118  | total loss: 1.61656 | time: 12.883s
| Adam | epoch: 071 | loss: 1.61656 - acc: 0.3386 | val_loss: 1.60639 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4176  | total loss: 1.62348 | time: 12.885s
| Adam | epoch: 072 | loss: 1.62348 - acc: 0.3254 | val_loss: 1.60591 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4234  | total loss: 1.58071 | time: 12.941s
| Adam | epoch: 073 | loss: 1.58071 - acc: 0.3581 | val_loss: 1.60824 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4292  | total loss: 1.58543 | time: 12.895s
| Adam | epoch: 074 | loss: 1.58543 - acc: 0.3541 | val_loss: 1.60795 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4350  | total loss: 1.60348 | time: 12.927s
| Adam | epoch: 075 | loss: 1.60348 - acc: 0.3478 | val_loss: 1.60706 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4408  | total loss: 1.61520 | time: 12.881s
| Adam | epoch: 076 | loss: 1.61520 - acc: 0.3315 | val_loss: 1.60789 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4466  | total loss: 1.60270 | time: 12.852s
| Adam | epoch: 077 | loss: 1.60270 - acc: 0.3608 | val_loss: 1.60691 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4524  | total loss: 1.58657 | time: 12.882s
| Adam | epoch: 078 | loss: 1.58657 - acc: 0.3535 | val_loss: 1.60566 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4582  | total loss: 1.60623 | time: 12.907s
| Adam | epoch: 079 | loss: 1.60623 - acc: 0.3335 | val_loss: 1.60950 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4640  | total loss: 1.58452 | time: 12.908s
| Adam | epoch: 080 | loss: 1.58452 - acc: 0.3605 | val_loss: 1.60512 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4698  | total loss: 1.60014 | time: 12.933s
| Adam | epoch: 081 | loss: 1.60014 - acc: 0.3539 | val_loss: 1.60720 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4756  | total loss: 1.60854 | time: 12.892s
| Adam | epoch: 082 | loss: 1.60854 - acc: 0.3403 | val_loss: 1.60654 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4814  | total loss: 1.62267 | time: 12.914s
| Adam | epoch: 083 | loss: 1.62267 - acc: 0.3208 | val_loss: 1.60609 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4872  | total loss: 1.59946 | time: 12.888s
| Adam | epoch: 084 | loss: 1.59946 - acc: 0.3520 | val_loss: 1.60662 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4930  | total loss: 1.61407 | time: 12.869s
| Adam | epoch: 085 | loss: 1.61407 - acc: 0.3461 | val_loss: 1.60589 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4988  | total loss: 1.63212 | time: 12.892s
| Adam | epoch: 086 | loss: 1.63212 - acc: 0.3234 | val_loss: 1.60907 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5046  | total loss: 1.62220 | time: 12.851s
| Adam | epoch: 087 | loss: 1.62220 - acc: 0.3268 | val_loss: 1.60742 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5104  | total loss: 1.58423 | time: 12.902s
| Adam | epoch: 088 | loss: 1.58423 - acc: 0.3671 | val_loss: 1.60680 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5162  | total loss: 1.59734 | time: 12.919s
| Adam | epoch: 089 | loss: 1.59734 - acc: 0.3444 | val_loss: 1.60883 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5220  | total loss: 1.61527 | time: 12.899s
| Adam | epoch: 090 | loss: 1.61527 - acc: 0.3324 | val_loss: 1.60671 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5278  | total loss: 1.60065 | time: 12.935s
| Adam | epoch: 091 | loss: 1.60065 - acc: 0.3600 | val_loss: 1.60655 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5336  | total loss: 1.60042 | time: 12.934s
| Adam | epoch: 092 | loss: 1.60042 - acc: 0.3505 | val_loss: 1.60549 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5394  | total loss: 1.59067 | time: 12.912s
| Adam | epoch: 093 | loss: 1.59067 - acc: 0.3678 | val_loss: 1.60944 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5452  | total loss: 1.61759 | time: 12.910s
| Adam | epoch: 094 | loss: 1.61759 - acc: 0.3364 | val_loss: 1.60702 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5510  | total loss: 1.61392 | time: 12.915s
| Adam | epoch: 095 | loss: 1.61392 - acc: 0.3442 | val_loss: 1.60537 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5568  | total loss: 1.59366 | time: 12.889s
| Adam | epoch: 096 | loss: 1.59366 - acc: 0.3557 | val_loss: 1.61090 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5626  | total loss: 1.59947 | time: 12.914s
| Adam | epoch: 097 | loss: 1.59947 - acc: 0.3575 | val_loss: 1.60615 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5684  | total loss: 1.62031 | time: 12.894s
| Adam | epoch: 098 | loss: 1.62031 - acc: 0.3454 | val_loss: 1.61097 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5742  | total loss: 1.62961 | time: 12.919s
| Adam | epoch: 099 | loss: 1.62961 - acc: 0.3449 | val_loss: 1.60821 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5800  | total loss: 1.62248 | time: 12.908s
| Adam | epoch: 100 | loss: 1.62248 - acc: 0.3283 | val_loss: 1.60926 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

