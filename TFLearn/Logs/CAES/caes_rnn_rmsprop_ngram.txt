Training Step: 56  | total loss: 1.78197 | time: 57.433s
| RMSProp | epoch: 001 | loss: 1.78197 - acc: 0.3066 | val_loss: 1.77796 - val_acc: 0.3288 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 112  | total loss: 1.71191 | time: 12.923s
| RMSProp | epoch: 002 | loss: 1.71191 - acc: 0.3445 | val_loss: 1.63812 - val_acc: 0.3288 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 168  | total loss: 1.60331 | time: 12.924s
| RMSProp | epoch: 003 | loss: 1.60331 - acc: 0.3397 | val_loss: 1.61358 - val_acc: 0.3288 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 224  | total loss: 1.59465 | time: 12.984s
| RMSProp | epoch: 004 | loss: 1.59465 - acc: 0.3726 | val_loss: 1.59969 - val_acc: 0.3612 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 280  | total loss: 1.57889 | time: 13.018s
| RMSProp | epoch: 005 | loss: 1.57889 - acc: 0.3858 | val_loss: 1.60235 - val_acc: 0.3531 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 336  | total loss: 1.59268 | time: 13.020s
| RMSProp | epoch: 006 | loss: 1.59268 - acc: 0.3790 | val_loss: 1.59741 - val_acc: 0.3612 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 392  | total loss: 1.55034 | time: 12.995s
| RMSProp | epoch: 007 | loss: 1.55034 - acc: 0.3900 | val_loss: 1.60332 - val_acc: 0.3612 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 448  | total loss: 1.58629 | time: 13.094s
| RMSProp | epoch: 008 | loss: 1.58629 - acc: 0.3760 | val_loss: 1.59548 - val_acc: 0.3639 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 504  | total loss: 1.59828 | time: 13.261s
| RMSProp | epoch: 009 | loss: 1.59828 - acc: 0.3796 | val_loss: 1.59817 - val_acc: 0.3612 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 560  | total loss: 1.56796 | time: 12.876s
| RMSProp | epoch: 010 | loss: 1.56796 - acc: 0.3981 | val_loss: 1.59821 - val_acc: 0.3693 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 616  | total loss: 1.56233 | time: 12.913s
| RMSProp | epoch: 011 | loss: 1.56233 - acc: 0.4139 | val_loss: 1.59703 - val_acc: 0.3774 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 672  | total loss: 1.56849 | time: 12.888s
| RMSProp | epoch: 012 | loss: 1.56849 - acc: 0.4067 | val_loss: 1.59576 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 728  | total loss: 1.54636 | time: 12.962s
| RMSProp | epoch: 013 | loss: 1.54636 - acc: 0.4146 | val_loss: 1.60479 - val_acc: 0.3558 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 784  | total loss: 1.54670 | time: 12.902s
| RMSProp | epoch: 014 | loss: 1.54670 - acc: 0.4130 | val_loss: 1.60523 - val_acc: 0.3585 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 840  | total loss: 1.55895 | time: 12.933s
| RMSProp | epoch: 015 | loss: 1.55895 - acc: 0.4085 | val_loss: 1.59859 - val_acc: 0.3612 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 896  | total loss: 1.57585 | time: 12.903s
| RMSProp | epoch: 016 | loss: 1.57585 - acc: 0.3903 | val_loss: 1.59593 - val_acc: 0.3558 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 952  | total loss: 1.55794 | time: 12.942s
| RMSProp | epoch: 017 | loss: 1.55794 - acc: 0.3993 | val_loss: 1.59032 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1008  | total loss: 1.57881 | time: 12.877s
| RMSProp | epoch: 018 | loss: 1.57881 - acc: 0.3834 | val_loss: 1.60035 - val_acc: 0.3639 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1064  | total loss: 1.51296 | time: 12.944s
| RMSProp | epoch: 019 | loss: 1.51296 - acc: 0.4110 | val_loss: 1.62732 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1120  | total loss: 1.53939 | time: 13.002s
| RMSProp | epoch: 020 | loss: 1.53939 - acc: 0.4113 | val_loss: 1.60093 - val_acc: 0.3666 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1176  | total loss: 1.57355 | time: 12.965s
| RMSProp | epoch: 021 | loss: 1.57355 - acc: 0.3983 | val_loss: 1.58138 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1232  | total loss: 1.51848 | time: 12.946s
| RMSProp | epoch: 022 | loss: 1.51848 - acc: 0.4232 | val_loss: 1.58918 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1288  | total loss: 1.53900 | time: 12.913s
| RMSProp | epoch: 023 | loss: 1.53900 - acc: 0.3983 | val_loss: 1.62936 - val_acc: 0.3693 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1344  | total loss: 1.56027 | time: 12.933s
| RMSProp | epoch: 024 | loss: 1.56027 - acc: 0.4039 | val_loss: 1.59386 - val_acc: 0.3801 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1400  | total loss: 1.50319 | time: 12.901s
| RMSProp | epoch: 025 | loss: 1.50319 - acc: 0.4034 | val_loss: 1.57224 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1456  | total loss: 1.50274 | time: 12.911s
| RMSProp | epoch: 026 | loss: 1.50274 - acc: 0.4239 | val_loss: 1.60739 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1512  | total loss: 1.51717 | time: 12.900s
| RMSProp | epoch: 027 | loss: 1.51717 - acc: 0.4283 | val_loss: 1.58696 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1568  | total loss: 1.53124 | time: 12.928s
| RMSProp | epoch: 028 | loss: 1.53124 - acc: 0.3953 | val_loss: 1.60571 - val_acc: 0.3801 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1624  | total loss: 1.53310 | time: 12.896s
| RMSProp | epoch: 029 | loss: 1.53310 - acc: 0.3958 | val_loss: 1.57987 - val_acc: 0.3854 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1680  | total loss: 1.51077 | time: 12.927s
| RMSProp | epoch: 030 | loss: 1.51077 - acc: 0.4121 | val_loss: 1.57873 - val_acc: 0.3774 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1736  | total loss: 1.53477 | time: 12.939s
| RMSProp | epoch: 031 | loss: 1.53477 - acc: 0.3927 | val_loss: 1.60646 - val_acc: 0.3827 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1792  | total loss: 1.52571 | time: 12.920s
| RMSProp | epoch: 032 | loss: 1.52571 - acc: 0.4091 | val_loss: 1.57128 - val_acc: 0.3827 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1848  | total loss: 1.49393 | time: 12.933s
| RMSProp | epoch: 033 | loss: 1.49393 - acc: 0.4242 | val_loss: 1.60786 - val_acc: 0.3774 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1904  | total loss: 1.51943 | time: 12.883s
| RMSProp | epoch: 034 | loss: 1.51943 - acc: 0.4134 | val_loss: 1.59099 - val_acc: 0.3774 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1960  | total loss: 1.46681 | time: 12.922s
| RMSProp | epoch: 035 | loss: 1.46681 - acc: 0.4293 | val_loss: 1.59109 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2016  | total loss: 1.49883 | time: 12.876s
| RMSProp | epoch: 036 | loss: 1.49883 - acc: 0.4369 | val_loss: 1.61082 - val_acc: 0.3693 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2072  | total loss: 1.46053 | time: 12.891s
| RMSProp | epoch: 037 | loss: 1.46053 - acc: 0.4443 | val_loss: 1.59323 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2128  | total loss: 1.47784 | time: 12.955s
| RMSProp | epoch: 038 | loss: 1.47784 - acc: 0.4233 | val_loss: 1.58176 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2184  | total loss: 1.47898 | time: 12.856s
| RMSProp | epoch: 039 | loss: 1.47898 - acc: 0.4363 | val_loss: 1.59869 - val_acc: 0.3827 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2240  | total loss: 1.47526 | time: 12.865s
| RMSProp | epoch: 040 | loss: 1.47526 - acc: 0.4362 | val_loss: 1.58091 - val_acc: 0.3774 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2296  | total loss: 1.45015 | time: 12.907s
| RMSProp | epoch: 041 | loss: 1.45015 - acc: 0.4461 | val_loss: 1.57276 - val_acc: 0.3881 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2352  | total loss: 1.58033 | time: 12.927s
| RMSProp | epoch: 042 | loss: 1.58033 - acc: 0.4110 | val_loss: 1.61147 - val_acc: 0.3585 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2408  | total loss: 1.45856 | time: 12.848s
| RMSProp | epoch: 043 | loss: 1.45856 - acc: 0.4359 | val_loss: 1.58054 - val_acc: 0.3827 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2464  | total loss: 1.42953 | time: 12.865s
| RMSProp | epoch: 044 | loss: 1.42953 - acc: 0.4636 | val_loss: 1.58747 - val_acc: 0.3801 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2520  | total loss: 1.43614 | time: 12.899s
| RMSProp | epoch: 045 | loss: 1.43614 - acc: 0.4638 | val_loss: 1.58363 - val_acc: 0.3935 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2576  | total loss: 1.44564 | time: 12.904s
| RMSProp | epoch: 046 | loss: 1.44564 - acc: 0.4484 | val_loss: 1.65105 - val_acc: 0.4016 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2632  | total loss: 1.43100 | time: 12.910s
| RMSProp | epoch: 047 | loss: 1.43100 - acc: 0.4501 | val_loss: 1.57046 - val_acc: 0.4043 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2688  | total loss: 1.45265 | time: 12.907s
| RMSProp | epoch: 048 | loss: 1.45265 - acc: 0.4327 | val_loss: 1.56122 - val_acc: 0.4178 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2744  | total loss: 1.44910 | time: 12.909s
| RMSProp | epoch: 049 | loss: 1.44910 - acc: 0.4336 | val_loss: 1.56543 - val_acc: 0.4016 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2800  | total loss: 1.44324 | time: 12.923s
| RMSProp | epoch: 050 | loss: 1.44324 - acc: 0.4717 | val_loss: 1.57793 - val_acc: 0.3935 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2856  | total loss: 1.44863 | time: 12.884s
| RMSProp | epoch: 051 | loss: 1.44863 - acc: 0.4475 | val_loss: 1.59283 - val_acc: 0.3854 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2912  | total loss: 1.40745 | time: 12.858s
| RMSProp | epoch: 052 | loss: 1.40745 - acc: 0.4868 | val_loss: 1.62826 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2968  | total loss: 1.43815 | time: 12.915s
| RMSProp | epoch: 053 | loss: 1.43815 - acc: 0.4542 | val_loss: 1.60614 - val_acc: 0.3801 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3024  | total loss: 1.40335 | time: 12.938s
| RMSProp | epoch: 054 | loss: 1.40335 - acc: 0.4778 | val_loss: 1.60927 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3080  | total loss: 1.43967 | time: 12.884s
| RMSProp | epoch: 055 | loss: 1.43967 - acc: 0.4518 | val_loss: 1.59973 - val_acc: 0.4205 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3136  | total loss: 1.40646 | time: 12.883s
| RMSProp | epoch: 056 | loss: 1.40646 - acc: 0.4629 | val_loss: 1.60223 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3192  | total loss: 1.40942 | time: 12.898s
| RMSProp | epoch: 057 | loss: 1.40942 - acc: 0.4808 | val_loss: 1.63355 - val_acc: 0.3962 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3248  | total loss: 1.40871 | time: 12.907s
| RMSProp | epoch: 058 | loss: 1.40871 - acc: 0.4846 | val_loss: 1.59416 - val_acc: 0.3962 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3304  | total loss: 1.42547 | time: 12.876s
| RMSProp | epoch: 059 | loss: 1.42547 - acc: 0.4843 | val_loss: 1.57358 - val_acc: 0.4124 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3360  | total loss: 1.36334 | time: 12.873s
| RMSProp | epoch: 060 | loss: 1.36334 - acc: 0.5028 | val_loss: 1.62329 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3416  | total loss: 1.36037 | time: 12.918s
| RMSProp | epoch: 061 | loss: 1.36037 - acc: 0.5066 | val_loss: 1.63004 - val_acc: 0.3935 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3472  | total loss: 1.36803 | time: 12.896s
| RMSProp | epoch: 062 | loss: 1.36803 - acc: 0.5034 | val_loss: 1.60347 - val_acc: 0.4016 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3528  | total loss: 1.40499 | time: 12.882s
| RMSProp | epoch: 063 | loss: 1.40499 - acc: 0.4704 | val_loss: 1.58602 - val_acc: 0.3854 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3584  | total loss: 1.39415 | time: 12.902s
| RMSProp | epoch: 064 | loss: 1.39415 - acc: 0.4860 | val_loss: 1.54533 - val_acc: 0.4124 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3640  | total loss: 1.37457 | time: 12.865s
| RMSProp | epoch: 065 | loss: 1.37457 - acc: 0.4957 | val_loss: 1.57453 - val_acc: 0.4070 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3696  | total loss: 1.34830 | time: 12.895s
| RMSProp | epoch: 066 | loss: 1.34830 - acc: 0.5281 | val_loss: 1.58869 - val_acc: 0.3881 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3752  | total loss: 1.38909 | time: 12.924s
| RMSProp | epoch: 067 | loss: 1.38909 - acc: 0.4686 | val_loss: 1.55916 - val_acc: 0.4124 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3808  | total loss: 1.38165 | time: 12.890s
| RMSProp | epoch: 068 | loss: 1.38165 - acc: 0.4963 | val_loss: 1.54046 - val_acc: 0.4340 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3864  | total loss: 1.38123 | time: 12.896s
| RMSProp | epoch: 069 | loss: 1.38123 - acc: 0.4708 | val_loss: 1.60479 - val_acc: 0.4124 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3920  | total loss: 1.36779 | time: 12.924s
| RMSProp | epoch: 070 | loss: 1.36779 - acc: 0.5046 | val_loss: 1.61474 - val_acc: 0.3962 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3976  | total loss: 1.35872 | time: 12.923s
| RMSProp | epoch: 071 | loss: 1.35872 - acc: 0.4945 | val_loss: 1.56858 - val_acc: 0.4151 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4032  | total loss: 1.33435 | time: 12.903s
| RMSProp | epoch: 072 | loss: 1.33435 - acc: 0.5250 | val_loss: 1.61916 - val_acc: 0.4259 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4088  | total loss: 1.37296 | time: 12.900s
| RMSProp | epoch: 073 | loss: 1.37296 - acc: 0.4881 | val_loss: 1.59538 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4144  | total loss: 1.29767 | time: 12.914s
| RMSProp | epoch: 074 | loss: 1.29767 - acc: 0.5307 | val_loss: 1.67489 - val_acc: 0.3881 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4200  | total loss: 1.29968 | time: 12.861s
| RMSProp | epoch: 075 | loss: 1.29968 - acc: 0.5393 | val_loss: 1.59704 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4256  | total loss: 1.31392 | time: 12.900s
| RMSProp | epoch: 076 | loss: 1.31392 - acc: 0.5183 | val_loss: 1.61733 - val_acc: 0.3935 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4312  | total loss: 1.31936 | time: 12.899s
| RMSProp | epoch: 077 | loss: 1.31936 - acc: 0.5285 | val_loss: 1.60950 - val_acc: 0.4016 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4368  | total loss: 1.34635 | time: 12.921s
| RMSProp | epoch: 078 | loss: 1.34635 - acc: 0.5042 | val_loss: 1.59629 - val_acc: 0.4070 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4424  | total loss: 1.30226 | time: 12.987s
| RMSProp | epoch: 079 | loss: 1.30226 - acc: 0.5118 | val_loss: 1.61732 - val_acc: 0.4205 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4480  | total loss: 1.32482 | time: 12.921s
| RMSProp | epoch: 080 | loss: 1.32482 - acc: 0.5280 | val_loss: 1.61573 - val_acc: 0.4070 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4536  | total loss: 1.31320 | time: 12.954s
| RMSProp | epoch: 081 | loss: 1.31320 - acc: 0.5115 | val_loss: 1.62736 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4592  | total loss: 1.28818 | time: 12.900s
| RMSProp | epoch: 082 | loss: 1.28818 - acc: 0.5304 | val_loss: 1.63244 - val_acc: 0.3881 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4648  | total loss: 1.34525 | time: 13.008s
| RMSProp | epoch: 083 | loss: 1.34525 - acc: 0.5059 | val_loss: 1.58480 - val_acc: 0.4097 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4704  | total loss: 1.29789 | time: 13.019s
| RMSProp | epoch: 084 | loss: 1.29789 - acc: 0.5335 | val_loss: 1.58596 - val_acc: 0.4070 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4760  | total loss: 1.34538 | time: 13.246s
| RMSProp | epoch: 085 | loss: 1.34538 - acc: 0.5275 | val_loss: 1.60315 - val_acc: 0.4124 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4816  | total loss: 1.30083 | time: 12.916s
| RMSProp | epoch: 086 | loss: 1.30083 - acc: 0.5303 | val_loss: 1.62347 - val_acc: 0.4178 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4872  | total loss: 1.32159 | time: 12.878s
| RMSProp | epoch: 087 | loss: 1.32159 - acc: 0.5180 | val_loss: 1.62592 - val_acc: 0.3935 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4928  | total loss: 1.29784 | time: 12.916s
| RMSProp | epoch: 088 | loss: 1.29784 - acc: 0.5320 | val_loss: 1.62261 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4984  | total loss: 1.37655 | time: 12.921s
| RMSProp | epoch: 089 | loss: 1.37655 - acc: 0.5046 | val_loss: 1.59240 - val_acc: 0.3962 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5040  | total loss: 1.24700 | time: 12.910s
| RMSProp | epoch: 090 | loss: 1.24700 - acc: 0.5640 | val_loss: 1.60035 - val_acc: 0.4043 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5096  | total loss: 1.27453 | time: 12.952s
| RMSProp | epoch: 091 | loss: 1.27453 - acc: 0.5525 | val_loss: 1.67633 - val_acc: 0.4097 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5152  | total loss: 1.26880 | time: 12.907s
| RMSProp | epoch: 092 | loss: 1.26880 - acc: 0.5292 | val_loss: 1.64735 - val_acc: 0.4097 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5208  | total loss: 1.24427 | time: 12.894s
| RMSProp | epoch: 093 | loss: 1.24427 - acc: 0.5578 | val_loss: 1.65689 - val_acc: 0.3881 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5264  | total loss: 1.25941 | time: 12.831s
| RMSProp | epoch: 094 | loss: 1.25941 - acc: 0.5478 | val_loss: 1.68467 - val_acc: 0.4070 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5320  | total loss: 1.24906 | time: 12.914s
| RMSProp | epoch: 095 | loss: 1.24906 - acc: 0.5574 | val_loss: 1.65061 - val_acc: 0.4016 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5376  | total loss: 1.26448 | time: 12.911s
| RMSProp | epoch: 096 | loss: 1.26448 - acc: 0.5528 | val_loss: 1.66475 - val_acc: 0.3935 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5432  | total loss: 1.21860 | time: 12.900s
| RMSProp | epoch: 097 | loss: 1.21860 - acc: 0.5553 | val_loss: 1.68685 - val_acc: 0.4205 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5488  | total loss: 1.19113 | time: 12.846s
| RMSProp | epoch: 098 | loss: 1.19113 - acc: 0.5737 | val_loss: 1.60106 - val_acc: 0.3962 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5544  | total loss: 1.21093 | time: 12.917s
| RMSProp | epoch: 099 | loss: 1.21093 - acc: 0.5682 | val_loss: 1.63201 - val_acc: 0.4016 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5600  | total loss: 1.23143 | time: 12.926s
| RMSProp | epoch: 100 | loss: 1.23143 - acc: 0.5630 | val_loss: 1.64674 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

