Training Step: 58  | total loss: 1.77722 | time: 129.138s
| RMSProp | epoch: 001 | loss: 1.77722 - acc: 0.3373 | val_loss: 1.77543 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 116  | total loss: 1.64122 | time: 19.652s
| RMSProp | epoch: 002 | loss: 1.64122 - acc: 0.3432 | val_loss: 1.61547 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 174  | total loss: 1.59323 | time: 19.723s
| RMSProp | epoch: 003 | loss: 1.59323 - acc: 0.3407 | val_loss: 1.61724 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 232  | total loss: 1.57386 | time: 19.689s
| RMSProp | epoch: 004 | loss: 1.57386 - acc: 0.3530 | val_loss: 1.61253 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 290  | total loss: 1.63482 | time: 19.668s
| RMSProp | epoch: 005 | loss: 1.63482 - acc: 0.3109 | val_loss: 1.61350 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 348  | total loss: 1.60211 | time: 19.675s
| RMSProp | epoch: 006 | loss: 1.60211 - acc: 0.3334 | val_loss: 1.61390 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 406  | total loss: 1.60427 | time: 19.792s
| RMSProp | epoch: 007 | loss: 1.60427 - acc: 0.3509 | val_loss: 1.61008 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 464  | total loss: 1.61012 | time: 19.708s
| RMSProp | epoch: 008 | loss: 1.61012 - acc: 0.3494 | val_loss: 1.61246 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 522  | total loss: 1.59256 | time: 19.662s
| RMSProp | epoch: 009 | loss: 1.59256 - acc: 0.3394 | val_loss: 1.61300 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 580  | total loss: 1.60713 | time: 19.707s
| RMSProp | epoch: 010 | loss: 1.60713 - acc: 0.3535 | val_loss: 1.61299 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 638  | total loss: 1.61034 | time: 19.648s
| RMSProp | epoch: 011 | loss: 1.61034 - acc: 0.3362 | val_loss: 1.61457 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 696  | total loss: 1.58242 | time: 19.779s
| RMSProp | epoch: 012 | loss: 1.58242 - acc: 0.3653 | val_loss: 1.61063 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 754  | total loss: 1.60820 | time: 19.748s
| RMSProp | epoch: 013 | loss: 1.60820 - acc: 0.3476 | val_loss: 1.61061 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 812  | total loss: 1.57943 | time: 19.650s
| RMSProp | epoch: 014 | loss: 1.57943 - acc: 0.3679 | val_loss: 1.61162 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 870  | total loss: 1.60265 | time: 19.673s
| RMSProp | epoch: 015 | loss: 1.60265 - acc: 0.3443 | val_loss: 1.61480 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 928  | total loss: 1.59390 | time: 19.727s
| RMSProp | epoch: 016 | loss: 1.59390 - acc: 0.3486 | val_loss: 1.60977 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 986  | total loss: 1.57244 | time: 19.604s
| RMSProp | epoch: 017 | loss: 1.57244 - acc: 0.3586 | val_loss: 1.61578 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1044  | total loss: 1.61335 | time: 19.731s
| RMSProp | epoch: 018 | loss: 1.61335 - acc: 0.3337 | val_loss: 1.61143 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1102  | total loss: 1.59277 | time: 19.613s
| RMSProp | epoch: 019 | loss: 1.59277 - acc: 0.3498 | val_loss: 1.61150 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1160  | total loss: 1.59349 | time: 19.681s
| RMSProp | epoch: 020 | loss: 1.59349 - acc: 0.3410 | val_loss: 1.61505 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1218  | total loss: 1.61259 | time: 19.702s
| RMSProp | epoch: 021 | loss: 1.61259 - acc: 0.3389 | val_loss: 1.61248 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1276  | total loss: 1.61652 | time: 19.692s
| RMSProp | epoch: 022 | loss: 1.61652 - acc: 0.3176 | val_loss: 1.61408 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1334  | total loss: 1.61387 | time: 19.656s
| RMSProp | epoch: 023 | loss: 1.61387 - acc: 0.3475 | val_loss: 1.61309 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1392  | total loss: 1.60459 | time: 19.675s
| RMSProp | epoch: 024 | loss: 1.60459 - acc: 0.3278 | val_loss: 1.61600 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1450  | total loss: 1.62896 | time: 19.802s
| RMSProp | epoch: 025 | loss: 1.62896 - acc: 0.3221 | val_loss: 1.61311 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1508  | total loss: 1.59018 | time: 19.744s
| RMSProp | epoch: 026 | loss: 1.59018 - acc: 0.3608 | val_loss: 1.61229 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1566  | total loss: 1.60983 | time: 19.711s
| RMSProp | epoch: 027 | loss: 1.60983 - acc: 0.3337 | val_loss: 1.61192 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1624  | total loss: 1.58946 | time: 19.707s
| RMSProp | epoch: 028 | loss: 1.58946 - acc: 0.3532 | val_loss: 1.61046 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1682  | total loss: 1.59983 | time: 19.691s
| RMSProp | epoch: 029 | loss: 1.59983 - acc: 0.3439 | val_loss: 1.61180 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1740  | total loss: 1.61958 | time: 19.679s
| RMSProp | epoch: 030 | loss: 1.61958 - acc: 0.3289 | val_loss: 1.61134 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1798  | total loss: 1.59454 | time: 19.672s
| RMSProp | epoch: 031 | loss: 1.59454 - acc: 0.3534 | val_loss: 1.61264 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1856  | total loss: 1.60301 | time: 19.645s
| RMSProp | epoch: 032 | loss: 1.60301 - acc: 0.3285 | val_loss: 1.61230 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1914  | total loss: 1.61203 | time: 19.645s
| RMSProp | epoch: 033 | loss: 1.61203 - acc: 0.3324 | val_loss: 1.61270 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1972  | total loss: 1.60243 | time: 19.682s
| RMSProp | epoch: 034 | loss: 1.60243 - acc: 0.3438 | val_loss: 1.61204 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2030  | total loss: 1.60995 | time: 19.710s
| RMSProp | epoch: 035 | loss: 1.60995 - acc: 0.3307 | val_loss: 1.61181 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2088  | total loss: 1.58810 | time: 19.705s
| RMSProp | epoch: 036 | loss: 1.58810 - acc: 0.3560 | val_loss: 1.61109 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2146  | total loss: 1.59525 | time: 19.718s
| RMSProp | epoch: 037 | loss: 1.59525 - acc: 0.3621 | val_loss: 1.61210 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2204  | total loss: 1.59820 | time: 19.695s
| RMSProp | epoch: 038 | loss: 1.59820 - acc: 0.3502 | val_loss: 1.61186 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2262  | total loss: 1.59160 | time: 19.593s
| RMSProp | epoch: 039 | loss: 1.59160 - acc: 0.3500 | val_loss: 1.61315 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2320  | total loss: 1.58108 | time: 19.744s
| RMSProp | epoch: 040 | loss: 1.58108 - acc: 0.3545 | val_loss: 1.61263 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2378  | total loss: 1.60552 | time: 19.785s
| RMSProp | epoch: 041 | loss: 1.60552 - acc: 0.3555 | val_loss: 1.61356 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2436  | total loss: 1.60405 | time: 19.718s
| RMSProp | epoch: 042 | loss: 1.60405 - acc: 0.3421 | val_loss: 1.61192 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2494  | total loss: 1.61091 | time: 19.738s
| RMSProp | epoch: 043 | loss: 1.61091 - acc: 0.3339 | val_loss: 1.61322 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2552  | total loss: 1.60496 | time: 19.726s
| RMSProp | epoch: 044 | loss: 1.60496 - acc: 0.3542 | val_loss: 1.61148 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2610  | total loss: 1.60810 | time: 19.654s
| RMSProp | epoch: 045 | loss: 1.60810 - acc: 0.3354 | val_loss: 1.61069 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2668  | total loss: 1.59756 | time: 19.713s
| RMSProp | epoch: 046 | loss: 1.59756 - acc: 0.3512 | val_loss: 1.61241 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2726  | total loss: 1.59088 | time: 19.778s
| RMSProp | epoch: 047 | loss: 1.59088 - acc: 0.3519 | val_loss: 1.61417 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2784  | total loss: 1.60778 | time: 19.730s
| RMSProp | epoch: 048 | loss: 1.60778 - acc: 0.3411 | val_loss: 1.61130 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2842  | total loss: 1.59503 | time: 19.657s
| RMSProp | epoch: 049 | loss: 1.59503 - acc: 0.3580 | val_loss: 1.61197 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2900  | total loss: 1.60457 | time: 19.774s
| RMSProp | epoch: 050 | loss: 1.60457 - acc: 0.3453 | val_loss: 1.61233 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2958  | total loss: 1.58865 | time: 19.684s
| RMSProp | epoch: 051 | loss: 1.58865 - acc: 0.3503 | val_loss: 1.61251 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3016  | total loss: 1.53833 | time: 19.668s
| RMSProp | epoch: 052 | loss: 1.53833 - acc: 0.3836 | val_loss: 1.61826 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3074  | total loss: 1.56226 | time: 19.720s
| RMSProp | epoch: 053 | loss: 1.56226 - acc: 0.3791 | val_loss: 1.61394 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3132  | total loss: 1.60040 | time: 19.671s
| RMSProp | epoch: 054 | loss: 1.60040 - acc: 0.3329 | val_loss: 1.61491 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3190  | total loss: 1.64353 | time: 19.624s
| RMSProp | epoch: 055 | loss: 1.64353 - acc: 0.3228 | val_loss: 1.61209 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3248  | total loss: 1.60837 | time: 19.618s
| RMSProp | epoch: 056 | loss: 1.60837 - acc: 0.3168 | val_loss: 1.61372 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3306  | total loss: 1.61825 | time: 19.774s
| RMSProp | epoch: 057 | loss: 1.61825 - acc: 0.3379 | val_loss: 1.61310 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3364  | total loss: 1.58326 | time: 19.759s
| RMSProp | epoch: 058 | loss: 1.58326 - acc: 0.3649 | val_loss: 1.61380 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3422  | total loss: 1.60846 | time: 19.655s
| RMSProp | epoch: 059 | loss: 1.60846 - acc: 0.3474 | val_loss: 1.60934 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3480  | total loss: 1.60618 | time: 19.750s
| RMSProp | epoch: 060 | loss: 1.60618 - acc: 0.3434 | val_loss: 1.61215 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3538  | total loss: 1.63175 | time: 19.740s
| RMSProp | epoch: 061 | loss: 1.63175 - acc: 0.3266 | val_loss: 1.61294 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3596  | total loss: 1.60462 | time: 19.664s
| RMSProp | epoch: 062 | loss: 1.60462 - acc: 0.3456 | val_loss: 1.61146 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3654  | total loss: 1.59422 | time: 19.670s
| RMSProp | epoch: 063 | loss: 1.59422 - acc: 0.3491 | val_loss: 1.61070 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3712  | total loss: 1.60846 | time: 19.644s
| RMSProp | epoch: 064 | loss: 1.60846 - acc: 0.3582 | val_loss: 1.61053 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3770  | total loss: 1.59865 | time: 19.682s
| RMSProp | epoch: 065 | loss: 1.59865 - acc: 0.3543 | val_loss: 1.61060 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3828  | total loss: 1.61361 | time: 19.580s
| RMSProp | epoch: 066 | loss: 1.61361 - acc: 0.3346 | val_loss: 1.61305 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3886  | total loss: 1.60619 | time: 19.924s
| RMSProp | epoch: 067 | loss: 1.60619 - acc: 0.3513 | val_loss: 1.60938 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3944  | total loss: 1.58199 | time: 19.847s
| RMSProp | epoch: 068 | loss: 1.58199 - acc: 0.3502 | val_loss: 1.61230 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4002  | total loss: 1.61356 | time: 19.759s
| RMSProp | epoch: 069 | loss: 1.61356 - acc: 0.3515 | val_loss: 1.61107 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4060  | total loss: 1.60739 | time: 19.719s
| RMSProp | epoch: 070 | loss: 1.60739 - acc: 0.3459 | val_loss: 1.61108 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4118  | total loss: 1.62649 | time: 19.682s
| RMSProp | epoch: 071 | loss: 1.62649 - acc: 0.3221 | val_loss: 1.61258 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4176  | total loss: 1.59765 | time: 19.624s
| RMSProp | epoch: 072 | loss: 1.59765 - acc: 0.3548 | val_loss: 1.61219 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4234  | total loss: 1.62512 | time: 19.671s
| RMSProp | epoch: 073 | loss: 1.62512 - acc: 0.3246 | val_loss: 1.61202 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4292  | total loss: 1.60403 | time: 19.797s
| RMSProp | epoch: 074 | loss: 1.60403 - acc: 0.3414 | val_loss: 1.61153 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4350  | total loss: 1.60589 | time: 19.711s
| RMSProp | epoch: 075 | loss: 1.60589 - acc: 0.3433 | val_loss: 1.61299 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4408  | total loss: 1.60792 | time: 19.735s
| RMSProp | epoch: 076 | loss: 1.60792 - acc: 0.3378 | val_loss: 1.61324 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4466  | total loss: 1.59398 | time: 19.734s
| RMSProp | epoch: 077 | loss: 1.59398 - acc: 0.3490 | val_loss: 1.61175 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4524  | total loss: 1.59167 | time: 19.694s
| RMSProp | epoch: 078 | loss: 1.59167 - acc: 0.3393 | val_loss: 1.61246 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4582  | total loss: 1.60619 | time: 19.728s
| RMSProp | epoch: 079 | loss: 1.60619 - acc: 0.3449 | val_loss: 1.61085 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4640  | total loss: 1.60625 | time: 19.759s
| RMSProp | epoch: 080 | loss: 1.60625 - acc: 0.3250 | val_loss: 1.61265 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4698  | total loss: 1.58632 | time: 19.722s
| RMSProp | epoch: 081 | loss: 1.58632 - acc: 0.3488 | val_loss: 1.61284 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4756  | total loss: 1.60500 | time: 19.677s
| RMSProp | epoch: 082 | loss: 1.60500 - acc: 0.3559 | val_loss: 1.61069 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4814  | total loss: 1.59812 | time: 19.693s
| RMSProp | epoch: 083 | loss: 1.59812 - acc: 0.3283 | val_loss: 1.61276 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4872  | total loss: 1.59485 | time: 19.692s
| RMSProp | epoch: 084 | loss: 1.59485 - acc: 0.3413 | val_loss: 1.61245 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4930  | total loss: 1.59177 | time: 19.895s
| RMSProp | epoch: 085 | loss: 1.59177 - acc: 0.3700 | val_loss: 1.61077 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4988  | total loss: 1.61033 | time: 19.684s
| RMSProp | epoch: 086 | loss: 1.61033 - acc: 0.3372 | val_loss: 1.61268 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5046  | total loss: 1.60710 | time: 19.731s
| RMSProp | epoch: 087 | loss: 1.60710 - acc: 0.3378 | val_loss: 1.61120 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5104  | total loss: 1.61447 | time: 19.629s
| RMSProp | epoch: 088 | loss: 1.61447 - acc: 0.3410 | val_loss: 1.61091 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5162  | total loss: 1.59465 | time: 19.600s
| RMSProp | epoch: 089 | loss: 1.59465 - acc: 0.3376 | val_loss: 1.61154 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5220  | total loss: 1.60326 | time: 19.706s
| RMSProp | epoch: 090 | loss: 1.60326 - acc: 0.3433 | val_loss: 1.61179 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5278  | total loss: 1.61773 | time: 19.666s
| RMSProp | epoch: 091 | loss: 1.61773 - acc: 0.3386 | val_loss: 1.61257 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5336  | total loss: 1.61612 | time: 19.710s
| RMSProp | epoch: 092 | loss: 1.61612 - acc: 0.3326 | val_loss: 1.61158 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5394  | total loss: 1.60632 | time: 19.662s
| RMSProp | epoch: 093 | loss: 1.60632 - acc: 0.3437 | val_loss: 1.61190 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5452  | total loss: 1.57317 | time: 19.727s
| RMSProp | epoch: 094 | loss: 1.57317 - acc: 0.3507 | val_loss: 1.61341 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5510  | total loss: 1.61045 | time: 19.695s
| RMSProp | epoch: 095 | loss: 1.61045 - acc: 0.3317 | val_loss: 1.61180 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5568  | total loss: 1.62395 | time: 19.672s
| RMSProp | epoch: 096 | loss: 1.62395 - acc: 0.3301 | val_loss: 1.61208 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5626  | total loss: 1.60838 | time: 19.703s
| RMSProp | epoch: 097 | loss: 1.60838 - acc: 0.3329 | val_loss: 1.61268 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5684  | total loss: 1.59689 | time: 19.726s
| RMSProp | epoch: 098 | loss: 1.59689 - acc: 0.3462 | val_loss: 1.61205 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5742  | total loss: 1.60198 | time: 19.668s
| RMSProp | epoch: 099 | loss: 1.60198 - acc: 0.3446 | val_loss: 1.61249 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5800  | total loss: 1.59131 | time: 19.823s
| RMSProp | epoch: 100 | loss: 1.59131 - acc: 0.3579 | val_loss: 1.61275 - val_acc: 0.3517 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

