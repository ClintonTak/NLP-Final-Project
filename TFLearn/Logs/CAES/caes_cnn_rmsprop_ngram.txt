Training Step: 56  | total loss: 1.77531 | time: 7.901s
| RMSProp | epoch: 001 | loss: 1.77531 - acc: 0.3127 | val_loss: 1.76965 - val_acc: 0.3504 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 112  | total loss: 1.67980 | time: 6.951s
| RMSProp | epoch: 002 | loss: 1.67980 - acc: 0.3533 | val_loss: 1.63386 - val_acc: 0.3504 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 168  | total loss: 1.62517 | time: 6.926s
| RMSProp | epoch: 003 | loss: 1.62517 - acc: 0.3497 | val_loss: 1.58457 - val_acc: 0.3504 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 224  | total loss: 1.60318 | time: 6.902s
| RMSProp | epoch: 004 | loss: 1.60318 - acc: 0.3440 | val_loss: 1.57719 - val_acc: 0.3504 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 280  | total loss: 1.61377 | time: 6.946s
| RMSProp | epoch: 005 | loss: 1.61377 - acc: 0.3543 | val_loss: 1.57259 - val_acc: 0.3504 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 336  | total loss: 1.57081 | time: 6.915s
| RMSProp | epoch: 006 | loss: 1.57081 - acc: 0.3741 | val_loss: 1.55915 - val_acc: 0.3504 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 392  | total loss: 1.59059 | time: 6.904s
| RMSProp | epoch: 007 | loss: 1.59059 - acc: 0.3694 | val_loss: 1.54186 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 448  | total loss: 1.53241 | time: 6.903s
| RMSProp | epoch: 008 | loss: 1.53241 - acc: 0.4003 | val_loss: 1.51052 - val_acc: 0.3774 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 504  | total loss: 1.52995 | time: 6.947s
| RMSProp | epoch: 009 | loss: 1.52995 - acc: 0.4236 | val_loss: 1.47818 - val_acc: 0.4124 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 560  | total loss: 1.48102 | time: 6.930s
| RMSProp | epoch: 010 | loss: 1.48102 - acc: 0.4383 | val_loss: 1.44157 - val_acc: 0.4313 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 616  | total loss: 1.42488 | time: 6.933s
| RMSProp | epoch: 011 | loss: 1.42488 - acc: 0.4752 | val_loss: 1.40766 - val_acc: 0.4636 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 672  | total loss: 1.39801 | time: 6.914s
| RMSProp | epoch: 012 | loss: 1.39801 - acc: 0.4948 | val_loss: 1.38381 - val_acc: 0.4690 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 728  | total loss: 1.37685 | time: 6.902s
| RMSProp | epoch: 013 | loss: 1.37685 - acc: 0.4726 | val_loss: 1.37329 - val_acc: 0.4825 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 784  | total loss: 1.37052 | time: 6.895s
| RMSProp | epoch: 014 | loss: 1.37052 - acc: 0.4935 | val_loss: 1.34112 - val_acc: 0.5040 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 840  | total loss: 1.31059 | time: 6.904s
| RMSProp | epoch: 015 | loss: 1.31059 - acc: 0.5142 | val_loss: 1.32426 - val_acc: 0.5067 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 896  | total loss: 1.29592 | time: 6.917s
| RMSProp | epoch: 016 | loss: 1.29592 - acc: 0.5222 | val_loss: 1.32000 - val_acc: 0.4960 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 952  | total loss: 1.26629 | time: 6.933s
| RMSProp | epoch: 017 | loss: 1.26629 - acc: 0.5515 | val_loss: 1.29996 - val_acc: 0.5175 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1008  | total loss: 1.24688 | time: 6.912s
| RMSProp | epoch: 018 | loss: 1.24688 - acc: 0.5457 | val_loss: 1.30596 - val_acc: 0.5121 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1064  | total loss: 1.27828 | time: 6.907s
| RMSProp | epoch: 019 | loss: 1.27828 - acc: 0.5121 | val_loss: 1.29400 - val_acc: 0.5391 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1120  | total loss: 1.21123 | time: 6.906s
| RMSProp | epoch: 020 | loss: 1.21123 - acc: 0.5555 | val_loss: 1.28034 - val_acc: 0.5310 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1176  | total loss: 1.20649 | time: 6.914s
| RMSProp | epoch: 021 | loss: 1.20649 - acc: 0.5635 | val_loss: 1.28159 - val_acc: 0.5283 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1232  | total loss: 1.21932 | time: 6.921s
| RMSProp | epoch: 022 | loss: 1.21932 - acc: 0.5499 | val_loss: 1.28596 - val_acc: 0.5364 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1288  | total loss: 1.19095 | time: 6.943s
| RMSProp | epoch: 023 | loss: 1.19095 - acc: 0.5537 | val_loss: 1.26719 - val_acc: 0.5445 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1344  | total loss: 1.15825 | time: 6.909s
| RMSProp | epoch: 024 | loss: 1.15825 - acc: 0.5778 | val_loss: 1.25006 - val_acc: 0.5499 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1400  | total loss: 1.16210 | time: 6.922s
| RMSProp | epoch: 025 | loss: 1.16210 - acc: 0.5766 | val_loss: 1.25300 - val_acc: 0.5553 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1456  | total loss: 1.14702 | time: 6.878s
| RMSProp | epoch: 026 | loss: 1.14702 - acc: 0.5991 | val_loss: 1.24635 - val_acc: 0.5472 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1512  | total loss: 1.10873 | time: 6.899s
| RMSProp | epoch: 027 | loss: 1.10873 - acc: 0.6099 | val_loss: 1.24088 - val_acc: 0.5526 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1568  | total loss: 1.20786 | time: 6.924s
| RMSProp | epoch: 028 | loss: 1.20786 - acc: 0.5766 | val_loss: 1.24474 - val_acc: 0.5526 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1624  | total loss: 1.20249 | time: 6.929s
| RMSProp | epoch: 029 | loss: 1.20249 - acc: 0.5826 | val_loss: 1.24444 - val_acc: 0.5337 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1680  | total loss: 1.08812 | time: 6.909s
| RMSProp | epoch: 030 | loss: 1.08812 - acc: 0.6137 | val_loss: 1.22871 - val_acc: 0.5445 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1736  | total loss: 1.08584 | time: 6.897s
| RMSProp | epoch: 031 | loss: 1.08584 - acc: 0.6186 | val_loss: 1.24911 - val_acc: 0.5445 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1792  | total loss: 1.13506 | time: 6.931s
| RMSProp | epoch: 032 | loss: 1.13506 - acc: 0.5918 | val_loss: 1.24597 - val_acc: 0.5472 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1848  | total loss: 1.05831 | time: 6.905s
| RMSProp | epoch: 033 | loss: 1.05831 - acc: 0.6258 | val_loss: 1.23895 - val_acc: 0.5553 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1904  | total loss: 1.06433 | time: 6.924s
| RMSProp | epoch: 034 | loss: 1.06433 - acc: 0.6221 | val_loss: 1.22202 - val_acc: 0.5606 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1960  | total loss: 1.04432 | time: 6.943s
| RMSProp | epoch: 035 | loss: 1.04432 - acc: 0.6157 | val_loss: 1.22403 - val_acc: 0.5553 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2016  | total loss: 1.05082 | time: 6.957s
| RMSProp | epoch: 036 | loss: 1.05082 - acc: 0.6371 | val_loss: 1.21894 - val_acc: 0.5606 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2072  | total loss: 1.02709 | time: 7.160s
| RMSProp | epoch: 037 | loss: 1.02709 - acc: 0.6364 | val_loss: 1.23282 - val_acc: 0.5687 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2128  | total loss: 1.01583 | time: 6.985s
| RMSProp | epoch: 038 | loss: 1.01583 - acc: 0.6351 | val_loss: 1.23687 - val_acc: 0.5633 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2184  | total loss: 1.04357 | time: 6.917s
| RMSProp | epoch: 039 | loss: 1.04357 - acc: 0.6239 | val_loss: 1.22891 - val_acc: 0.5606 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2240  | total loss: 1.03733 | time: 6.887s
| RMSProp | epoch: 040 | loss: 1.03733 - acc: 0.6314 | val_loss: 1.24199 - val_acc: 0.5391 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2296  | total loss: 0.98095 | time: 6.895s
| RMSProp | epoch: 041 | loss: 0.98095 - acc: 0.6530 | val_loss: 1.23761 - val_acc: 0.5606 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2352  | total loss: 1.00812 | time: 6.883s
| RMSProp | epoch: 042 | loss: 1.00812 - acc: 0.6470 | val_loss: 1.23118 - val_acc: 0.5687 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2408  | total loss: 1.00217 | time: 6.904s
| RMSProp | epoch: 043 | loss: 1.00217 - acc: 0.6508 | val_loss: 1.24553 - val_acc: 0.5580 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2464  | total loss: 0.96668 | time: 6.915s
| RMSProp | epoch: 044 | loss: 0.96668 - acc: 0.6513 | val_loss: 1.24974 - val_acc: 0.5580 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2520  | total loss: 0.97891 | time: 6.870s
| RMSProp | epoch: 045 | loss: 0.97891 - acc: 0.6555 | val_loss: 1.23732 - val_acc: 0.5660 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2576  | total loss: 0.93794 | time: 6.866s
| RMSProp | epoch: 046 | loss: 0.93794 - acc: 0.6623 | val_loss: 1.24942 - val_acc: 0.5553 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2632  | total loss: 1.02635 | time: 6.892s
| RMSProp | epoch: 047 | loss: 1.02635 - acc: 0.6276 | val_loss: 1.23096 - val_acc: 0.5499 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2688  | total loss: 0.97625 | time: 6.913s
| RMSProp | epoch: 048 | loss: 0.97625 - acc: 0.6574 | val_loss: 1.24297 - val_acc: 0.5526 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2744  | total loss: 0.95444 | time: 6.861s
| RMSProp | epoch: 049 | loss: 0.95444 - acc: 0.6671 | val_loss: 1.26141 - val_acc: 0.5660 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2800  | total loss: 0.95296 | time: 6.899s
| RMSProp | epoch: 050 | loss: 0.95296 - acc: 0.6536 | val_loss: 1.26251 - val_acc: 0.5714 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2856  | total loss: 0.91781 | time: 6.905s
| RMSProp | epoch: 051 | loss: 0.91781 - acc: 0.6810 | val_loss: 1.25267 - val_acc: 0.5714 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2912  | total loss: 0.93032 | time: 7.126s
| RMSProp | epoch: 052 | loss: 0.93032 - acc: 0.6727 | val_loss: 1.25106 - val_acc: 0.5606 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2968  | total loss: 0.87436 | time: 7.055s
| RMSProp | epoch: 053 | loss: 0.87436 - acc: 0.7091 | val_loss: 1.25992 - val_acc: 0.5741 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3024  | total loss: 0.85518 | time: 6.896s
| RMSProp | epoch: 054 | loss: 0.85518 - acc: 0.7027 | val_loss: 1.24258 - val_acc: 0.5822 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3080  | total loss: 0.88588 | time: 6.874s
| RMSProp | epoch: 055 | loss: 0.88588 - acc: 0.6820 | val_loss: 1.26360 - val_acc: 0.5580 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3136  | total loss: 0.84757 | time: 6.886s
| RMSProp | epoch: 056 | loss: 0.84757 - acc: 0.6990 | val_loss: 1.24673 - val_acc: 0.5741 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3192  | total loss: 0.88059 | time: 6.904s
| RMSProp | epoch: 057 | loss: 0.88059 - acc: 0.6803 | val_loss: 1.30520 - val_acc: 0.5364 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3248  | total loss: 0.86844 | time: 6.924s
| RMSProp | epoch: 058 | loss: 0.86844 - acc: 0.6887 | val_loss: 1.26346 - val_acc: 0.5526 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3304  | total loss: 0.87201 | time: 6.947s
| RMSProp | epoch: 059 | loss: 0.87201 - acc: 0.6809 | val_loss: 1.27149 - val_acc: 0.5553 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3360  | total loss: 0.90403 | time: 6.902s
| RMSProp | epoch: 060 | loss: 0.90403 - acc: 0.6872 | val_loss: 1.26365 - val_acc: 0.5687 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3416  | total loss: 0.86503 | time: 6.889s
| RMSProp | epoch: 061 | loss: 0.86503 - acc: 0.6823 | val_loss: 1.27248 - val_acc: 0.5472 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3472  | total loss: 0.88833 | time: 6.879s
| RMSProp | epoch: 062 | loss: 0.88833 - acc: 0.6756 | val_loss: 1.27813 - val_acc: 0.5580 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3528  | total loss: 0.88004 | time: 6.917s
| RMSProp | epoch: 063 | loss: 0.88004 - acc: 0.6923 | val_loss: 1.26931 - val_acc: 0.5499 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3584  | total loss: 0.83456 | time: 6.905s
| RMSProp | epoch: 064 | loss: 0.83456 - acc: 0.6919 | val_loss: 1.27150 - val_acc: 0.5633 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3640  | total loss: 0.84054 | time: 6.913s
| RMSProp | epoch: 065 | loss: 0.84054 - acc: 0.7069 | val_loss: 1.29953 - val_acc: 0.5633 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3696  | total loss: 0.84826 | time: 6.893s
| RMSProp | epoch: 066 | loss: 0.84826 - acc: 0.6984 | val_loss: 1.28639 - val_acc: 0.5472 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3752  | total loss: 0.84504 | time: 6.888s
| RMSProp | epoch: 067 | loss: 0.84504 - acc: 0.6986 | val_loss: 1.29956 - val_acc: 0.5660 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3808  | total loss: 0.85569 | time: 6.894s
| RMSProp | epoch: 068 | loss: 0.85569 - acc: 0.6989 | val_loss: 1.28382 - val_acc: 0.5499 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3864  | total loss: 0.79833 | time: 6.893s
| RMSProp | epoch: 069 | loss: 0.79833 - acc: 0.7378 | val_loss: 1.27853 - val_acc: 0.5633 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3920  | total loss: 0.78250 | time: 6.934s
| RMSProp | epoch: 070 | loss: 0.78250 - acc: 0.7175 | val_loss: 1.29405 - val_acc: 0.5553 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3976  | total loss: 0.89003 | time: 6.878s
| RMSProp | epoch: 071 | loss: 0.89003 - acc: 0.6932 | val_loss: 1.28678 - val_acc: 0.5499 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4032  | total loss: 0.79843 | time: 6.901s
| RMSProp | epoch: 072 | loss: 0.79843 - acc: 0.7274 | val_loss: 1.29351 - val_acc: 0.5633 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4088  | total loss: 0.80049 | time: 6.903s
| RMSProp | epoch: 073 | loss: 0.80049 - acc: 0.7065 | val_loss: 1.28433 - val_acc: 0.5526 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4144  | total loss: 0.86625 | time: 6.901s
| RMSProp | epoch: 074 | loss: 0.86625 - acc: 0.6891 | val_loss: 1.29414 - val_acc: 0.5499 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4200  | total loss: 0.77780 | time: 6.868s
| RMSProp | epoch: 075 | loss: 0.77780 - acc: 0.7276 | val_loss: 1.30092 - val_acc: 0.5526 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4256  | total loss: 0.85708 | time: 6.931s
| RMSProp | epoch: 076 | loss: 0.85708 - acc: 0.6953 | val_loss: 1.29962 - val_acc: 0.5741 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4312  | total loss: 0.78441 | time: 6.989s
| RMSProp | epoch: 077 | loss: 0.78441 - acc: 0.7142 | val_loss: 1.32377 - val_acc: 0.5580 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4368  | total loss: 0.75744 | time: 6.918s
| RMSProp | epoch: 078 | loss: 0.75744 - acc: 0.7323 | val_loss: 1.31408 - val_acc: 0.5768 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4424  | total loss: 0.80139 | time: 6.958s
| RMSProp | epoch: 079 | loss: 0.80139 - acc: 0.7203 | val_loss: 1.32309 - val_acc: 0.5606 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4480  | total loss: 0.80000 | time: 7.071s
| RMSProp | epoch: 080 | loss: 0.80000 - acc: 0.7165 | val_loss: 1.32498 - val_acc: 0.5472 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4536  | total loss: 0.75381 | time: 6.995s
| RMSProp | epoch: 081 | loss: 0.75381 - acc: 0.7429 | val_loss: 1.33868 - val_acc: 0.5606 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4592  | total loss: 0.80000 | time: 7.102s
| RMSProp | epoch: 082 | loss: 0.80000 - acc: 0.7103 | val_loss: 1.33412 - val_acc: 0.5553 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4648  | total loss: 0.73372 | time: 7.034s
| RMSProp | epoch: 083 | loss: 0.73372 - acc: 0.7369 | val_loss: 1.32558 - val_acc: 0.5418 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4704  | total loss: 0.76266 | time: 7.152s
| RMSProp | epoch: 084 | loss: 0.76266 - acc: 0.7370 | val_loss: 1.31568 - val_acc: 0.5337 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4760  | total loss: 0.97870 | time: 7.134s
| RMSProp | epoch: 085 | loss: 0.97870 - acc: 0.7006 | val_loss: 1.35165 - val_acc: 0.5499 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4816  | total loss: 1.00178 | time: 7.126s
| RMSProp | epoch: 086 | loss: 1.00178 - acc: 0.6828 | val_loss: 1.33053 - val_acc: 0.5364 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4872  | total loss: 0.76597 | time: 7.083s
| RMSProp | epoch: 087 | loss: 0.76597 - acc: 0.7254 | val_loss: 1.32756 - val_acc: 0.5553 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4928  | total loss: 0.75309 | time: 7.017s
| RMSProp | epoch: 088 | loss: 0.75309 - acc: 0.7220 | val_loss: 1.34143 - val_acc: 0.5741 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4984  | total loss: 0.75452 | time: 6.881s
| RMSProp | epoch: 089 | loss: 0.75452 - acc: 0.7238 | val_loss: 1.35066 - val_acc: 0.5499 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5040  | total loss: 0.72233 | time: 6.889s
| RMSProp | epoch: 090 | loss: 0.72233 - acc: 0.7464 | val_loss: 1.36654 - val_acc: 0.5526 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5096  | total loss: 0.73316 | time: 6.887s
| RMSProp | epoch: 091 | loss: 0.73316 - acc: 0.7506 | val_loss: 1.35894 - val_acc: 0.5553 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5152  | total loss: 0.72181 | time: 6.897s
| RMSProp | epoch: 092 | loss: 0.72181 - acc: 0.7452 | val_loss: 1.38313 - val_acc: 0.5418 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5208  | total loss: 0.69232 | time: 6.903s
| RMSProp | epoch: 093 | loss: 0.69232 - acc: 0.7585 | val_loss: 1.36521 - val_acc: 0.5418 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5264  | total loss: 0.70536 | time: 6.895s
| RMSProp | epoch: 094 | loss: 0.70536 - acc: 0.7569 | val_loss: 1.36691 - val_acc: 0.5660 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5320  | total loss: 0.72915 | time: 6.884s
| RMSProp | epoch: 095 | loss: 0.72915 - acc: 0.7541 | val_loss: 1.40429 - val_acc: 0.5391 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5376  | total loss: 0.70427 | time: 6.905s
| RMSProp | epoch: 096 | loss: 0.70427 - acc: 0.7527 | val_loss: 1.36882 - val_acc: 0.5391 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5432  | total loss: 0.71227 | time: 6.906s
| RMSProp | epoch: 097 | loss: 0.71227 - acc: 0.7523 | val_loss: 1.42614 - val_acc: 0.5553 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5488  | total loss: 0.76046 | time: 6.883s
| RMSProp | epoch: 098 | loss: 0.76046 - acc: 0.7376 | val_loss: 1.39721 - val_acc: 0.5526 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5544  | total loss: 0.68450 | time: 6.886s
| RMSProp | epoch: 099 | loss: 0.68450 - acc: 0.7628 | val_loss: 1.39957 - val_acc: 0.5472 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5600  | total loss: 0.70173 | time: 6.893s
| RMSProp | epoch: 100 | loss: 0.70173 - acc: 0.7501 | val_loss: 1.38471 - val_acc: 0.5633 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

