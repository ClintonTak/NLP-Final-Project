Training Step: 56  | total loss: 1.61612 | time: 128.122s
| Adam | epoch: 001 | loss: 1.61612 - acc: 0.3408 | val_loss: 1.61203 - val_acc: 0.3531 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 112  | total loss: 1.60704 | time: 19.291s
| Adam | epoch: 002 | loss: 1.60704 - acc: 0.3374 | val_loss: 1.61044 - val_acc: 0.3531 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 168  | total loss: 1.60940 | time: 19.138s
| Adam | epoch: 003 | loss: 1.60940 - acc: 0.3377 | val_loss: 1.59890 - val_acc: 0.3477 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 224  | total loss: 1.58468 | time: 19.147s
| Adam | epoch: 004 | loss: 1.58468 - acc: 0.3654 | val_loss: 1.58777 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 280  | total loss: 1.57436 | time: 19.168s
| Adam | epoch: 005 | loss: 1.57436 - acc: 0.3836 | val_loss: 1.58943 - val_acc: 0.3827 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 336  | total loss: 1.58058 | time: 19.214s
| Adam | epoch: 006 | loss: 1.58058 - acc: 0.3744 | val_loss: 1.58934 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 392  | total loss: 1.57577 | time: 19.159s
| Adam | epoch: 007 | loss: 1.57577 - acc: 0.3807 | val_loss: 1.59707 - val_acc: 0.3693 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 448  | total loss: 1.57489 | time: 19.145s
| Adam | epoch: 008 | loss: 1.57489 - acc: 0.3833 | val_loss: 1.59057 - val_acc: 0.3801 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 504  | total loss: 1.59870 | time: 19.128s
| Adam | epoch: 009 | loss: 1.59870 - acc: 0.3658 | val_loss: 1.59518 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 560  | total loss: 1.57283 | time: 19.143s
| Adam | epoch: 010 | loss: 1.57283 - acc: 0.3939 | val_loss: 1.59424 - val_acc: 0.3801 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 616  | total loss: 1.56400 | time: 19.147s
| Adam | epoch: 011 | loss: 1.56400 - acc: 0.4136 | val_loss: 1.59299 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 672  | total loss: 1.58811 | time: 19.240s
| Adam | epoch: 012 | loss: 1.58811 - acc: 0.3799 | val_loss: 1.59467 - val_acc: 0.3801 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 728  | total loss: 1.56853 | time: 19.148s
| Adam | epoch: 013 | loss: 1.56853 - acc: 0.3924 | val_loss: 1.59732 - val_acc: 0.3854 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 784  | total loss: 1.57007 | time: 19.152s
| Adam | epoch: 014 | loss: 1.57007 - acc: 0.3955 | val_loss: 1.59409 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 840  | total loss: 1.56594 | time: 19.159s
| Adam | epoch: 015 | loss: 1.56594 - acc: 0.3791 | val_loss: 1.60393 - val_acc: 0.3854 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 896  | total loss: 1.55318 | time: 19.248s
| Adam | epoch: 016 | loss: 1.55318 - acc: 0.4174 | val_loss: 1.59646 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 952  | total loss: 1.58527 | time: 19.171s
| Adam | epoch: 017 | loss: 1.58527 - acc: 0.3755 | val_loss: 1.60060 - val_acc: 0.3854 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1008  | total loss: 1.52369 | time: 19.202s
| Adam | epoch: 018 | loss: 1.52369 - acc: 0.4135 | val_loss: 1.60444 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1064  | total loss: 1.56969 | time: 19.167s
| Adam | epoch: 019 | loss: 1.56969 - acc: 0.3945 | val_loss: 1.59025 - val_acc: 0.4016 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1120  | total loss: 1.55510 | time: 19.175s
| Adam | epoch: 020 | loss: 1.55510 - acc: 0.3991 | val_loss: 1.58308 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1176  | total loss: 1.56931 | time: 19.191s
| Adam | epoch: 021 | loss: 1.56931 - acc: 0.3877 | val_loss: 1.61772 - val_acc: 0.3612 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1232  | total loss: 1.53716 | time: 19.159s
| Adam | epoch: 022 | loss: 1.53716 - acc: 0.4113 | val_loss: 1.60542 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1288  | total loss: 1.54916 | time: 19.167s
| Adam | epoch: 023 | loss: 1.54916 - acc: 0.4082 | val_loss: 1.59170 - val_acc: 0.3935 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1344  | total loss: 1.53497 | time: 19.177s
| Adam | epoch: 024 | loss: 1.53497 - acc: 0.4169 | val_loss: 1.58877 - val_acc: 0.3935 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1400  | total loss: 1.53921 | time: 19.165s
| Adam | epoch: 025 | loss: 1.53921 - acc: 0.4097 | val_loss: 1.61880 - val_acc: 0.3801 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1456  | total loss: 1.54381 | time: 19.140s
| Adam | epoch: 026 | loss: 1.54381 - acc: 0.4194 | val_loss: 1.59920 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1512  | total loss: 1.53832 | time: 19.095s
| Adam | epoch: 027 | loss: 1.53832 - acc: 0.3962 | val_loss: 1.57976 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1568  | total loss: 1.54275 | time: 19.203s
| Adam | epoch: 028 | loss: 1.54275 - acc: 0.4101 | val_loss: 1.57139 - val_acc: 0.3935 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1624  | total loss: 1.52197 | time: 19.216s
| Adam | epoch: 029 | loss: 1.52197 - acc: 0.4025 | val_loss: 1.59068 - val_acc: 0.3827 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1680  | total loss: 1.50708 | time: 19.152s
| Adam | epoch: 030 | loss: 1.50708 - acc: 0.4299 | val_loss: 1.56550 - val_acc: 0.3989 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1736  | total loss: 1.52560 | time: 19.118s
| Adam | epoch: 031 | loss: 1.52560 - acc: 0.4029 | val_loss: 1.57142 - val_acc: 0.3854 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1792  | total loss: 1.46634 | time: 19.146s
| Adam | epoch: 032 | loss: 1.46634 - acc: 0.4451 | val_loss: 1.58313 - val_acc: 0.3827 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1848  | total loss: 1.50856 | time: 19.123s
| Adam | epoch: 033 | loss: 1.50856 - acc: 0.4243 | val_loss: 1.58279 - val_acc: 0.3962 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1904  | total loss: 1.41684 | time: 19.189s
| Adam | epoch: 034 | loss: 1.41684 - acc: 0.4662 | val_loss: 1.55866 - val_acc: 0.4097 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1960  | total loss: 1.48245 | time: 19.184s
| Adam | epoch: 035 | loss: 1.48245 - acc: 0.4326 | val_loss: 1.56526 - val_acc: 0.4178 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2016  | total loss: 1.47611 | time: 19.173s
| Adam | epoch: 036 | loss: 1.47611 - acc: 0.4272 | val_loss: 1.54755 - val_acc: 0.3854 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2072  | total loss: 1.46903 | time: 19.188s
| Adam | epoch: 037 | loss: 1.46903 - acc: 0.4289 | val_loss: 1.52828 - val_acc: 0.4205 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2128  | total loss: 1.40412 | time: 19.162s
| Adam | epoch: 038 | loss: 1.40412 - acc: 0.4787 | val_loss: 1.54734 - val_acc: 0.4097 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2184  | total loss: 1.45964 | time: 19.185s
| Adam | epoch: 039 | loss: 1.45964 - acc: 0.4518 | val_loss: 1.58547 - val_acc: 0.3693 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2240  | total loss: 1.39567 | time: 19.133s
| Adam | epoch: 040 | loss: 1.39567 - acc: 0.4590 | val_loss: 1.52885 - val_acc: 0.4286 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2296  | total loss: 1.43133 | time: 19.142s
| Adam | epoch: 041 | loss: 1.43133 - acc: 0.4549 | val_loss: 1.59497 - val_acc: 0.3989 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2352  | total loss: 1.43316 | time: 19.220s
| Adam | epoch: 042 | loss: 1.43316 - acc: 0.4545 | val_loss: 1.54686 - val_acc: 0.3854 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2408  | total loss: 1.42105 | time: 19.256s
| Adam | epoch: 043 | loss: 1.42105 - acc: 0.4712 | val_loss: 1.53162 - val_acc: 0.4097 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2464  | total loss: 1.42912 | time: 19.215s
| Adam | epoch: 044 | loss: 1.42912 - acc: 0.4762 | val_loss: 1.51513 - val_acc: 0.4232 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2520  | total loss: 1.37152 | time: 19.178s
| Adam | epoch: 045 | loss: 1.37152 - acc: 0.5046 | val_loss: 1.53609 - val_acc: 0.4232 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2576  | total loss: 1.40461 | time: 19.151s
| Adam | epoch: 046 | loss: 1.40461 - acc: 0.4840 | val_loss: 1.53052 - val_acc: 0.4286 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2632  | total loss: 1.44888 | time: 19.191s
| Adam | epoch: 047 | loss: 1.44888 - acc: 0.4695 | val_loss: 1.49694 - val_acc: 0.4447 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2688  | total loss: 1.42746 | time: 19.185s
| Adam | epoch: 048 | loss: 1.42746 - acc: 0.4806 | val_loss: 1.49398 - val_acc: 0.4367 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2744  | total loss: 1.38924 | time: 19.183s
| Adam | epoch: 049 | loss: 1.38924 - acc: 0.4902 | val_loss: 1.50674 - val_acc: 0.4367 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2800  | total loss: 1.33233 | time: 19.198s
| Adam | epoch: 050 | loss: 1.33233 - acc: 0.5076 | val_loss: 1.48037 - val_acc: 0.4286 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2856  | total loss: 1.38149 | time: 19.866s
| Adam | epoch: 051 | loss: 1.38149 - acc: 0.4768 | val_loss: 1.47973 - val_acc: 0.4340 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2912  | total loss: 1.32747 | time: 20.245s
| Adam | epoch: 052 | loss: 1.32747 - acc: 0.5151 | val_loss: 1.51470 - val_acc: 0.4447 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2968  | total loss: 1.34475 | time: 19.833s
| Adam | epoch: 053 | loss: 1.34475 - acc: 0.5059 | val_loss: 1.50174 - val_acc: 0.4394 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3024  | total loss: 1.26312 | time: 19.200s
| Adam | epoch: 054 | loss: 1.26312 - acc: 0.5516 | val_loss: 1.52393 - val_acc: 0.4340 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3080  | total loss: 1.34185 | time: 19.161s
| Adam | epoch: 055 | loss: 1.34185 - acc: 0.5169 | val_loss: 1.48905 - val_acc: 0.4501 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3136  | total loss: 1.27529 | time: 19.150s
| Adam | epoch: 056 | loss: 1.27529 - acc: 0.5035 | val_loss: 1.46776 - val_acc: 0.4501 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3192  | total loss: 1.26044 | time: 19.176s
| Adam | epoch: 057 | loss: 1.26044 - acc: 0.5329 | val_loss: 1.47364 - val_acc: 0.4474 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3248  | total loss: 1.29357 | time: 19.174s
| Adam | epoch: 058 | loss: 1.29357 - acc: 0.5281 | val_loss: 1.48392 - val_acc: 0.4582 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3304  | total loss: 1.34587 | time: 19.253s
| Adam | epoch: 059 | loss: 1.34587 - acc: 0.5019 | val_loss: 1.47349 - val_acc: 0.4582 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3360  | total loss: 1.30882 | time: 19.159s
| Adam | epoch: 060 | loss: 1.30882 - acc: 0.5117 | val_loss: 1.48931 - val_acc: 0.4501 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3416  | total loss: 1.27844 | time: 19.195s
| Adam | epoch: 061 | loss: 1.27844 - acc: 0.5104 | val_loss: 1.48946 - val_acc: 0.4555 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3472  | total loss: 1.26367 | time: 19.078s
| Adam | epoch: 062 | loss: 1.26367 - acc: 0.5367 | val_loss: 1.51739 - val_acc: 0.4555 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3528  | total loss: 1.27832 | time: 19.235s
| Adam | epoch: 063 | loss: 1.27832 - acc: 0.5238 | val_loss: 1.51289 - val_acc: 0.4555 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3584  | total loss: 1.26506 | time: 19.233s
| Adam | epoch: 064 | loss: 1.26506 - acc: 0.5247 | val_loss: 1.49284 - val_acc: 0.4690 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3640  | total loss: 1.24172 | time: 19.150s
| Adam | epoch: 065 | loss: 1.24172 - acc: 0.5534 | val_loss: 1.47606 - val_acc: 0.4717 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3696  | total loss: 1.24704 | time: 19.218s
| Adam | epoch: 066 | loss: 1.24704 - acc: 0.5502 | val_loss: 1.48509 - val_acc: 0.4690 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3752  | total loss: 1.28794 | time: 19.125s
| Adam | epoch: 067 | loss: 1.28794 - acc: 0.5106 | val_loss: 1.47907 - val_acc: 0.4582 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3808  | total loss: 1.27988 | time: 19.132s
| Adam | epoch: 068 | loss: 1.27988 - acc: 0.5269 | val_loss: 1.46829 - val_acc: 0.4501 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3864  | total loss: 1.25284 | time: 19.173s
| Adam | epoch: 069 | loss: 1.25284 - acc: 0.5462 | val_loss: 1.47072 - val_acc: 0.4663 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3920  | total loss: 1.26183 | time: 19.159s
| Adam | epoch: 070 | loss: 1.26183 - acc: 0.5314 | val_loss: 1.47174 - val_acc: 0.4609 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3976  | total loss: 1.28529 | time: 19.167s
| Adam | epoch: 071 | loss: 1.28529 - acc: 0.5236 | val_loss: 1.48886 - val_acc: 0.4528 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4032  | total loss: 1.20365 | time: 19.303s
| Adam | epoch: 072 | loss: 1.20365 - acc: 0.5663 | val_loss: 1.49467 - val_acc: 0.4636 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4088  | total loss: 1.22866 | time: 19.139s
| Adam | epoch: 073 | loss: 1.22866 - acc: 0.5507 | val_loss: 1.49604 - val_acc: 0.4474 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4144  | total loss: 1.19971 | time: 19.180s
| Adam | epoch: 074 | loss: 1.19971 - acc: 0.5584 | val_loss: 1.51318 - val_acc: 0.4555 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4200  | total loss: 1.18970 | time: 19.279s
| Adam | epoch: 075 | loss: 1.18970 - acc: 0.5515 | val_loss: 1.49963 - val_acc: 0.4798 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4256  | total loss: 1.22426 | time: 19.204s
| Adam | epoch: 076 | loss: 1.22426 - acc: 0.5719 | val_loss: 1.48047 - val_acc: 0.4528 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4312  | total loss: 1.22320 | time: 19.199s
| Adam | epoch: 077 | loss: 1.22320 - acc: 0.5449 | val_loss: 1.48900 - val_acc: 0.4825 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4368  | total loss: 1.22589 | time: 19.207s
| Adam | epoch: 078 | loss: 1.22589 - acc: 0.5517 | val_loss: 1.47955 - val_acc: 0.4636 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4424  | total loss: 1.25803 | time: 19.211s
| Adam | epoch: 079 | loss: 1.25803 - acc: 0.5445 | val_loss: 1.45988 - val_acc: 0.4825 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4480  | total loss: 1.17424 | time: 19.173s
| Adam | epoch: 080 | loss: 1.17424 - acc: 0.5694 | val_loss: 1.49488 - val_acc: 0.4690 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4536  | total loss: 1.23683 | time: 19.303s
| Adam | epoch: 081 | loss: 1.23683 - acc: 0.5357 | val_loss: 1.47149 - val_acc: 0.4690 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4592  | total loss: 1.23648 | time: 19.115s
| Adam | epoch: 082 | loss: 1.23648 - acc: 0.5454 | val_loss: 1.48418 - val_acc: 0.4690 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4648  | total loss: 1.21930 | time: 19.106s
| Adam | epoch: 083 | loss: 1.21930 - acc: 0.5613 | val_loss: 1.48803 - val_acc: 0.4798 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4704  | total loss: 1.26275 | time: 19.165s
| Adam | epoch: 084 | loss: 1.26275 - acc: 0.5447 | val_loss: 1.51490 - val_acc: 0.4555 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4760  | total loss: 1.18929 | time: 19.210s
| Adam | epoch: 085 | loss: 1.18929 - acc: 0.5483 | val_loss: 1.50419 - val_acc: 0.4636 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4816  | total loss: 1.24638 | time: 19.194s
| Adam | epoch: 086 | loss: 1.24638 - acc: 0.5436 | val_loss: 1.48896 - val_acc: 0.4582 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4872  | total loss: 1.14719 | time: 19.107s
| Adam | epoch: 087 | loss: 1.14719 - acc: 0.5874 | val_loss: 1.51823 - val_acc: 0.4744 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4928  | total loss: 1.25201 | time: 19.176s
| Adam | epoch: 088 | loss: 1.25201 - acc: 0.5429 | val_loss: 1.49784 - val_acc: 0.4663 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4984  | total loss: 1.25693 | time: 19.158s
| Adam | epoch: 089 | loss: 1.25693 - acc: 0.5591 | val_loss: 1.50441 - val_acc: 0.4825 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5040  | total loss: 1.14965 | time: 19.148s
| Adam | epoch: 090 | loss: 1.14965 - acc: 0.5860 | val_loss: 1.50353 - val_acc: 0.4744 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5096  | total loss: 1.18748 | time: 19.101s
| Adam | epoch: 091 | loss: 1.18748 - acc: 0.5599 | val_loss: 1.49398 - val_acc: 0.4852 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5152  | total loss: 1.13516 | time: 19.189s
| Adam | epoch: 092 | loss: 1.13516 - acc: 0.5861 | val_loss: 1.48548 - val_acc: 0.4663 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5208  | total loss: 1.13599 | time: 19.086s
| Adam | epoch: 093 | loss: 1.13599 - acc: 0.5809 | val_loss: 1.52376 - val_acc: 0.4609 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5264  | total loss: 1.09215 | time: 19.229s
| Adam | epoch: 094 | loss: 1.09215 - acc: 0.5973 | val_loss: 1.52346 - val_acc: 0.4636 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5320  | total loss: 1.07960 | time: 19.195s
| Adam | epoch: 095 | loss: 1.07960 - acc: 0.5996 | val_loss: 1.56174 - val_acc: 0.4798 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5376  | total loss: 1.11222 | time: 19.209s
| Adam | epoch: 096 | loss: 1.11222 - acc: 0.6030 | val_loss: 1.55363 - val_acc: 0.4825 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5432  | total loss: 1.10734 | time: 19.149s
| Adam | epoch: 097 | loss: 1.10734 - acc: 0.5905 | val_loss: 1.56141 - val_acc: 0.4798 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5488  | total loss: 1.08363 | time: 19.136s
| Adam | epoch: 098 | loss: 1.08363 - acc: 0.5925 | val_loss: 1.53848 - val_acc: 0.4744 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5544  | total loss: 1.12432 | time: 19.197s
| Adam | epoch: 099 | loss: 1.12432 - acc: 0.5800 | val_loss: 1.55388 - val_acc: 0.4582 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5600  | total loss: 1.13356 | time: 19.181s
| Adam | epoch: 100 | loss: 1.13356 - acc: 0.5652 | val_loss: 1.56724 - val_acc: 0.4609 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

