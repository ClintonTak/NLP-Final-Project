Training Step: 108  | total loss: 1.66180 | time: 72.239s
| Adam | epoch: 001 | loss: 1.66180 - acc: 0.3182 | val_loss: 1.62745 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 216  | total loss: 1.59325 | time: 27.214s
| Adam | epoch: 002 | loss: 1.59325 - acc: 0.3577 | val_loss: 1.63245 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 324  | total loss: 1.58325 | time: 27.200s
| Adam | epoch: 003 | loss: 1.58325 - acc: 0.3516 | val_loss: 1.63337 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 432  | total loss: 1.56558 | time: 27.298s
| Adam | epoch: 004 | loss: 1.56558 - acc: 0.3514 | val_loss: 1.63839 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 540  | total loss: 1.60647 | time: 27.204s
| Adam | epoch: 005 | loss: 1.60647 - acc: 0.3543 | val_loss: 1.62870 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 648  | total loss: 1.58242 | time: 27.216s
| Adam | epoch: 006 | loss: 1.58242 - acc: 0.3742 | val_loss: 1.63116 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 756  | total loss: 1.62063 | time: 27.301s
| Adam | epoch: 007 | loss: 1.62063 - acc: 0.3338 | val_loss: 1.63309 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 864  | total loss: 1.62101 | time: 27.313s
| Adam | epoch: 008 | loss: 1.62101 - acc: 0.2903 | val_loss: 1.63288 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 972  | total loss: 1.58498 | time: 27.236s
| Adam | epoch: 009 | loss: 1.58498 - acc: 0.3467 | val_loss: 1.63397 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1080  | total loss: 1.62416 | time: 27.237s
| Adam | epoch: 010 | loss: 1.62416 - acc: 0.3346 | val_loss: 1.62362 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1188  | total loss: 1.58497 | time: 27.230s
| Adam | epoch: 011 | loss: 1.58497 - acc: 0.3250 | val_loss: 1.62954 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1296  | total loss: 1.59828 | time: 27.205s
| Adam | epoch: 012 | loss: 1.59828 - acc: 0.3258 | val_loss: 1.63263 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1404  | total loss: 1.61792 | time: 27.202s
| Adam | epoch: 013 | loss: 1.61792 - acc: 0.3559 | val_loss: 1.62609 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1512  | total loss: 1.63883 | time: 27.217s
| Adam | epoch: 014 | loss: 1.63883 - acc: 0.3238 | val_loss: 1.63283 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1620  | total loss: 1.57919 | time: 27.283s
| Adam | epoch: 015 | loss: 1.57919 - acc: 0.3542 | val_loss: 1.63544 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1728  | total loss: 1.57264 | time: 27.209s
| Adam | epoch: 016 | loss: 1.57264 - acc: 0.3446 | val_loss: 1.63450 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1836  | total loss: 1.62943 | time: 27.227s
| Adam | epoch: 017 | loss: 1.62943 - acc: 0.2998 | val_loss: 1.62887 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1944  | total loss: 1.59288 | time: 27.168s
| Adam | epoch: 018 | loss: 1.59288 - acc: 0.3450 | val_loss: 1.62782 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2052  | total loss: 1.64108 | time: 27.193s
| Adam | epoch: 019 | loss: 1.64108 - acc: 0.2992 | val_loss: 1.63667 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2160  | total loss: 1.62300 | time: 27.238s
| Adam | epoch: 020 | loss: 1.62300 - acc: 0.3308 | val_loss: 1.62809 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2268  | total loss: 1.59159 | time: 27.250s
| Adam | epoch: 021 | loss: 1.59159 - acc: 0.3331 | val_loss: 1.62916 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2376  | total loss: 1.59866 | time: 27.253s
| Adam | epoch: 022 | loss: 1.59866 - acc: 0.3492 | val_loss: 1.63319 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2484  | total loss: 1.59458 | time: 27.237s
| Adam | epoch: 023 | loss: 1.59458 - acc: 0.3580 | val_loss: 1.62844 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2592  | total loss: 1.55461 | time: 27.188s
| Adam | epoch: 024 | loss: 1.55461 - acc: 0.3743 | val_loss: 1.63669 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2700  | total loss: 1.59405 | time: 27.238s
| Adam | epoch: 025 | loss: 1.59405 - acc: 0.3373 | val_loss: 1.62683 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2808  | total loss: 1.59284 | time: 27.310s
| Adam | epoch: 026 | loss: 1.59284 - acc: 0.3593 | val_loss: 1.62756 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2916  | total loss: 1.63507 | time: 27.251s
| Adam | epoch: 027 | loss: 1.63507 - acc: 0.2991 | val_loss: 1.62711 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3024  | total loss: 1.64709 | time: 27.292s
| Adam | epoch: 028 | loss: 1.64709 - acc: 0.3344 | val_loss: 1.62465 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3132  | total loss: 1.57316 | time: 27.349s
| Adam | epoch: 029 | loss: 1.57316 - acc: 0.3514 | val_loss: 1.62728 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3240  | total loss: 1.58900 | time: 27.303s
| Adam | epoch: 030 | loss: 1.58900 - acc: 0.3555 | val_loss: 1.62779 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3348  | total loss: 1.59871 | time: 27.336s
| Adam | epoch: 031 | loss: 1.59871 - acc: 0.3298 | val_loss: 1.62887 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3456  | total loss: 1.63316 | time: 27.338s
| Adam | epoch: 032 | loss: 1.63316 - acc: 0.3347 | val_loss: 1.62748 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3564  | total loss: 1.60415 | time: 27.189s
| Adam | epoch: 033 | loss: 1.60415 - acc: 0.3302 | val_loss: 1.63054 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3672  | total loss: 1.61013 | time: 27.294s
| Adam | epoch: 034 | loss: 1.61013 - acc: 0.3418 | val_loss: 1.62900 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3780  | total loss: 1.58273 | time: 27.390s
| Adam | epoch: 035 | loss: 1.58273 - acc: 0.3477 | val_loss: 1.63005 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3888  | total loss: 1.58664 | time: 27.377s
| Adam | epoch: 036 | loss: 1.58664 - acc: 0.3644 | val_loss: 1.63192 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3996  | total loss: 1.63199 | time: 27.303s
| Adam | epoch: 037 | loss: 1.63199 - acc: 0.3275 | val_loss: 1.63229 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4104  | total loss: 1.57258 | time: 27.389s
| Adam | epoch: 038 | loss: 1.57258 - acc: 0.3662 | val_loss: 1.62868 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4212  | total loss: 1.57743 | time: 27.321s
| Adam | epoch: 039 | loss: 1.57743 - acc: 0.3500 | val_loss: 1.63263 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4320  | total loss: 1.59563 | time: 27.337s
| Adam | epoch: 040 | loss: 1.59563 - acc: 0.3508 | val_loss: 1.63000 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4428  | total loss: 1.59635 | time: 27.291s
| Adam | epoch: 041 | loss: 1.59635 - acc: 0.3473 | val_loss: 1.62965 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4536  | total loss: 1.58139 | time: 27.386s
| Adam | epoch: 042 | loss: 1.58139 - acc: 0.3567 | val_loss: 1.62842 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4644  | total loss: 1.62572 | time: 27.334s
| Adam | epoch: 043 | loss: 1.62572 - acc: 0.2986 | val_loss: 1.63436 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4752  | total loss: 1.61722 | time: 27.232s
| Adam | epoch: 044 | loss: 1.61722 - acc: 0.3239 | val_loss: 1.62903 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4860  | total loss: 1.64696 | time: 27.356s
| Adam | epoch: 045 | loss: 1.64696 - acc: 0.3274 | val_loss: 1.62723 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4968  | total loss: 1.59998 | time: 27.365s
| Adam | epoch: 046 | loss: 1.59998 - acc: 0.3241 | val_loss: 1.62943 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5076  | total loss: 1.57973 | time: 27.313s
| Adam | epoch: 047 | loss: 1.57973 - acc: 0.3674 | val_loss: 1.62926 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5184  | total loss: 1.58753 | time: 27.235s
| Adam | epoch: 048 | loss: 1.58753 - acc: 0.3517 | val_loss: 1.62728 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5292  | total loss: 1.60535 | time: 27.265s
| Adam | epoch: 049 | loss: 1.60535 - acc: 0.3579 | val_loss: 1.62934 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5400  | total loss: 1.62039 | time: 27.305s
| Adam | epoch: 050 | loss: 1.62039 - acc: 0.3402 | val_loss: 1.63113 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5508  | total loss: 1.60639 | time: 27.396s
| Adam | epoch: 051 | loss: 1.60639 - acc: 0.3206 | val_loss: 1.62689 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5616  | total loss: 1.59839 | time: 27.321s
| Adam | epoch: 052 | loss: 1.59839 - acc: 0.3386 | val_loss: 1.62565 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5724  | total loss: 1.57392 | time: 27.282s
| Adam | epoch: 053 | loss: 1.57392 - acc: 0.3578 | val_loss: 1.62986 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5832  | total loss: 1.60305 | time: 27.314s
| Adam | epoch: 054 | loss: 1.60305 - acc: 0.3424 | val_loss: 1.62728 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5940  | total loss: 1.58444 | time: 27.288s
| Adam | epoch: 055 | loss: 1.58444 - acc: 0.3548 | val_loss: 1.62859 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6048  | total loss: 1.59853 | time: 27.277s
| Adam | epoch: 056 | loss: 1.59853 - acc: 0.3602 | val_loss: 1.62726 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6156  | total loss: 1.58982 | time: 27.320s
| Adam | epoch: 057 | loss: 1.58982 - acc: 0.3605 | val_loss: 1.62711 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6264  | total loss: 1.61752 | time: 27.299s
| Adam | epoch: 058 | loss: 1.61752 - acc: 0.3294 | val_loss: 1.63282 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6372  | total loss: 1.60705 | time: 27.326s
| Adam | epoch: 059 | loss: 1.60705 - acc: 0.3240 | val_loss: 1.62999 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6480  | total loss: 1.55933 | time: 27.344s
| Adam | epoch: 060 | loss: 1.55933 - acc: 0.3407 | val_loss: 1.63135 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6588  | total loss: 1.65588 | time: 27.269s
| Adam | epoch: 061 | loss: 1.65588 - acc: 0.2869 | val_loss: 1.63005 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6696  | total loss: 1.62783 | time: 27.266s
| Adam | epoch: 062 | loss: 1.62783 - acc: 0.3129 | val_loss: 1.62666 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6804  | total loss: 1.61640 | time: 27.308s
| Adam | epoch: 063 | loss: 1.61640 - acc: 0.3142 | val_loss: 1.63276 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6912  | total loss: 1.61709 | time: 27.355s
| Adam | epoch: 064 | loss: 1.61709 - acc: 0.3433 | val_loss: 1.62983 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7020  | total loss: 1.57182 | time: 27.289s
| Adam | epoch: 065 | loss: 1.57182 - acc: 0.3574 | val_loss: 1.63262 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7128  | total loss: 1.58645 | time: 27.336s
| Adam | epoch: 066 | loss: 1.58645 - acc: 0.3601 | val_loss: 1.62827 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7236  | total loss: 1.58391 | time: 27.374s
| Adam | epoch: 067 | loss: 1.58391 - acc: 0.3431 | val_loss: 1.62818 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7344  | total loss: 1.59188 | time: 27.289s
| Adam | epoch: 068 | loss: 1.59188 - acc: 0.3394 | val_loss: 1.63015 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7452  | total loss: 1.59745 | time: 27.237s
| Adam | epoch: 069 | loss: 1.59745 - acc: 0.3446 | val_loss: 1.62799 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7560  | total loss: 1.57446 | time: 27.345s
| Adam | epoch: 070 | loss: 1.57446 - acc: 0.3703 | val_loss: 1.62853 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7668  | total loss: 1.60024 | time: 27.359s
| Adam | epoch: 071 | loss: 1.60024 - acc: 0.3607 | val_loss: 1.62800 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7776  | total loss: 1.60778 | time: 27.241s
| Adam | epoch: 072 | loss: 1.60778 - acc: 0.3461 | val_loss: 1.63340 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7884  | total loss: 1.61569 | time: 27.268s
| Adam | epoch: 073 | loss: 1.61569 - acc: 0.3361 | val_loss: 1.62709 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7992  | total loss: 1.55554 | time: 27.315s
| Adam | epoch: 074 | loss: 1.55554 - acc: 0.3642 | val_loss: 1.63367 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8100  | total loss: 1.57097 | time: 27.271s
| Adam | epoch: 075 | loss: 1.57097 - acc: 0.3490 | val_loss: 1.63022 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8208  | total loss: 1.60296 | time: 27.321s
| Adam | epoch: 076 | loss: 1.60296 - acc: 0.3196 | val_loss: 1.62896 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8316  | total loss: 1.57430 | time: 27.308s
| Adam | epoch: 077 | loss: 1.57430 - acc: 0.3404 | val_loss: 1.63231 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8424  | total loss: 1.59724 | time: 27.372s
| Adam | epoch: 078 | loss: 1.59724 - acc: 0.3393 | val_loss: 1.62784 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8532  | total loss: 1.61005 | time: 27.292s
| Adam | epoch: 079 | loss: 1.61005 - acc: 0.3467 | val_loss: 1.63019 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8640  | total loss: 1.57432 | time: 27.319s
| Adam | epoch: 080 | loss: 1.57432 - acc: 0.3563 | val_loss: 1.62800 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8748  | total loss: 1.53806 | time: 27.228s
| Adam | epoch: 081 | loss: 1.53806 - acc: 0.4001 | val_loss: 1.63208 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8856  | total loss: 1.61962 | time: 27.346s
| Adam | epoch: 082 | loss: 1.61962 - acc: 0.3286 | val_loss: 1.63477 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8964  | total loss: 1.61637 | time: 27.357s
| Adam | epoch: 083 | loss: 1.61637 - acc: 0.3447 | val_loss: 1.62949 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9072  | total loss: 1.61988 | time: 27.314s
| Adam | epoch: 084 | loss: 1.61988 - acc: 0.3225 | val_loss: 1.63507 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9180  | total loss: 1.59675 | time: 27.375s
| Adam | epoch: 085 | loss: 1.59675 - acc: 0.3583 | val_loss: 1.63038 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9288  | total loss: 1.60231 | time: 27.354s
| Adam | epoch: 086 | loss: 1.60231 - acc: 0.3469 | val_loss: 1.63335 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9396  | total loss: 1.57220 | time: 27.411s
| Adam | epoch: 087 | loss: 1.57220 - acc: 0.3483 | val_loss: 1.62554 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9504  | total loss: 1.57649 | time: 27.288s
| Adam | epoch: 088 | loss: 1.57649 - acc: 0.3650 | val_loss: 1.62625 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9612  | total loss: 1.57096 | time: 27.308s
| Adam | epoch: 089 | loss: 1.57096 - acc: 0.3617 | val_loss: 1.62723 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9720  | total loss: 1.59153 | time: 27.324s
| Adam | epoch: 090 | loss: 1.59153 - acc: 0.3471 | val_loss: 1.63292 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9828  | total loss: 1.57666 | time: 27.280s
| Adam | epoch: 091 | loss: 1.57666 - acc: 0.3948 | val_loss: 1.59327 - val_acc: 0.3570 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9936  | total loss: 1.59094 | time: 27.359s
| Adam | epoch: 092 | loss: 1.59094 - acc: 0.3880 | val_loss: 1.60057 - val_acc: 0.4278 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10044  | total loss: 1.45902 | time: 27.238s
| Adam | epoch: 093 | loss: 1.45902 - acc: 0.4380 | val_loss: 1.63153 - val_acc: 0.4147 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10152  | total loss: 1.48569 | time: 27.322s
| Adam | epoch: 094 | loss: 1.48569 - acc: 0.4468 | val_loss: 1.62527 - val_acc: 0.4147 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10260  | total loss: 1.50642 | time: 27.350s
| Adam | epoch: 095 | loss: 1.50642 - acc: 0.4327 | val_loss: 1.60541 - val_acc: 0.3858 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10368  | total loss: 1.45769 | time: 27.429s
| Adam | epoch: 096 | loss: 1.45769 - acc: 0.4115 | val_loss: 1.63020 - val_acc: 0.3701 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10476  | total loss: 1.53529 | time: 27.289s
| Adam | epoch: 097 | loss: 1.53529 - acc: 0.3918 | val_loss: 1.61800 - val_acc: 0.3648 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10584  | total loss: 1.55431 | time: 27.312s
| Adam | epoch: 098 | loss: 1.55431 - acc: 0.3735 | val_loss: 1.60254 - val_acc: 0.3780 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10692  | total loss: 1.44407 | time: 27.405s
| Adam | epoch: 099 | loss: 1.44407 - acc: 0.4461 | val_loss: 1.59747 - val_acc: 0.3911 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10800  | total loss: 1.40594 | time: 27.301s
| Adam | epoch: 100 | loss: 1.40594 - acc: 0.4633 | val_loss: 1.59240 - val_acc: 0.4121 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

