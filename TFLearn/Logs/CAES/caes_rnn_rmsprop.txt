Training Step: 58  | total loss: 1.77970 | time: 57.758s
| RMSProp | epoch: 001 | loss: 1.77970 - acc: 0.3448 | val_loss: 1.77521 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 116  | total loss: 1.63980 | time: 13.110s
| RMSProp | epoch: 002 | loss: 1.63980 - acc: 0.3490 | val_loss: 1.59009 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 174  | total loss: 1.62202 | time: 13.145s
| RMSProp | epoch: 003 | loss: 1.62202 - acc: 0.3420 | val_loss: 1.58412 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 232  | total loss: 1.63440 | time: 13.128s
| RMSProp | epoch: 004 | loss: 1.63440 - acc: 0.3204 | val_loss: 1.59293 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 290  | total loss: 1.60918 | time: 13.062s
| RMSProp | epoch: 005 | loss: 1.60918 - acc: 0.3355 | val_loss: 1.58803 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 348  | total loss: 1.58901 | time: 13.073s
| RMSProp | epoch: 006 | loss: 1.58901 - acc: 0.3523 | val_loss: 1.58580 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 406  | total loss: 1.61039 | time: 13.147s
| RMSProp | epoch: 007 | loss: 1.61039 - acc: 0.3294 | val_loss: 1.58696 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 464  | total loss: 1.60748 | time: 13.101s
| RMSProp | epoch: 008 | loss: 1.60748 - acc: 0.3288 | val_loss: 1.58927 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 522  | total loss: 1.59461 | time: 13.115s
| RMSProp | epoch: 009 | loss: 1.59461 - acc: 0.3582 | val_loss: 1.58955 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 580  | total loss: 1.61739 | time: 13.122s
| RMSProp | epoch: 010 | loss: 1.61739 - acc: 0.3313 | val_loss: 1.58449 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 638  | total loss: 1.58048 | time: 13.137s
| RMSProp | epoch: 011 | loss: 1.58048 - acc: 0.3406 | val_loss: 1.58896 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 696  | total loss: 1.61832 | time: 13.137s
| RMSProp | epoch: 012 | loss: 1.61832 - acc: 0.3475 | val_loss: 1.58749 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 754  | total loss: 1.63127 | time: 13.133s
| RMSProp | epoch: 013 | loss: 1.63127 - acc: 0.3243 | val_loss: 1.58929 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 812  | total loss: 1.62538 | time: 13.121s
| RMSProp | epoch: 014 | loss: 1.62538 - acc: 0.3391 | val_loss: 1.58639 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 870  | total loss: 1.60026 | time: 13.126s
| RMSProp | epoch: 015 | loss: 1.60026 - acc: 0.3504 | val_loss: 1.58413 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 928  | total loss: 1.61784 | time: 13.122s
| RMSProp | epoch: 016 | loss: 1.61784 - acc: 0.3238 | val_loss: 1.58711 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 986  | total loss: 1.64123 | time: 13.117s
| RMSProp | epoch: 017 | loss: 1.64123 - acc: 0.3210 | val_loss: 1.59121 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1044  | total loss: 1.61328 | time: 13.127s
| RMSProp | epoch: 018 | loss: 1.61328 - acc: 0.3435 | val_loss: 1.58651 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1102  | total loss: 1.59963 | time: 13.109s
| RMSProp | epoch: 019 | loss: 1.59963 - acc: 0.3379 | val_loss: 1.58328 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1160  | total loss: 1.59552 | time: 13.133s
| RMSProp | epoch: 020 | loss: 1.59552 - acc: 0.3351 | val_loss: 1.58303 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1218  | total loss: 1.59231 | time: 13.152s
| RMSProp | epoch: 021 | loss: 1.59231 - acc: 0.3281 | val_loss: 1.58862 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1276  | total loss: 1.62592 | time: 13.129s
| RMSProp | epoch: 022 | loss: 1.62592 - acc: 0.3354 | val_loss: 1.58574 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1334  | total loss: 1.59341 | time: 13.094s
| RMSProp | epoch: 023 | loss: 1.59341 - acc: 0.3401 | val_loss: 1.58355 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1392  | total loss: 1.60359 | time: 13.097s
| RMSProp | epoch: 024 | loss: 1.60359 - acc: 0.3449 | val_loss: 1.58438 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1450  | total loss: 1.61284 | time: 13.148s
| RMSProp | epoch: 025 | loss: 1.61284 - acc: 0.3316 | val_loss: 1.58476 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1508  | total loss: 1.62101 | time: 13.077s
| RMSProp | epoch: 026 | loss: 1.62101 - acc: 0.3430 | val_loss: 1.58621 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1566  | total loss: 1.61773 | time: 13.149s
| RMSProp | epoch: 027 | loss: 1.61773 - acc: 0.3410 | val_loss: 1.58375 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1624  | total loss: 1.59415 | time: 13.120s
| RMSProp | epoch: 028 | loss: 1.59415 - acc: 0.3564 | val_loss: 1.58599 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1682  | total loss: 1.59255 | time: 13.096s
| RMSProp | epoch: 029 | loss: 1.59255 - acc: 0.3529 | val_loss: 1.58256 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1740  | total loss: 1.60128 | time: 13.128s
| RMSProp | epoch: 030 | loss: 1.60128 - acc: 0.3409 | val_loss: 1.58543 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1798  | total loss: 1.59041 | time: 13.143s
| RMSProp | epoch: 031 | loss: 1.59041 - acc: 0.3403 | val_loss: 1.58691 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1856  | total loss: 1.58174 | time: 13.038s
| RMSProp | epoch: 032 | loss: 1.58174 - acc: 0.3601 | val_loss: 1.58586 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1914  | total loss: 1.60790 | time: 13.097s
| RMSProp | epoch: 033 | loss: 1.60790 - acc: 0.3563 | val_loss: 1.58494 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1972  | total loss: 1.59734 | time: 13.106s
| RMSProp | epoch: 034 | loss: 1.59734 - acc: 0.3442 | val_loss: 1.58380 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2030  | total loss: 1.58493 | time: 13.115s
| RMSProp | epoch: 035 | loss: 1.58493 - acc: 0.3495 | val_loss: 1.58511 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2088  | total loss: 1.61647 | time: 13.119s
| RMSProp | epoch: 036 | loss: 1.61647 - acc: 0.3357 | val_loss: 1.58646 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2146  | total loss: 1.61468 | time: 13.113s
| RMSProp | epoch: 037 | loss: 1.61468 - acc: 0.3302 | val_loss: 1.58354 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2204  | total loss: 1.61465 | time: 13.019s
| RMSProp | epoch: 038 | loss: 1.61465 - acc: 0.3296 | val_loss: 1.58480 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2262  | total loss: 1.63420 | time: 13.057s
| RMSProp | epoch: 039 | loss: 1.63420 - acc: 0.3151 | val_loss: 1.58856 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2320  | total loss: 1.59436 | time: 13.051s
| RMSProp | epoch: 040 | loss: 1.59436 - acc: 0.3376 | val_loss: 1.58705 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2378  | total loss: 1.60804 | time: 13.148s
| RMSProp | epoch: 041 | loss: 1.60804 - acc: 0.3257 | val_loss: 1.58712 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2436  | total loss: 1.59764 | time: 13.113s
| RMSProp | epoch: 042 | loss: 1.59764 - acc: 0.3432 | val_loss: 1.58571 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2494  | total loss: 1.61507 | time: 13.105s
| RMSProp | epoch: 043 | loss: 1.61507 - acc: 0.3339 | val_loss: 1.58457 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2552  | total loss: 1.57624 | time: 13.122s
| RMSProp | epoch: 044 | loss: 1.57624 - acc: 0.3520 | val_loss: 1.58583 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2610  | total loss: 1.60507 | time: 13.087s
| RMSProp | epoch: 045 | loss: 1.60507 - acc: 0.3590 | val_loss: 1.58525 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2668  | total loss: 1.60694 | time: 13.067s
| RMSProp | epoch: 046 | loss: 1.60694 - acc: 0.3300 | val_loss: 1.58681 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2726  | total loss: 1.59383 | time: 13.120s
| RMSProp | epoch: 047 | loss: 1.59383 - acc: 0.3560 | val_loss: 1.58303 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2784  | total loss: 1.59786 | time: 13.134s
| RMSProp | epoch: 048 | loss: 1.59786 - acc: 0.3531 | val_loss: 1.58382 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2842  | total loss: 1.61544 | time: 13.064s
| RMSProp | epoch: 049 | loss: 1.61544 - acc: 0.3007 | val_loss: 1.58998 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2900  | total loss: 1.60244 | time: 13.115s
| RMSProp | epoch: 050 | loss: 1.60244 - acc: 0.3498 | val_loss: 1.58310 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2958  | total loss: 1.62223 | time: 13.157s
| RMSProp | epoch: 051 | loss: 1.62223 - acc: 0.3213 | val_loss: 1.58782 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3016  | total loss: 1.60678 | time: 13.086s
| RMSProp | epoch: 052 | loss: 1.60678 - acc: 0.3512 | val_loss: 1.58476 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3074  | total loss: 1.63209 | time: 13.087s
| RMSProp | epoch: 053 | loss: 1.63209 - acc: 0.3225 | val_loss: 1.58645 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3132  | total loss: 1.56759 | time: 13.079s
| RMSProp | epoch: 054 | loss: 1.56759 - acc: 0.3561 | val_loss: 1.58318 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3190  | total loss: 1.59419 | time: 13.093s
| RMSProp | epoch: 055 | loss: 1.59419 - acc: 0.3204 | val_loss: 1.59051 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3248  | total loss: 1.61609 | time: 13.092s
| RMSProp | epoch: 056 | loss: 1.61609 - acc: 0.3156 | val_loss: 1.58592 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3306  | total loss: 1.64257 | time: 13.128s
| RMSProp | epoch: 057 | loss: 1.64257 - acc: 0.3059 | val_loss: 1.58451 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3364  | total loss: 1.64261 | time: 13.040s
| RMSProp | epoch: 058 | loss: 1.64261 - acc: 0.3081 | val_loss: 1.58870 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3422  | total loss: 1.62103 | time: 13.079s
| RMSProp | epoch: 059 | loss: 1.62103 - acc: 0.3399 | val_loss: 1.58432 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3480  | total loss: 1.57981 | time: 13.106s
| RMSProp | epoch: 060 | loss: 1.57981 - acc: 0.3511 | val_loss: 1.58492 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3538  | total loss: 1.62904 | time: 13.069s
| RMSProp | epoch: 061 | loss: 1.62904 - acc: 0.3293 | val_loss: 1.58656 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3596  | total loss: 1.60387 | time: 13.080s
| RMSProp | epoch: 062 | loss: 1.60387 - acc: 0.3619 | val_loss: 1.58303 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3654  | total loss: 1.58592 | time: 13.120s
| RMSProp | epoch: 063 | loss: 1.58592 - acc: 0.3578 | val_loss: 1.58370 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3712  | total loss: 1.60286 | time: 13.060s
| RMSProp | epoch: 064 | loss: 1.60286 - acc: 0.3303 | val_loss: 1.58688 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3770  | total loss: 1.61699 | time: 13.133s
| RMSProp | epoch: 065 | loss: 1.61699 - acc: 0.3350 | val_loss: 1.58462 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3828  | total loss: 1.61133 | time: 13.199s
| RMSProp | epoch: 066 | loss: 1.61133 - acc: 0.3331 | val_loss: 1.58466 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3886  | total loss: 1.57093 | time: 13.049s
| RMSProp | epoch: 067 | loss: 1.57093 - acc: 0.3597 | val_loss: 1.58748 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3944  | total loss: 1.62000 | time: 13.097s
| RMSProp | epoch: 068 | loss: 1.62000 - acc: 0.3348 | val_loss: 1.58586 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4002  | total loss: 1.60253 | time: 13.101s
| RMSProp | epoch: 069 | loss: 1.60253 - acc: 0.3517 | val_loss: 1.58298 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4060  | total loss: 1.62380 | time: 13.090s
| RMSProp | epoch: 070 | loss: 1.62380 - acc: 0.3367 | val_loss: 1.58431 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4118  | total loss: 1.58837 | time: 13.111s
| RMSProp | epoch: 071 | loss: 1.58837 - acc: 0.3506 | val_loss: 1.58560 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4176  | total loss: 1.61545 | time: 13.101s
| RMSProp | epoch: 072 | loss: 1.61545 - acc: 0.3364 | val_loss: 1.58585 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4234  | total loss: 1.61000 | time: 13.107s
| RMSProp | epoch: 073 | loss: 1.61000 - acc: 0.3422 | val_loss: 1.58559 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4292  | total loss: 1.59333 | time: 13.087s
| RMSProp | epoch: 074 | loss: 1.59333 - acc: 0.3395 | val_loss: 1.58418 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4350  | total loss: 1.59083 | time: 13.086s
| RMSProp | epoch: 075 | loss: 1.59083 - acc: 0.3296 | val_loss: 1.58620 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4408  | total loss: 1.60962 | time: 13.166s
| RMSProp | epoch: 076 | loss: 1.60962 - acc: 0.3623 | val_loss: 1.58328 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4466  | total loss: 1.60029 | time: 13.083s
| RMSProp | epoch: 077 | loss: 1.60029 - acc: 0.3470 | val_loss: 1.58451 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4524  | total loss: 1.60419 | time: 13.205s
| RMSProp | epoch: 078 | loss: 1.60419 - acc: 0.3394 | val_loss: 1.58422 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4582  | total loss: 1.59829 | time: 13.144s
| RMSProp | epoch: 079 | loss: 1.59829 - acc: 0.3302 | val_loss: 1.58485 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4640  | total loss: 1.60763 | time: 13.140s
| RMSProp | epoch: 080 | loss: 1.60763 - acc: 0.3273 | val_loss: 1.58582 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4698  | total loss: 1.62433 | time: 13.113s
| RMSProp | epoch: 081 | loss: 1.62433 - acc: 0.3330 | val_loss: 1.58690 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4756  | total loss: 1.59552 | time: 13.113s
| RMSProp | epoch: 082 | loss: 1.59552 - acc: 0.3494 | val_loss: 1.58418 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4814  | total loss: 1.58835 | time: 13.144s
| RMSProp | epoch: 083 | loss: 1.58835 - acc: 0.3285 | val_loss: 1.58747 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4872  | total loss: 1.60296 | time: 13.117s
| RMSProp | epoch: 084 | loss: 1.60296 - acc: 0.3322 | val_loss: 1.58613 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4930  | total loss: 1.61948 | time: 13.100s
| RMSProp | epoch: 085 | loss: 1.61948 - acc: 0.3517 | val_loss: 1.58671 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4988  | total loss: 1.61859 | time: 13.108s
| RMSProp | epoch: 086 | loss: 1.61859 - acc: 0.3421 | val_loss: 1.58477 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5046  | total loss: 1.59688 | time: 13.118s
| RMSProp | epoch: 087 | loss: 1.59688 - acc: 0.3407 | val_loss: 1.58640 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5104  | total loss: 1.59948 | time: 13.153s
| RMSProp | epoch: 088 | loss: 1.59948 - acc: 0.3466 | val_loss: 1.58466 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5162  | total loss: 1.62898 | time: 13.107s
| RMSProp | epoch: 089 | loss: 1.62898 - acc: 0.2993 | val_loss: 1.59047 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5220  | total loss: 1.58659 | time: 13.121s
| RMSProp | epoch: 090 | loss: 1.58659 - acc: 0.3407 | val_loss: 1.58677 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5278  | total loss: 1.61332 | time: 13.095s
| RMSProp | epoch: 091 | loss: 1.61332 - acc: 0.3441 | val_loss: 1.58502 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5336  | total loss: 1.58012 | time: 13.133s
| RMSProp | epoch: 092 | loss: 1.58012 - acc: 0.3652 | val_loss: 1.58448 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5394  | total loss: 1.61648 | time: 13.123s
| RMSProp | epoch: 093 | loss: 1.61648 - acc: 0.3288 | val_loss: 1.58708 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5452  | total loss: 1.63032 | time: 13.108s
| RMSProp | epoch: 094 | loss: 1.63032 - acc: 0.3324 | val_loss: 1.58559 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5510  | total loss: 1.60802 | time: 13.103s
| RMSProp | epoch: 095 | loss: 1.60802 - acc: 0.3479 | val_loss: 1.58477 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5568  | total loss: 1.60083 | time: 13.120s
| RMSProp | epoch: 096 | loss: 1.60083 - acc: 0.3453 | val_loss: 1.58453 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5626  | total loss: 1.58251 | time: 13.141s
| RMSProp | epoch: 097 | loss: 1.58251 - acc: 0.3591 | val_loss: 1.58375 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5684  | total loss: 1.60701 | time: 13.111s
| RMSProp | epoch: 098 | loss: 1.60701 - acc: 0.3378 | val_loss: 1.58464 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5742  | total loss: 1.61533 | time: 13.109s
| RMSProp | epoch: 099 | loss: 1.61533 - acc: 0.3335 | val_loss: 1.58390 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5800  | total loss: 1.58974 | time: 13.135s
| RMSProp | epoch: 100 | loss: 1.58974 - acc: 0.3620 | val_loss: 1.58318 - val_acc: 0.3727 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

