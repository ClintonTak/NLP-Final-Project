Training Step: 107  | total loss: 1.69999 | time: 4.904s
| RMSProp | epoch: 001 | loss: 1.69999 - acc: 0.3462 | val_loss: 1.66130 - val_acc: 0.3316 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 214  | total loss: 1.58550 | time: 3.979s
| RMSProp | epoch: 002 | loss: 1.58550 - acc: 0.3554 | val_loss: 1.61621 - val_acc: 0.3316 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 321  | total loss: 1.61157 | time: 3.864s
| RMSProp | epoch: 003 | loss: 1.61157 - acc: 0.3273 | val_loss: 1.61527 - val_acc: 0.3316 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 428  | total loss: 1.55936 | time: 3.903s
| RMSProp | epoch: 004 | loss: 1.55936 - acc: 0.3796 | val_loss: 1.61438 - val_acc: 0.3316 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 535  | total loss: 1.55797 | time: 3.960s
| RMSProp | epoch: 005 | loss: 1.55797 - acc: 0.3997 | val_loss: 1.58679 - val_acc: 0.4289 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 642  | total loss: 1.54746 | time: 4.011s
| RMSProp | epoch: 006 | loss: 1.54746 - acc: 0.4009 | val_loss: 1.56940 - val_acc: 0.4289 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 749  | total loss: 1.55864 | time: 3.946s
| RMSProp | epoch: 007 | loss: 1.55864 - acc: 0.4209 | val_loss: 1.55635 - val_acc: 0.4342 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 856  | total loss: 1.56799 | time: 3.997s
| RMSProp | epoch: 008 | loss: 1.56799 - acc: 0.4129 | val_loss: 1.54671 - val_acc: 0.4105 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 963  | total loss: 1.48594 | time: 3.921s
| RMSProp | epoch: 009 | loss: 1.48594 - acc: 0.4458 | val_loss: 1.54023 - val_acc: 0.4316 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 1070  | total loss: 1.53586 | time: 3.891s
| RMSProp | epoch: 010 | loss: 1.53586 - acc: 0.4203 | val_loss: 1.54378 - val_acc: 0.4368 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 1177  | total loss: 1.56322 | time: 3.864s
| RMSProp | epoch: 011 | loss: 1.56322 - acc: 0.4205 | val_loss: 1.53644 - val_acc: 0.4342 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 1284  | total loss: 1.49939 | time: 4.164s
| RMSProp | epoch: 012 | loss: 1.49939 - acc: 0.4280 | val_loss: 1.52553 - val_acc: 0.4447 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 1391  | total loss: 1.45152 | time: 4.077s
| RMSProp | epoch: 013 | loss: 1.45152 - acc: 0.4438 | val_loss: 1.52861 - val_acc: 0.4500 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 1498  | total loss: 1.53683 | time: 4.040s
| RMSProp | epoch: 014 | loss: 1.53683 - acc: 0.4195 | val_loss: 1.53792 - val_acc: 0.4105 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 1605  | total loss: 1.46525 | time: 3.986s
| RMSProp | epoch: 015 | loss: 1.46525 - acc: 0.4423 | val_loss: 1.52295 - val_acc: 0.4553 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 1712  | total loss: 1.53148 | time: 3.819s
| RMSProp | epoch: 016 | loss: 1.53148 - acc: 0.3831 | val_loss: 1.50693 - val_acc: 0.4632 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 1819  | total loss: 1.47122 | time: 4.125s
| RMSProp | epoch: 017 | loss: 1.47122 - acc: 0.4336 | val_loss: 1.50465 - val_acc: 0.4421 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 1926  | total loss: 1.46227 | time: 3.918s
| RMSProp | epoch: 018 | loss: 1.46227 - acc: 0.4391 | val_loss: 1.50511 - val_acc: 0.4632 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 2033  | total loss: 1.51437 | time: 3.926s
| RMSProp | epoch: 019 | loss: 1.51437 - acc: 0.3881 | val_loss: 1.49996 - val_acc: 0.4632 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 2140  | total loss: 1.48903 | time: 3.943s
| RMSProp | epoch: 020 | loss: 1.48903 - acc: 0.4547 | val_loss: 1.49668 - val_acc: 0.4526 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 2247  | total loss: 1.46565 | time: 3.953s
| RMSProp | epoch: 021 | loss: 1.46565 - acc: 0.4409 | val_loss: 1.49704 - val_acc: 0.4526 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 2354  | total loss: 1.44212 | time: 3.994s
| RMSProp | epoch: 022 | loss: 1.44212 - acc: 0.4538 | val_loss: 1.49776 - val_acc: 0.4263 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 2461  | total loss: 1.48651 | time: 3.890s
| RMSProp | epoch: 023 | loss: 1.48651 - acc: 0.4374 | val_loss: 1.50474 - val_acc: 0.4316 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 2568  | total loss: 1.49252 | time: 3.921s
| RMSProp | epoch: 024 | loss: 1.49252 - acc: 0.4156 | val_loss: 1.48632 - val_acc: 0.4632 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 2675  | total loss: 1.48684 | time: 3.819s
| RMSProp | epoch: 025 | loss: 1.48684 - acc: 0.4312 | val_loss: 1.49127 - val_acc: 0.4632 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 2782  | total loss: 1.43686 | time: 3.818s
| RMSProp | epoch: 026 | loss: 1.43686 - acc: 0.4491 | val_loss: 1.49456 - val_acc: 0.4553 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 2889  | total loss: 1.37675 | time: 3.896s
| RMSProp | epoch: 027 | loss: 1.37675 - acc: 0.5088 | val_loss: 1.48705 - val_acc: 0.4658 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 2996  | total loss: 1.50682 | time: 3.813s
| RMSProp | epoch: 028 | loss: 1.50682 - acc: 0.4174 | val_loss: 1.48145 - val_acc: 0.4684 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 3103  | total loss: 1.47731 | time: 3.792s
| RMSProp | epoch: 029 | loss: 1.47731 - acc: 0.4217 | val_loss: 1.49656 - val_acc: 0.4368 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 3210  | total loss: 1.46273 | time: 3.872s
| RMSProp | epoch: 030 | loss: 1.46273 - acc: 0.4218 | val_loss: 1.48589 - val_acc: 0.4342 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 3317  | total loss: 1.46233 | time: 4.107s
| RMSProp | epoch: 031 | loss: 1.46233 - acc: 0.4482 | val_loss: 1.49391 - val_acc: 0.4447 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 3424  | total loss: 1.45042 | time: 3.895s
| RMSProp | epoch: 032 | loss: 1.45042 - acc: 0.4392 | val_loss: 1.49863 - val_acc: 0.4579 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 3531  | total loss: 1.49053 | time: 3.851s
| RMSProp | epoch: 033 | loss: 1.49053 - acc: 0.4313 | val_loss: 1.48666 - val_acc: 0.4605 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 3638  | total loss: 1.46928 | time: 4.015s
| RMSProp | epoch: 034 | loss: 1.46928 - acc: 0.4579 | val_loss: 1.49567 - val_acc: 0.4500 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 3745  | total loss: 1.52722 | time: 4.121s
| RMSProp | epoch: 035 | loss: 1.52722 - acc: 0.4194 | val_loss: 1.48302 - val_acc: 0.4368 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 3852  | total loss: 1.44508 | time: 3.921s
| RMSProp | epoch: 036 | loss: 1.44508 - acc: 0.4487 | val_loss: 1.49108 - val_acc: 0.4526 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 3959  | total loss: 1.40836 | time: 4.121s
| RMSProp | epoch: 037 | loss: 1.40836 - acc: 0.4682 | val_loss: 1.49619 - val_acc: 0.4526 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 4066  | total loss: 1.42884 | time: 4.088s
| RMSProp | epoch: 038 | loss: 1.42884 - acc: 0.4296 | val_loss: 1.49760 - val_acc: 0.4447 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 4173  | total loss: 1.48912 | time: 4.343s
| RMSProp | epoch: 039 | loss: 1.48912 - acc: 0.4106 | val_loss: 1.49469 - val_acc: 0.4474 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 4280  | total loss: 1.46387 | time: 4.136s
| RMSProp | epoch: 040 | loss: 1.46387 - acc: 0.4301 | val_loss: 1.48507 - val_acc: 0.4605 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 4387  | total loss: 1.49827 | time: 4.020s
| RMSProp | epoch: 041 | loss: 1.49827 - acc: 0.4466 | val_loss: 1.50415 - val_acc: 0.4342 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 4494  | total loss: 1.38614 | time: 4.000s
| RMSProp | epoch: 042 | loss: 1.38614 - acc: 0.4848 | val_loss: 1.49244 - val_acc: 0.4368 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 4601  | total loss: 1.40686 | time: 4.043s
| RMSProp | epoch: 043 | loss: 1.40686 - acc: 0.4861 | val_loss: 1.50356 - val_acc: 0.4474 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 4708  | total loss: 1.48591 | time: 4.001s
| RMSProp | epoch: 044 | loss: 1.48591 - acc: 0.4160 | val_loss: 1.49323 - val_acc: 0.4395 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 4815  | total loss: 1.43956 | time: 4.115s
| RMSProp | epoch: 045 | loss: 1.43956 - acc: 0.4578 | val_loss: 1.49385 - val_acc: 0.4474 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 4922  | total loss: 1.43218 | time: 4.045s
| RMSProp | epoch: 046 | loss: 1.43218 - acc: 0.4428 | val_loss: 1.47584 - val_acc: 0.4474 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 5029  | total loss: 1.45122 | time: 4.038s
| RMSProp | epoch: 047 | loss: 1.45122 - acc: 0.4452 | val_loss: 1.48696 - val_acc: 0.4605 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 5136  | total loss: 1.44153 | time: 4.090s
| RMSProp | epoch: 048 | loss: 1.44153 - acc: 0.4561 | val_loss: 1.47860 - val_acc: 0.4579 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 5243  | total loss: 1.43106 | time: 4.329s
| RMSProp | epoch: 049 | loss: 1.43106 - acc: 0.4616 | val_loss: 1.48923 - val_acc: 0.4474 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 5350  | total loss: 1.41120 | time: 4.196s
| RMSProp | epoch: 050 | loss: 1.41120 - acc: 0.4566 | val_loss: 1.47993 - val_acc: 0.4605 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 5457  | total loss: 1.44903 | time: 4.505s
| RMSProp | epoch: 051 | loss: 1.44903 - acc: 0.4505 | val_loss: 1.47392 - val_acc: 0.4526 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 5564  | total loss: 1.40634 | time: 4.476s
| RMSProp | epoch: 052 | loss: 1.40634 - acc: 0.4897 | val_loss: 1.50487 - val_acc: 0.4447 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 5671  | total loss: 1.46569 | time: 4.454s
| RMSProp | epoch: 053 | loss: 1.46569 - acc: 0.4227 | val_loss: 1.48868 - val_acc: 0.4500 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 5778  | total loss: 1.40620 | time: 4.382s
| RMSProp | epoch: 054 | loss: 1.40620 - acc: 0.4378 | val_loss: 1.49657 - val_acc: 0.4474 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 5885  | total loss: 1.44988 | time: 4.408s
| RMSProp | epoch: 055 | loss: 1.44988 - acc: 0.4616 | val_loss: 1.49460 - val_acc: 0.4447 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 5992  | total loss: 1.46258 | time: 4.332s
| RMSProp | epoch: 056 | loss: 1.46258 - acc: 0.4431 | val_loss: 1.49393 - val_acc: 0.4368 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 6099  | total loss: 1.39954 | time: 4.317s
| RMSProp | epoch: 057 | loss: 1.39954 - acc: 0.4486 | val_loss: 1.48982 - val_acc: 0.4526 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 6206  | total loss: 1.43514 | time: 4.300s
| RMSProp | epoch: 058 | loss: 1.43514 - acc: 0.4545 | val_loss: 1.48907 - val_acc: 0.4289 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 6313  | total loss: 1.45242 | time: 4.215s
| RMSProp | epoch: 059 | loss: 1.45242 - acc: 0.4264 | val_loss: 1.49428 - val_acc: 0.4500 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 6420  | total loss: 1.42597 | time: 4.030s
| RMSProp | epoch: 060 | loss: 1.42597 - acc: 0.4758 | val_loss: 1.48345 - val_acc: 0.4553 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 6527  | total loss: 1.41101 | time: 3.829s
| RMSProp | epoch: 061 | loss: 1.41101 - acc: 0.4599 | val_loss: 1.49319 - val_acc: 0.4605 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 6634  | total loss: 1.40050 | time: 3.942s
| RMSProp | epoch: 062 | loss: 1.40050 - acc: 0.4832 | val_loss: 1.47972 - val_acc: 0.4579 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 6741  | total loss: 1.42225 | time: 3.888s
| RMSProp | epoch: 063 | loss: 1.42225 - acc: 0.4527 | val_loss: 1.49290 - val_acc: 0.4368 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 6848  | total loss: 1.46684 | time: 3.911s
| RMSProp | epoch: 064 | loss: 1.46684 - acc: 0.4443 | val_loss: 1.47469 - val_acc: 0.4526 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 6955  | total loss: 1.50990 | time: 3.875s
| RMSProp | epoch: 065 | loss: 1.50990 - acc: 0.4166 | val_loss: 1.50400 - val_acc: 0.4474 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 7062  | total loss: 1.42146 | time: 3.935s
| RMSProp | epoch: 066 | loss: 1.42146 - acc: 0.4682 | val_loss: 1.49684 - val_acc: 0.4500 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 7169  | total loss: 1.42659 | time: 3.949s
| RMSProp | epoch: 067 | loss: 1.42659 - acc: 0.4739 | val_loss: 1.47982 - val_acc: 0.4579 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 7276  | total loss: 1.44795 | time: 3.816s
| RMSProp | epoch: 068 | loss: 1.44795 - acc: 0.4641 | val_loss: 1.48757 - val_acc: 0.4684 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 7383  | total loss: 1.41858 | time: 3.849s
| RMSProp | epoch: 069 | loss: 1.41858 - acc: 0.4713 | val_loss: 1.47570 - val_acc: 0.4553 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 7490  | total loss: 1.43647 | time: 4.109s
| RMSProp | epoch: 070 | loss: 1.43647 - acc: 0.4771 | val_loss: 1.47484 - val_acc: 0.4474 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 7597  | total loss: 1.45992 | time: 3.891s
| RMSProp | epoch: 071 | loss: 1.45992 - acc: 0.4383 | val_loss: 1.47739 - val_acc: 0.4500 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 7704  | total loss: 1.43643 | time: 3.948s
| RMSProp | epoch: 072 | loss: 1.43643 - acc: 0.4538 | val_loss: 1.49008 - val_acc: 0.4500 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 7811  | total loss: 1.37742 | time: 4.144s
| RMSProp | epoch: 073 | loss: 1.37742 - acc: 0.4575 | val_loss: 1.48145 - val_acc: 0.4474 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 7918  | total loss: 1.46685 | time: 4.258s
| RMSProp | epoch: 074 | loss: 1.46685 - acc: 0.4412 | val_loss: 1.48409 - val_acc: 0.4500 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 8025  | total loss: 1.40694 | time: 4.178s
| RMSProp | epoch: 075 | loss: 1.40694 - acc: 0.4809 | val_loss: 1.47423 - val_acc: 0.4605 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 8132  | total loss: 1.45773 | time: 3.840s
| RMSProp | epoch: 076 | loss: 1.45773 - acc: 0.4361 | val_loss: 1.48029 - val_acc: 0.4447 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 8239  | total loss: 1.46021 | time: 3.982s
| RMSProp | epoch: 077 | loss: 1.46021 - acc: 0.4492 | val_loss: 1.48429 - val_acc: 0.4605 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 8346  | total loss: 1.37276 | time: 4.117s
| RMSProp | epoch: 078 | loss: 1.37276 - acc: 0.4815 | val_loss: 1.47011 - val_acc: 0.4737 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 8453  | total loss: 1.46162 | time: 4.018s
| RMSProp | epoch: 079 | loss: 1.46162 - acc: 0.4450 | val_loss: 1.47671 - val_acc: 0.4395 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 8560  | total loss: 1.43074 | time: 3.910s
| RMSProp | epoch: 080 | loss: 1.43074 - acc: 0.4752 | val_loss: 1.47343 - val_acc: 0.4553 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 8667  | total loss: 1.41046 | time: 4.009s
| RMSProp | epoch: 081 | loss: 1.41046 - acc: 0.4779 | val_loss: 1.49388 - val_acc: 0.4316 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 8774  | total loss: 1.40342 | time: 3.818s
| RMSProp | epoch: 082 | loss: 1.40342 - acc: 0.4513 | val_loss: 1.50100 - val_acc: 0.4447 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 8881  | total loss: 1.42214 | time: 3.768s
| RMSProp | epoch: 083 | loss: 1.42214 - acc: 0.4495 | val_loss: 1.48698 - val_acc: 0.4526 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 8988  | total loss: 1.41315 | time: 3.911s
| RMSProp | epoch: 084 | loss: 1.41315 - acc: 0.4604 | val_loss: 1.48968 - val_acc: 0.4500 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 9095  | total loss: 1.43244 | time: 3.879s
| RMSProp | epoch: 085 | loss: 1.43244 - acc: 0.4493 | val_loss: 1.48089 - val_acc: 0.4474 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 9202  | total loss: 1.38314 | time: 4.024s
| RMSProp | epoch: 086 | loss: 1.38314 - acc: 0.4803 | val_loss: 1.48349 - val_acc: 0.4421 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 9309  | total loss: 1.44031 | time: 3.926s
| RMSProp | epoch: 087 | loss: 1.44031 - acc: 0.4403 | val_loss: 1.48404 - val_acc: 0.4605 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 9416  | total loss: 1.43178 | time: 3.937s
| RMSProp | epoch: 088 | loss: 1.43178 - acc: 0.4442 | val_loss: 1.48931 - val_acc: 0.4421 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 9523  | total loss: 1.42148 | time: 3.937s
| RMSProp | epoch: 089 | loss: 1.42148 - acc: 0.4441 | val_loss: 1.51100 - val_acc: 0.4474 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 9630  | total loss: 1.45008 | time: 3.960s
| RMSProp | epoch: 090 | loss: 1.45008 - acc: 0.4450 | val_loss: 1.47736 - val_acc: 0.4658 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 9737  | total loss: 1.40274 | time: 3.898s
| RMSProp | epoch: 091 | loss: 1.40274 - acc: 0.4566 | val_loss: 1.48301 - val_acc: 0.4342 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 9844  | total loss: 1.43806 | time: 3.788s
| RMSProp | epoch: 092 | loss: 1.43806 - acc: 0.4331 | val_loss: 1.51481 - val_acc: 0.4026 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 9951  | total loss: 1.43198 | time: 3.807s
| RMSProp | epoch: 093 | loss: 1.43198 - acc: 0.4553 | val_loss: 1.47613 - val_acc: 0.4421 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 10058  | total loss: 1.44139 | time: 4.016s
| RMSProp | epoch: 094 | loss: 1.44139 - acc: 0.4440 | val_loss: 1.48481 - val_acc: 0.4605 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 10165  | total loss: 1.45710 | time: 3.912s
| RMSProp | epoch: 095 | loss: 1.45710 - acc: 0.4237 | val_loss: 1.47240 - val_acc: 0.4500 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 10272  | total loss: 1.48304 | time: 3.963s
| RMSProp | epoch: 096 | loss: 1.48304 - acc: 0.4051 | val_loss: 1.53241 - val_acc: 0.4105 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 10379  | total loss: 1.41723 | time: 3.965s
| RMSProp | epoch: 097 | loss: 1.41723 - acc: 0.4676 | val_loss: 1.48614 - val_acc: 0.4526 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 10486  | total loss: 1.44207 | time: 3.981s
| RMSProp | epoch: 098 | loss: 1.44207 - acc: 0.4551 | val_loss: 1.47127 - val_acc: 0.4579 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 10593  | total loss: 1.43150 | time: 3.939s
| RMSProp | epoch: 099 | loss: 1.43150 - acc: 0.4301 | val_loss: 1.47394 - val_acc: 0.4579 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

Training Step: 10700  | total loss: 1.40604 | time: 3.937s
| RMSProp | epoch: 100 | loss: 1.40604 - acc: 0.4553 | val_loss: 1.46723 - val_acc: 0.4500 -- iter: 3421/3421 

 -------------------------------------------------------------------------------- 

