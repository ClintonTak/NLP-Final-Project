Training Step: 58  | total loss: 1.60902 | time: 126.263s
| Adam | epoch: 001 | loss: 1.60902 - acc: 0.3473 | val_loss: 1.63188 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 116  | total loss: 1.61841 | time: 19.716s
| Adam | epoch: 002 | loss: 1.61841 - acc: 0.3433 | val_loss: 1.63184 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 174  | total loss: 1.63897 | time: 19.727s
| Adam | epoch: 003 | loss: 1.63897 - acc: 0.3453 | val_loss: 1.63100 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 232  | total loss: 1.60824 | time: 19.660s
| Adam | epoch: 004 | loss: 1.60824 - acc: 0.3547 | val_loss: 1.62574 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 290  | total loss: 1.59537 | time: 19.689s
| Adam | epoch: 005 | loss: 1.59537 - acc: 0.3548 | val_loss: 1.62884 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 348  | total loss: 1.60203 | time: 19.616s
| Adam | epoch: 006 | loss: 1.60203 - acc: 0.3525 | val_loss: 1.62493 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 406  | total loss: 1.59074 | time: 19.688s
| Adam | epoch: 007 | loss: 1.59074 - acc: 0.3594 | val_loss: 1.63099 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 464  | total loss: 1.58609 | time: 19.654s
| Adam | epoch: 008 | loss: 1.58609 - acc: 0.3701 | val_loss: 1.63337 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 522  | total loss: 1.59962 | time: 19.674s
| Adam | epoch: 009 | loss: 1.59962 - acc: 0.3459 | val_loss: 1.62710 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 580  | total loss: 1.60612 | time: 19.720s
| Adam | epoch: 010 | loss: 1.60612 - acc: 0.3532 | val_loss: 1.62506 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 638  | total loss: 1.61146 | time: 19.721s
| Adam | epoch: 011 | loss: 1.61146 - acc: 0.3412 | val_loss: 1.62252 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 696  | total loss: 1.62708 | time: 19.635s
| Adam | epoch: 012 | loss: 1.62708 - acc: 0.3172 | val_loss: 1.61977 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 754  | total loss: 1.60929 | time: 19.659s
| Adam | epoch: 013 | loss: 1.60929 - acc: 0.3333 | val_loss: 1.62051 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 812  | total loss: 1.60954 | time: 19.759s
| Adam | epoch: 014 | loss: 1.60954 - acc: 0.3494 | val_loss: 1.62793 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 870  | total loss: 1.60431 | time: 19.722s
| Adam | epoch: 015 | loss: 1.60431 - acc: 0.3488 | val_loss: 1.62371 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 928  | total loss: 1.60038 | time: 19.683s
| Adam | epoch: 016 | loss: 1.60038 - acc: 0.3441 | val_loss: 1.62193 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 986  | total loss: 1.63581 | time: 19.676s
| Adam | epoch: 017 | loss: 1.63581 - acc: 0.3137 | val_loss: 1.62236 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1044  | total loss: 1.57446 | time: 19.701s
| Adam | epoch: 018 | loss: 1.57446 - acc: 0.3706 | val_loss: 1.62547 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1102  | total loss: 1.63884 | time: 19.713s
| Adam | epoch: 019 | loss: 1.63884 - acc: 0.3395 | val_loss: 1.62891 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1160  | total loss: 1.56441 | time: 19.732s
| Adam | epoch: 020 | loss: 1.56441 - acc: 0.3710 | val_loss: 1.62993 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1218  | total loss: 1.61475 | time: 19.719s
| Adam | epoch: 021 | loss: 1.61475 - acc: 0.3317 | val_loss: 1.62046 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1276  | total loss: 1.59698 | time: 19.759s
| Adam | epoch: 022 | loss: 1.59698 - acc: 0.3672 | val_loss: 1.62891 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1334  | total loss: 1.61102 | time: 19.679s
| Adam | epoch: 023 | loss: 1.61102 - acc: 0.3438 | val_loss: 1.62321 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1392  | total loss: 1.57573 | time: 19.700s
| Adam | epoch: 024 | loss: 1.57573 - acc: 0.3790 | val_loss: 1.62852 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1450  | total loss: 1.59498 | time: 19.779s
| Adam | epoch: 025 | loss: 1.59498 - acc: 0.3381 | val_loss: 1.61931 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1508  | total loss: 1.59317 | time: 19.684s
| Adam | epoch: 026 | loss: 1.59317 - acc: 0.3323 | val_loss: 1.62127 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1566  | total loss: 1.60998 | time: 19.794s
| Adam | epoch: 027 | loss: 1.60998 - acc: 0.3628 | val_loss: 1.62661 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1624  | total loss: 1.59317 | time: 19.703s
| Adam | epoch: 028 | loss: 1.59317 - acc: 0.3644 | val_loss: 1.61966 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1682  | total loss: 1.60878 | time: 19.724s
| Adam | epoch: 029 | loss: 1.60878 - acc: 0.3427 | val_loss: 1.62052 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1740  | total loss: 1.57553 | time: 19.799s
| Adam | epoch: 030 | loss: 1.57553 - acc: 0.3590 | val_loss: 1.62216 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1798  | total loss: 1.62569 | time: 19.705s
| Adam | epoch: 031 | loss: 1.62569 - acc: 0.3522 | val_loss: 1.62709 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1856  | total loss: 1.59127 | time: 19.690s
| Adam | epoch: 032 | loss: 1.59127 - acc: 0.3408 | val_loss: 1.62537 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1914  | total loss: 1.58854 | time: 19.723s
| Adam | epoch: 033 | loss: 1.58854 - acc: 0.3472 | val_loss: 1.62316 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1972  | total loss: 1.60725 | time: 19.758s
| Adam | epoch: 034 | loss: 1.60725 - acc: 0.3502 | val_loss: 1.62220 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2030  | total loss: 1.56967 | time: 19.720s
| Adam | epoch: 035 | loss: 1.56967 - acc: 0.3612 | val_loss: 1.62467 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2088  | total loss: 1.58664 | time: 19.752s
| Adam | epoch: 036 | loss: 1.58664 - acc: 0.3617 | val_loss: 1.62559 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2146  | total loss: 1.60270 | time: 19.675s
| Adam | epoch: 037 | loss: 1.60270 - acc: 0.3392 | val_loss: 1.62363 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2204  | total loss: 1.60828 | time: 19.679s
| Adam | epoch: 038 | loss: 1.60828 - acc: 0.3357 | val_loss: 1.61881 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2262  | total loss: 1.57981 | time: 19.741s
| Adam | epoch: 039 | loss: 1.57981 - acc: 0.3703 | val_loss: 1.62334 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2320  | total loss: 1.59441 | time: 19.699s
| Adam | epoch: 040 | loss: 1.59441 - acc: 0.3532 | val_loss: 1.62477 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2378  | total loss: 1.59904 | time: 19.628s
| Adam | epoch: 041 | loss: 1.59904 - acc: 0.3558 | val_loss: 1.62454 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2436  | total loss: 1.60042 | time: 19.662s
| Adam | epoch: 042 | loss: 1.60042 - acc: 0.3485 | val_loss: 1.62074 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2494  | total loss: 1.60981 | time: 19.633s
| Adam | epoch: 043 | loss: 1.60981 - acc: 0.3409 | val_loss: 1.62442 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2552  | total loss: 1.58551 | time: 19.706s
| Adam | epoch: 044 | loss: 1.58551 - acc: 0.3551 | val_loss: 1.62514 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2610  | total loss: 1.61194 | time: 19.642s
| Adam | epoch: 045 | loss: 1.61194 - acc: 0.3355 | val_loss: 1.62611 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2668  | total loss: 1.59629 | time: 19.622s
| Adam | epoch: 046 | loss: 1.59629 - acc: 0.3280 | val_loss: 1.62155 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2726  | total loss: 1.59147 | time: 19.680s
| Adam | epoch: 047 | loss: 1.59147 - acc: 0.3538 | val_loss: 1.62491 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2784  | total loss: 1.58078 | time: 19.779s
| Adam | epoch: 048 | loss: 1.58078 - acc: 0.3597 | val_loss: 1.63200 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2842  | total loss: 1.60274 | time: 19.730s
| Adam | epoch: 049 | loss: 1.60274 - acc: 0.3555 | val_loss: 1.63453 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2900  | total loss: 1.58296 | time: 19.694s
| Adam | epoch: 050 | loss: 1.58296 - acc: 0.3445 | val_loss: 1.62319 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2958  | total loss: 1.58911 | time: 19.693s
| Adam | epoch: 051 | loss: 1.58911 - acc: 0.3604 | val_loss: 1.62827 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3016  | total loss: 1.63311 | time: 19.720s
| Adam | epoch: 052 | loss: 1.63311 - acc: 0.3309 | val_loss: 1.62717 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3074  | total loss: 1.60904 | time: 19.731s
| Adam | epoch: 053 | loss: 1.60904 - acc: 0.3638 | val_loss: 1.62797 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3132  | total loss: 1.60128 | time: 19.700s
| Adam | epoch: 054 | loss: 1.60128 - acc: 0.3580 | val_loss: 1.62209 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3190  | total loss: 1.58732 | time: 19.730s
| Adam | epoch: 055 | loss: 1.58732 - acc: 0.3599 | val_loss: 1.62390 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3248  | total loss: 1.61954 | time: 19.769s
| Adam | epoch: 056 | loss: 1.61954 - acc: 0.3167 | val_loss: 1.62237 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3306  | total loss: 1.58023 | time: 19.766s
| Adam | epoch: 057 | loss: 1.58023 - acc: 0.3804 | val_loss: 1.62862 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3364  | total loss: 1.60698 | time: 19.727s
| Adam | epoch: 058 | loss: 1.60698 - acc: 0.3577 | val_loss: 1.62513 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3422  | total loss: 1.61343 | time: 19.704s
| Adam | epoch: 059 | loss: 1.61343 - acc: 0.3437 | val_loss: 1.62395 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3480  | total loss: 1.57939 | time: 19.710s
| Adam | epoch: 060 | loss: 1.57939 - acc: 0.3529 | val_loss: 1.62176 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3538  | total loss: 1.58536 | time: 19.762s
| Adam | epoch: 061 | loss: 1.58536 - acc: 0.3560 | val_loss: 1.62531 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3596  | total loss: 1.59205 | time: 19.659s
| Adam | epoch: 062 | loss: 1.59205 - acc: 0.3496 | val_loss: 1.62306 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3654  | total loss: 1.61117 | time: 19.777s
| Adam | epoch: 063 | loss: 1.61117 - acc: 0.3366 | val_loss: 1.62347 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3712  | total loss: 1.59043 | time: 19.683s
| Adam | epoch: 064 | loss: 1.59043 - acc: 0.3678 | val_loss: 1.62566 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3770  | total loss: 1.58528 | time: 19.782s
| Adam | epoch: 065 | loss: 1.58528 - acc: 0.3715 | val_loss: 1.62287 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3828  | total loss: 1.60349 | time: 19.714s
| Adam | epoch: 066 | loss: 1.60349 - acc: 0.3464 | val_loss: 1.62561 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3886  | total loss: 1.62287 | time: 19.664s
| Adam | epoch: 067 | loss: 1.62287 - acc: 0.3321 | val_loss: 1.62324 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3944  | total loss: 1.60270 | time: 19.702s
| Adam | epoch: 068 | loss: 1.60270 - acc: 0.3400 | val_loss: 1.62537 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4002  | total loss: 1.60681 | time: 19.685s
| Adam | epoch: 069 | loss: 1.60681 - acc: 0.3446 | val_loss: 1.62528 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4060  | total loss: 1.60736 | time: 19.760s
| Adam | epoch: 070 | loss: 1.60736 - acc: 0.3456 | val_loss: 1.62479 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4118  | total loss: 1.58425 | time: 19.772s
| Adam | epoch: 071 | loss: 1.58425 - acc: 0.3616 | val_loss: 1.62491 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4176  | total loss: 1.60617 | time: 19.657s
| Adam | epoch: 072 | loss: 1.60617 - acc: 0.3433 | val_loss: 1.62532 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4234  | total loss: 1.57369 | time: 19.671s
| Adam | epoch: 073 | loss: 1.57369 - acc: 0.3554 | val_loss: 1.62707 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4292  | total loss: 1.58351 | time: 19.779s
| Adam | epoch: 074 | loss: 1.58351 - acc: 0.3567 | val_loss: 1.62371 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4350  | total loss: 1.61769 | time: 19.691s
| Adam | epoch: 075 | loss: 1.61769 - acc: 0.3367 | val_loss: 1.62182 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4408  | total loss: 1.61066 | time: 19.744s
| Adam | epoch: 076 | loss: 1.61066 - acc: 0.3522 | val_loss: 1.62375 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4466  | total loss: 1.62501 | time: 19.765s
| Adam | epoch: 077 | loss: 1.62501 - acc: 0.3463 | val_loss: 1.62356 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4524  | total loss: 1.60021 | time: 19.810s
| Adam | epoch: 078 | loss: 1.60021 - acc: 0.3561 | val_loss: 1.62752 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4582  | total loss: 1.61575 | time: 19.672s
| Adam | epoch: 079 | loss: 1.61575 - acc: 0.3418 | val_loss: 1.62153 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4640  | total loss: 1.62979 | time: 19.781s
| Adam | epoch: 080 | loss: 1.62979 - acc: 0.3428 | val_loss: 1.62871 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4698  | total loss: 1.61168 | time: 19.717s
| Adam | epoch: 081 | loss: 1.61168 - acc: 0.3397 | val_loss: 1.62231 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4756  | total loss: 1.60363 | time: 19.714s
| Adam | epoch: 082 | loss: 1.60363 - acc: 0.3582 | val_loss: 1.62749 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4814  | total loss: 1.61121 | time: 19.707s
| Adam | epoch: 083 | loss: 1.61121 - acc: 0.3699 | val_loss: 1.62579 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4872  | total loss: 1.58518 | time: 19.677s
| Adam | epoch: 084 | loss: 1.58518 - acc: 0.3598 | val_loss: 1.61978 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4930  | total loss: 1.59530 | time: 19.748s
| Adam | epoch: 085 | loss: 1.59530 - acc: 0.3568 | val_loss: 1.62175 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4988  | total loss: 1.63702 | time: 19.721s
| Adam | epoch: 086 | loss: 1.63702 - acc: 0.3166 | val_loss: 1.61878 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5046  | total loss: 1.59168 | time: 19.706s
| Adam | epoch: 087 | loss: 1.59168 - acc: 0.3537 | val_loss: 1.62904 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5104  | total loss: 1.61301 | time: 19.708s
| Adam | epoch: 088 | loss: 1.61301 - acc: 0.3438 | val_loss: 1.62341 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5162  | total loss: 1.58589 | time: 19.742s
| Adam | epoch: 089 | loss: 1.58589 - acc: 0.3700 | val_loss: 1.62201 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5220  | total loss: 1.62392 | time: 19.737s
| Adam | epoch: 090 | loss: 1.62392 - acc: 0.3335 | val_loss: 1.62104 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5278  | total loss: 1.60178 | time: 19.714s
| Adam | epoch: 091 | loss: 1.60178 - acc: 0.3420 | val_loss: 1.62348 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5336  | total loss: 1.60468 | time: 19.686s
| Adam | epoch: 092 | loss: 1.60468 - acc: 0.3452 | val_loss: 1.62583 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5394  | total loss: 1.61883 | time: 19.741s
| Adam | epoch: 093 | loss: 1.61883 - acc: 0.3298 | val_loss: 1.62335 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5452  | total loss: 1.60949 | time: 19.742s
| Adam | epoch: 094 | loss: 1.60949 - acc: 0.3413 | val_loss: 1.62318 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5510  | total loss: 1.58677 | time: 19.986s
| Adam | epoch: 095 | loss: 1.58677 - acc: 0.3576 | val_loss: 1.63102 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5568  | total loss: 1.60115 | time: 20.965s
| Adam | epoch: 096 | loss: 1.60115 - acc: 0.3486 | val_loss: 1.62779 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5626  | total loss: 1.59411 | time: 21.534s
| Adam | epoch: 097 | loss: 1.59411 - acc: 0.3544 | val_loss: 1.62350 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5684  | total loss: 1.59794 | time: 20.585s
| Adam | epoch: 098 | loss: 1.59794 - acc: 0.3651 | val_loss: 1.62089 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5742  | total loss: 1.58792 | time: 20.797s
| Adam | epoch: 099 | loss: 1.58792 - acc: 0.3536 | val_loss: 1.62120 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5800  | total loss: 1.60177 | time: 21.356s
| Adam | epoch: 100 | loss: 1.60177 - acc: 0.3548 | val_loss: 1.63010 - val_acc: 0.2992 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

