Training Step: 58  | total loss: 1.77237 | time: 5.002s
| RMSProp | epoch: 001 | loss: 1.77237 - acc: 0.3223 | val_loss: 1.77035 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 116  | total loss: 1.67883 | time: 4.108s
| RMSProp | epoch: 002 | loss: 1.67883 - acc: 0.3512 | val_loss: 1.66597 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 174  | total loss: 1.60626 | time: 4.061s
| RMSProp | epoch: 003 | loss: 1.60626 - acc: 0.3398 | val_loss: 1.64535 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 232  | total loss: 1.60062 | time: 4.043s
| RMSProp | epoch: 004 | loss: 1.60062 - acc: 0.3463 | val_loss: 1.64204 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 290  | total loss: 1.56083 | time: 4.033s
| RMSProp | epoch: 005 | loss: 1.56083 - acc: 0.3678 | val_loss: 1.64469 - val_acc: 0.3307 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 348  | total loss: 1.56175 | time: 4.033s
| RMSProp | epoch: 006 | loss: 1.56175 - acc: 0.3732 | val_loss: 1.63517 - val_acc: 0.3333 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 406  | total loss: 1.54921 | time: 4.023s
| RMSProp | epoch: 007 | loss: 1.54921 - acc: 0.3869 | val_loss: 1.59729 - val_acc: 0.3648 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 464  | total loss: 1.50548 | time: 4.055s
| RMSProp | epoch: 008 | loss: 1.50548 - acc: 0.4091 | val_loss: 1.57334 - val_acc: 0.3806 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 522  | total loss: 1.47784 | time: 4.057s
| RMSProp | epoch: 009 | loss: 1.47784 - acc: 0.4378 | val_loss: 1.53094 - val_acc: 0.4068 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 580  | total loss: 1.42370 | time: 4.047s
| RMSProp | epoch: 010 | loss: 1.42370 - acc: 0.4759 | val_loss: 1.50389 - val_acc: 0.4436 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 638  | total loss: 1.41935 | time: 4.055s
| RMSProp | epoch: 011 | loss: 1.41935 - acc: 0.4682 | val_loss: 1.46851 - val_acc: 0.4514 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 696  | total loss: 1.35958 | time: 4.059s
| RMSProp | epoch: 012 | loss: 1.35958 - acc: 0.4967 | val_loss: 1.43801 - val_acc: 0.4514 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 754  | total loss: 1.35532 | time: 4.064s
| RMSProp | epoch: 013 | loss: 1.35532 - acc: 0.4870 | val_loss: 1.40441 - val_acc: 0.4698 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 812  | total loss: 1.30996 | time: 4.059s
| RMSProp | epoch: 014 | loss: 1.30996 - acc: 0.5087 | val_loss: 1.38389 - val_acc: 0.4961 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 870  | total loss: 1.28151 | time: 4.047s
| RMSProp | epoch: 015 | loss: 1.28151 - acc: 0.5133 | val_loss: 1.35930 - val_acc: 0.4934 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 928  | total loss: 1.31066 | time: 4.060s
| RMSProp | epoch: 016 | loss: 1.31066 - acc: 0.5203 | val_loss: 1.34203 - val_acc: 0.5092 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 986  | total loss: 1.28010 | time: 4.044s
| RMSProp | epoch: 017 | loss: 1.28010 - acc: 0.5284 | val_loss: 1.33054 - val_acc: 0.5197 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1044  | total loss: 1.27475 | time: 4.063s
| RMSProp | epoch: 018 | loss: 1.27475 - acc: 0.5153 | val_loss: 1.31151 - val_acc: 0.5276 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1102  | total loss: 1.18181 | time: 4.056s
| RMSProp | epoch: 019 | loss: 1.18181 - acc: 0.5699 | val_loss: 1.31443 - val_acc: 0.5197 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1160  | total loss: 1.20258 | time: 4.063s
| RMSProp | epoch: 020 | loss: 1.20258 - acc: 0.5671 | val_loss: 1.29156 - val_acc: 0.5381 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1218  | total loss: 1.12529 | time: 4.059s
| RMSProp | epoch: 021 | loss: 1.12529 - acc: 0.5875 | val_loss: 1.27246 - val_acc: 0.5433 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1276  | total loss: 1.13809 | time: 4.050s
| RMSProp | epoch: 022 | loss: 1.13809 - acc: 0.5877 | val_loss: 1.27164 - val_acc: 0.5354 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1334  | total loss: 1.14600 | time: 4.061s
| RMSProp | epoch: 023 | loss: 1.14600 - acc: 0.5864 | val_loss: 1.26763 - val_acc: 0.5407 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1392  | total loss: 1.14087 | time: 4.018s
| RMSProp | epoch: 024 | loss: 1.14087 - acc: 0.5719 | val_loss: 1.25056 - val_acc: 0.5643 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1450  | total loss: 1.08109 | time: 4.058s
| RMSProp | epoch: 025 | loss: 1.08109 - acc: 0.6191 | val_loss: 1.23699 - val_acc: 0.5774 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1508  | total loss: 1.07751 | time: 4.073s
| RMSProp | epoch: 026 | loss: 1.07751 - acc: 0.6130 | val_loss: 1.22999 - val_acc: 0.5827 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1566  | total loss: 1.07478 | time: 4.047s
| RMSProp | epoch: 027 | loss: 1.07478 - acc: 0.6057 | val_loss: 1.21778 - val_acc: 0.5643 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1624  | total loss: 1.05902 | time: 4.061s
| RMSProp | epoch: 028 | loss: 1.05902 - acc: 0.6200 | val_loss: 1.20975 - val_acc: 0.5617 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1682  | total loss: 1.06853 | time: 4.052s
| RMSProp | epoch: 029 | loss: 1.06853 - acc: 0.6307 | val_loss: 1.20527 - val_acc: 0.5853 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1740  | total loss: 1.02066 | time: 4.070s
| RMSProp | epoch: 030 | loss: 1.02066 - acc: 0.6451 | val_loss: 1.21119 - val_acc: 0.5801 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1798  | total loss: 1.06306 | time: 4.068s
| RMSProp | epoch: 031 | loss: 1.06306 - acc: 0.6205 | val_loss: 1.21631 - val_acc: 0.5696 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1856  | total loss: 1.04395 | time: 4.066s
| RMSProp | epoch: 032 | loss: 1.04395 - acc: 0.6356 | val_loss: 1.21077 - val_acc: 0.5643 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1914  | total loss: 0.97856 | time: 4.090s
| RMSProp | epoch: 033 | loss: 0.97856 - acc: 0.6566 | val_loss: 1.20927 - val_acc: 0.5564 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1972  | total loss: 0.94560 | time: 4.052s
| RMSProp | epoch: 034 | loss: 0.94560 - acc: 0.6749 | val_loss: 1.18365 - val_acc: 0.5879 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2030  | total loss: 0.95758 | time: 4.066s
| RMSProp | epoch: 035 | loss: 0.95758 - acc: 0.6676 | val_loss: 1.19393 - val_acc: 0.5774 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2088  | total loss: 0.97469 | time: 4.046s
| RMSProp | epoch: 036 | loss: 0.97469 - acc: 0.6495 | val_loss: 1.17547 - val_acc: 0.5827 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2146  | total loss: 0.93969 | time: 4.064s
| RMSProp | epoch: 037 | loss: 0.93969 - acc: 0.6677 | val_loss: 1.17808 - val_acc: 0.5801 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2204  | total loss: 0.92252 | time: 4.048s
| RMSProp | epoch: 038 | loss: 0.92252 - acc: 0.6679 | val_loss: 1.17922 - val_acc: 0.5958 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2262  | total loss: 0.90919 | time: 4.010s
| RMSProp | epoch: 039 | loss: 0.90919 - acc: 0.6803 | val_loss: 1.18601 - val_acc: 0.5801 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2320  | total loss: 0.95053 | time: 4.009s
| RMSProp | epoch: 040 | loss: 0.95053 - acc: 0.6577 | val_loss: 1.17748 - val_acc: 0.5722 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2378  | total loss: 0.87804 | time: 3.999s
| RMSProp | epoch: 041 | loss: 0.87804 - acc: 0.6844 | val_loss: 1.16371 - val_acc: 0.5801 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2436  | total loss: 0.88386 | time: 4.062s
| RMSProp | epoch: 042 | loss: 0.88386 - acc: 0.6808 | val_loss: 1.17118 - val_acc: 0.5722 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2494  | total loss: 0.87653 | time: 4.062s
| RMSProp | epoch: 043 | loss: 0.87653 - acc: 0.6944 | val_loss: 1.17076 - val_acc: 0.5696 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2552  | total loss: 0.86514 | time: 4.059s
| RMSProp | epoch: 044 | loss: 0.86514 - acc: 0.6984 | val_loss: 1.16764 - val_acc: 0.5932 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2610  | total loss: 0.85782 | time: 4.060s
| RMSProp | epoch: 045 | loss: 0.85782 - acc: 0.6954 | val_loss: 1.17307 - val_acc: 0.5774 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2668  | total loss: 0.85070 | time: 4.059s
| RMSProp | epoch: 046 | loss: 0.85070 - acc: 0.6985 | val_loss: 1.18779 - val_acc: 0.5722 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2726  | total loss: 0.81888 | time: 4.054s
| RMSProp | epoch: 047 | loss: 0.81888 - acc: 0.7212 | val_loss: 1.17050 - val_acc: 0.5906 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2784  | total loss: 0.85999 | time: 4.034s
| RMSProp | epoch: 048 | loss: 0.85999 - acc: 0.7086 | val_loss: 1.16412 - val_acc: 0.5696 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2842  | total loss: 0.80682 | time: 4.172s
| RMSProp | epoch: 049 | loss: 0.80682 - acc: 0.7247 | val_loss: 1.16751 - val_acc: 0.5669 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2900  | total loss: 0.81703 | time: 4.062s
| RMSProp | epoch: 050 | loss: 0.81703 - acc: 0.7126 | val_loss: 1.16941 - val_acc: 0.5827 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2958  | total loss: 0.80888 | time: 4.046s
| RMSProp | epoch: 051 | loss: 0.80888 - acc: 0.7088 | val_loss: 1.16912 - val_acc: 0.5853 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3016  | total loss: 0.78960 | time: 4.071s
| RMSProp | epoch: 052 | loss: 0.78960 - acc: 0.7320 | val_loss: 1.17602 - val_acc: 0.5827 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3074  | total loss: 0.77884 | time: 4.064s
| RMSProp | epoch: 053 | loss: 0.77884 - acc: 0.7290 | val_loss: 1.17673 - val_acc: 0.5827 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3132  | total loss: 0.76361 | time: 4.071s
| RMSProp | epoch: 054 | loss: 0.76361 - acc: 0.7368 | val_loss: 1.19219 - val_acc: 0.5827 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3190  | total loss: 0.79478 | time: 4.067s
| RMSProp | epoch: 055 | loss: 0.79478 - acc: 0.7120 | val_loss: 1.16273 - val_acc: 0.5827 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3248  | total loss: 0.78124 | time: 4.068s
| RMSProp | epoch: 056 | loss: 0.78124 - acc: 0.7217 | val_loss: 1.18094 - val_acc: 0.5932 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3306  | total loss: 0.72500 | time: 4.068s
| RMSProp | epoch: 057 | loss: 0.72500 - acc: 0.7506 | val_loss: 1.18263 - val_acc: 0.5827 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3364  | total loss: 0.68441 | time: 4.051s
| RMSProp | epoch: 058 | loss: 0.68441 - acc: 0.7754 | val_loss: 1.18765 - val_acc: 0.5853 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3422  | total loss: 0.71679 | time: 4.056s
| RMSProp | epoch: 059 | loss: 0.71679 - acc: 0.7516 | val_loss: 1.19426 - val_acc: 0.5853 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3480  | total loss: 0.69191 | time: 4.086s
| RMSProp | epoch: 060 | loss: 0.69191 - acc: 0.7660 | val_loss: 1.20567 - val_acc: 0.5906 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3538  | total loss: 0.74886 | time: 4.098s
| RMSProp | epoch: 061 | loss: 0.74886 - acc: 0.7417 | val_loss: 1.18847 - val_acc: 0.5932 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3596  | total loss: 0.74068 | time: 4.056s
| RMSProp | epoch: 062 | loss: 0.74068 - acc: 0.7332 | val_loss: 1.19855 - val_acc: 0.5984 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3654  | total loss: 0.69624 | time: 4.060s
| RMSProp | epoch: 063 | loss: 0.69624 - acc: 0.7549 | val_loss: 1.22577 - val_acc: 0.5984 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3712  | total loss: 0.71629 | time: 4.060s
| RMSProp | epoch: 064 | loss: 0.71629 - acc: 0.7519 | val_loss: 1.23439 - val_acc: 0.5774 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3770  | total loss: 0.69337 | time: 4.061s
| RMSProp | epoch: 065 | loss: 0.69337 - acc: 0.7477 | val_loss: 1.22115 - val_acc: 0.5827 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3828  | total loss: 0.68144 | time: 4.063s
| RMSProp | epoch: 066 | loss: 0.68144 - acc: 0.7759 | val_loss: 1.21369 - val_acc: 0.5827 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3886  | total loss: 0.68634 | time: 4.063s
| RMSProp | epoch: 067 | loss: 0.68634 - acc: 0.7551 | val_loss: 1.20544 - val_acc: 0.5774 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3944  | total loss: 0.63911 | time: 4.042s
| RMSProp | epoch: 068 | loss: 0.63911 - acc: 0.7889 | val_loss: 1.23716 - val_acc: 0.5853 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4002  | total loss: 0.66146 | time: 4.057s
| RMSProp | epoch: 069 | loss: 0.66146 - acc: 0.7728 | val_loss: 1.23002 - val_acc: 0.5958 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4060  | total loss: 0.64344 | time: 4.062s
| RMSProp | epoch: 070 | loss: 0.64344 - acc: 0.7827 | val_loss: 1.27114 - val_acc: 0.5748 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4118  | total loss: 0.60132 | time: 4.055s
| RMSProp | epoch: 071 | loss: 0.60132 - acc: 0.7930 | val_loss: 1.26233 - val_acc: 0.5801 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4176  | total loss: 0.66638 | time: 4.030s
| RMSProp | epoch: 072 | loss: 0.66638 - acc: 0.7542 | val_loss: 1.24680 - val_acc: 0.5906 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4234  | total loss: 0.65225 | time: 4.050s
| RMSProp | epoch: 073 | loss: 0.65225 - acc: 0.7726 | val_loss: 1.25010 - val_acc: 0.5722 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4292  | total loss: 0.58937 | time: 4.077s
| RMSProp | epoch: 074 | loss: 0.58937 - acc: 0.8046 | val_loss: 1.26831 - val_acc: 0.5827 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4350  | total loss: 0.59212 | time: 4.062s
| RMSProp | epoch: 075 | loss: 0.59212 - acc: 0.7826 | val_loss: 1.25605 - val_acc: 0.5827 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4408  | total loss: 0.64599 | time: 4.071s
| RMSProp | epoch: 076 | loss: 0.64599 - acc: 0.7734 | val_loss: 1.34639 - val_acc: 0.5696 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4466  | total loss: 0.61308 | time: 4.070s
| RMSProp | epoch: 077 | loss: 0.61308 - acc: 0.7853 | val_loss: 1.29435 - val_acc: 0.5643 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4524  | total loss: 0.60459 | time: 4.061s
| RMSProp | epoch: 078 | loss: 0.60459 - acc: 0.7850 | val_loss: 1.25235 - val_acc: 0.5748 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4582  | total loss: 0.59505 | time: 4.060s
| RMSProp | epoch: 079 | loss: 0.59505 - acc: 0.7933 | val_loss: 1.26731 - val_acc: 0.5643 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4640  | total loss: 0.54749 | time: 4.080s
| RMSProp | epoch: 080 | loss: 0.54749 - acc: 0.8126 | val_loss: 1.27360 - val_acc: 0.5669 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4698  | total loss: 0.60346 | time: 4.098s
| RMSProp | epoch: 081 | loss: 0.60346 - acc: 0.8026 | val_loss: 1.29283 - val_acc: 0.5879 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4756  | total loss: 0.60183 | time: 4.061s
| RMSProp | epoch: 082 | loss: 0.60183 - acc: 0.7564 | val_loss: 1.21528 - val_acc: 0.5879 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4814  | total loss: 0.57982 | time: 4.102s
| RMSProp | epoch: 083 | loss: 0.57982 - acc: 0.7992 | val_loss: 1.26387 - val_acc: 0.5774 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4872  | total loss: 0.60537 | time: 4.069s
| RMSProp | epoch: 084 | loss: 0.60537 - acc: 0.7846 | val_loss: 1.30004 - val_acc: 0.5774 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4930  | total loss: 0.59104 | time: 4.054s
| RMSProp | epoch: 085 | loss: 0.59104 - acc: 0.7977 | val_loss: 1.25805 - val_acc: 0.5906 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4988  | total loss: 0.54991 | time: 4.073s
| RMSProp | epoch: 086 | loss: 0.54991 - acc: 0.8204 | val_loss: 1.26372 - val_acc: 0.5906 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5046  | total loss: 0.54547 | time: 4.063s
| RMSProp | epoch: 087 | loss: 0.54547 - acc: 0.8204 | val_loss: 1.31068 - val_acc: 0.5932 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5104  | total loss: 0.58433 | time: 4.067s
| RMSProp | epoch: 088 | loss: 0.58433 - acc: 0.7845 | val_loss: 1.30689 - val_acc: 0.5748 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5162  | total loss: 0.56562 | time: 4.060s
| RMSProp | epoch: 089 | loss: 0.56562 - acc: 0.8032 | val_loss: 1.29774 - val_acc: 0.5853 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5220  | total loss: 0.60914 | time: 4.055s
| RMSProp | epoch: 090 | loss: 0.60914 - acc: 0.7884 | val_loss: 1.28464 - val_acc: 0.5932 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5278  | total loss: 0.85994 | time: 4.071s
| RMSProp | epoch: 091 | loss: 0.85994 - acc: 0.7499 | val_loss: 1.27160 - val_acc: 0.5879 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5336  | total loss: 0.55154 | time: 4.053s
| RMSProp | epoch: 092 | loss: 0.55154 - acc: 0.8051 | val_loss: 1.33711 - val_acc: 0.5932 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5394  | total loss: 0.51771 | time: 4.063s
| RMSProp | epoch: 093 | loss: 0.51771 - acc: 0.8268 | val_loss: 1.28152 - val_acc: 0.5774 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5452  | total loss: 0.54389 | time: 4.066s
| RMSProp | epoch: 094 | loss: 0.54389 - acc: 0.8073 | val_loss: 1.32156 - val_acc: 0.5722 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5510  | total loss: 0.52711 | time: 4.081s
| RMSProp | epoch: 095 | loss: 0.52711 - acc: 0.8133 | val_loss: 1.30576 - val_acc: 0.5774 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5568  | total loss: 0.51613 | time: 4.063s
| RMSProp | epoch: 096 | loss: 0.51613 - acc: 0.8060 | val_loss: 1.31407 - val_acc: 0.5853 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5626  | total loss: 0.56241 | time: 4.059s
| RMSProp | epoch: 097 | loss: 0.56241 - acc: 0.8036 | val_loss: 1.28602 - val_acc: 0.5722 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5684  | total loss: 0.52153 | time: 4.075s
| RMSProp | epoch: 098 | loss: 0.52153 - acc: 0.8120 | val_loss: 1.35083 - val_acc: 0.5774 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5742  | total loss: 0.53642 | time: 4.059s
| RMSProp | epoch: 099 | loss: 0.53642 - acc: 0.8245 | val_loss: 1.38096 - val_acc: 0.5512 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5800  | total loss: 0.52028 | time: 4.064s
| RMSProp | epoch: 100 | loss: 0.52028 - acc: 0.8262 | val_loss: 1.37956 - val_acc: 0.5643 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

