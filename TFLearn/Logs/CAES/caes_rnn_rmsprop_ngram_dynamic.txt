Training Step: 56  | total loss: 1.77906 | time: 127.628s
| RMSProp | epoch: 001 | loss: 1.77906 - acc: 0.3758 | val_loss: 1.77730 - val_acc: 0.3181 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 112  | total loss: 1.67207 | time: 19.241s
| RMSProp | epoch: 002 | loss: 1.67207 - acc: 0.3538 | val_loss: 1.63127 - val_acc: 0.3181 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 168  | total loss: 1.60395 | time: 19.182s
| RMSProp | epoch: 003 | loss: 1.60395 - acc: 0.3521 | val_loss: 1.62417 - val_acc: 0.3181 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 224  | total loss: 1.58459 | time: 19.143s
| RMSProp | epoch: 004 | loss: 1.58459 - acc: 0.3635 | val_loss: 1.62190 - val_acc: 0.3181 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 280  | total loss: 1.59992 | time: 19.214s
| RMSProp | epoch: 005 | loss: 1.59992 - acc: 0.3739 | val_loss: 1.60292 - val_acc: 0.3881 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 336  | total loss: 1.60231 | time: 19.149s
| RMSProp | epoch: 006 | loss: 1.60231 - acc: 0.3541 | val_loss: 1.60611 - val_acc: 0.3881 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 392  | total loss: 1.57601 | time: 19.143s
| RMSProp | epoch: 007 | loss: 1.57601 - acc: 0.3937 | val_loss: 1.60570 - val_acc: 0.3881 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 448  | total loss: 1.59532 | time: 19.223s
| RMSProp | epoch: 008 | loss: 1.59532 - acc: 0.3795 | val_loss: 1.60530 - val_acc: 0.3585 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 504  | total loss: 1.58652 | time: 19.232s
| RMSProp | epoch: 009 | loss: 1.58652 - acc: 0.3841 | val_loss: 1.60307 - val_acc: 0.3854 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 560  | total loss: 1.58476 | time: 19.092s
| RMSProp | epoch: 010 | loss: 1.58476 - acc: 0.3935 | val_loss: 1.60330 - val_acc: 0.3585 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 616  | total loss: 1.57468 | time: 19.142s
| RMSProp | epoch: 011 | loss: 1.57468 - acc: 0.3894 | val_loss: 1.60015 - val_acc: 0.3666 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 672  | total loss: 1.59405 | time: 19.170s
| RMSProp | epoch: 012 | loss: 1.59405 - acc: 0.3760 | val_loss: 1.60506 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 728  | total loss: 1.52464 | time: 19.148s
| RMSProp | epoch: 013 | loss: 1.52464 - acc: 0.4068 | val_loss: 1.59859 - val_acc: 0.3935 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 784  | total loss: 1.59294 | time: 19.274s
| RMSProp | epoch: 014 | loss: 1.59294 - acc: 0.3812 | val_loss: 1.60089 - val_acc: 0.3639 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 840  | total loss: 1.57861 | time: 21.074s
| RMSProp | epoch: 015 | loss: 1.57861 - acc: 0.4000 | val_loss: 1.60602 - val_acc: 0.3666 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 896  | total loss: 1.58137 | time: 20.914s
| RMSProp | epoch: 016 | loss: 1.58137 - acc: 0.3880 | val_loss: 1.83662 - val_acc: 0.2911 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 952  | total loss: 1.57980 | time: 20.774s
| RMSProp | epoch: 017 | loss: 1.57980 - acc: 0.3894 | val_loss: 1.59744 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1008  | total loss: 1.56440 | time: 21.691s
| RMSProp | epoch: 018 | loss: 1.56440 - acc: 0.3912 | val_loss: 1.59762 - val_acc: 0.3881 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1064  | total loss: 1.55925 | time: 21.189s
| RMSProp | epoch: 019 | loss: 1.55925 - acc: 0.3984 | val_loss: 1.59724 - val_acc: 0.3666 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1120  | total loss: 1.54214 | time: 21.324s
| RMSProp | epoch: 020 | loss: 1.54214 - acc: 0.4208 | val_loss: 1.60515 - val_acc: 0.3827 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1176  | total loss: 1.54510 | time: 20.398s
| RMSProp | epoch: 021 | loss: 1.54510 - acc: 0.4029 | val_loss: 1.62284 - val_acc: 0.3558 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1232  | total loss: 1.55894 | time: 20.466s
| RMSProp | epoch: 022 | loss: 1.55894 - acc: 0.4009 | val_loss: 1.60573 - val_acc: 0.3801 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1288  | total loss: 1.53961 | time: 20.578s
| RMSProp | epoch: 023 | loss: 1.53961 - acc: 0.4046 | val_loss: 1.61036 - val_acc: 0.3477 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1344  | total loss: 1.53988 | time: 20.866s
| RMSProp | epoch: 024 | loss: 1.53988 - acc: 0.4062 | val_loss: 1.61376 - val_acc: 0.3639 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1400  | total loss: 1.55161 | time: 21.454s
| RMSProp | epoch: 025 | loss: 1.55161 - acc: 0.3990 | val_loss: 1.60477 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1456  | total loss: 1.66789 | time: 20.932s
| RMSProp | epoch: 026 | loss: 1.66789 - acc: 0.3873 | val_loss: 1.60826 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1512  | total loss: 1.57736 | time: 21.229s
| RMSProp | epoch: 027 | loss: 1.57736 - acc: 0.3897 | val_loss: 1.62351 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1568  | total loss: 1.54692 | time: 21.028s
| RMSProp | epoch: 028 | loss: 1.54692 - acc: 0.4145 | val_loss: 1.62234 - val_acc: 0.3612 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1624  | total loss: 1.54279 | time: 20.314s
| RMSProp | epoch: 029 | loss: 1.54279 - acc: 0.4144 | val_loss: 1.62505 - val_acc: 0.3612 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1680  | total loss: 1.53006 | time: 20.842s
| RMSProp | epoch: 030 | loss: 1.53006 - acc: 0.4236 | val_loss: 1.63876 - val_acc: 0.3666 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1736  | total loss: 1.58461 | time: 20.836s
| RMSProp | epoch: 031 | loss: 1.58461 - acc: 0.3939 | val_loss: 1.60805 - val_acc: 0.3693 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1792  | total loss: 1.55120 | time: 20.769s
| RMSProp | epoch: 032 | loss: 1.55120 - acc: 0.4077 | val_loss: 1.62050 - val_acc: 0.3693 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1848  | total loss: 1.54780 | time: 20.507s
| RMSProp | epoch: 033 | loss: 1.54780 - acc: 0.4012 | val_loss: 1.61971 - val_acc: 0.3666 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1904  | total loss: 1.55134 | time: 21.395s
| RMSProp | epoch: 034 | loss: 1.55134 - acc: 0.3993 | val_loss: 1.62776 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1960  | total loss: 1.52410 | time: 20.913s
| RMSProp | epoch: 035 | loss: 1.52410 - acc: 0.4078 | val_loss: 1.62903 - val_acc: 0.3585 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2016  | total loss: 1.56060 | time: 21.649s
| RMSProp | epoch: 036 | loss: 1.56060 - acc: 0.3819 | val_loss: 1.61970 - val_acc: 0.3558 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2072  | total loss: 1.57120 | time: 20.181s
| RMSProp | epoch: 037 | loss: 1.57120 - acc: 0.3926 | val_loss: 1.63872 - val_acc: 0.3450 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2128  | total loss: 1.50480 | time: 20.825s
| RMSProp | epoch: 038 | loss: 1.50480 - acc: 0.4302 | val_loss: 1.63813 - val_acc: 0.3477 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2184  | total loss: 1.50633 | time: 21.219s
| RMSProp | epoch: 039 | loss: 1.50633 - acc: 0.4347 | val_loss: 1.61219 - val_acc: 0.3774 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2240  | total loss: 1.46162 | time: 20.551s
| RMSProp | epoch: 040 | loss: 1.46162 - acc: 0.4529 | val_loss: 1.60975 - val_acc: 0.3558 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2296  | total loss: 1.48366 | time: 20.446s
| RMSProp | epoch: 041 | loss: 1.48366 - acc: 0.4438 | val_loss: 1.61764 - val_acc: 0.3477 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2352  | total loss: 1.51109 | time: 20.656s
| RMSProp | epoch: 042 | loss: 1.51109 - acc: 0.4262 | val_loss: 1.62514 - val_acc: 0.3558 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2408  | total loss: 1.50232 | time: 21.296s
| RMSProp | epoch: 043 | loss: 1.50232 - acc: 0.4174 | val_loss: 1.61036 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2464  | total loss: 1.46282 | time: 21.026s
| RMSProp | epoch: 044 | loss: 1.46282 - acc: 0.4351 | val_loss: 1.62687 - val_acc: 0.3666 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2520  | total loss: 1.46306 | time: 22.031s
| RMSProp | epoch: 045 | loss: 1.46306 - acc: 0.4413 | val_loss: 1.62310 - val_acc: 0.3612 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2576  | total loss: 1.48417 | time: 21.245s
| RMSProp | epoch: 046 | loss: 1.48417 - acc: 0.4231 | val_loss: 1.60478 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2632  | total loss: 1.48364 | time: 21.400s
| RMSProp | epoch: 047 | loss: 1.48364 - acc: 0.4331 | val_loss: 1.62683 - val_acc: 0.3558 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2688  | total loss: 1.46696 | time: 20.725s
| RMSProp | epoch: 048 | loss: 1.46696 - acc: 0.4592 | val_loss: 1.62058 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2744  | total loss: 1.47117 | time: 21.372s
| RMSProp | epoch: 049 | loss: 1.47117 - acc: 0.4314 | val_loss: 1.66731 - val_acc: 0.3504 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2800  | total loss: 1.46301 | time: 21.471s
| RMSProp | epoch: 050 | loss: 1.46301 - acc: 0.4589 | val_loss: 1.62004 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2856  | total loss: 1.51348 | time: 21.453s
| RMSProp | epoch: 051 | loss: 1.51348 - acc: 0.4218 | val_loss: 1.62754 - val_acc: 0.3827 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2912  | total loss: 1.44737 | time: 20.643s
| RMSProp | epoch: 052 | loss: 1.44737 - acc: 0.4429 | val_loss: 1.61849 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2968  | total loss: 1.39941 | time: 20.824s
| RMSProp | epoch: 053 | loss: 1.39941 - acc: 0.4716 | val_loss: 1.61974 - val_acc: 0.3558 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3024  | total loss: 1.40968 | time: 21.132s
| RMSProp | epoch: 054 | loss: 1.40968 - acc: 0.4773 | val_loss: 1.62023 - val_acc: 0.3639 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3080  | total loss: 1.42569 | time: 21.151s
| RMSProp | epoch: 055 | loss: 1.42569 - acc: 0.4455 | val_loss: 1.62100 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3136  | total loss: 1.41805 | time: 21.282s
| RMSProp | epoch: 056 | loss: 1.41805 - acc: 0.4728 | val_loss: 1.63358 - val_acc: 0.3801 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3192  | total loss: 1.43127 | time: 21.046s
| RMSProp | epoch: 057 | loss: 1.43127 - acc: 0.4609 | val_loss: 1.64097 - val_acc: 0.3666 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3248  | total loss: 1.39470 | time: 20.753s
| RMSProp | epoch: 058 | loss: 1.39470 - acc: 0.4835 | val_loss: 1.62329 - val_acc: 0.3693 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3304  | total loss: 1.37956 | time: 20.962s
| RMSProp | epoch: 059 | loss: 1.37956 - acc: 0.4966 | val_loss: 1.65596 - val_acc: 0.3612 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3360  | total loss: 1.41074 | time: 21.162s
| RMSProp | epoch: 060 | loss: 1.41074 - acc: 0.4674 | val_loss: 1.63327 - val_acc: 0.3935 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3416  | total loss: 1.42239 | time: 20.749s
| RMSProp | epoch: 061 | loss: 1.42239 - acc: 0.4733 | val_loss: 1.64688 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3472  | total loss: 1.42296 | time: 21.325s
| RMSProp | epoch: 062 | loss: 1.42296 - acc: 0.4611 | val_loss: 1.66043 - val_acc: 0.3639 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3528  | total loss: 1.39231 | time: 20.953s
| RMSProp | epoch: 063 | loss: 1.39231 - acc: 0.4910 | val_loss: 1.66337 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3584  | total loss: 1.35006 | time: 21.188s
| RMSProp | epoch: 064 | loss: 1.35006 - acc: 0.4944 | val_loss: 1.64186 - val_acc: 0.3989 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3640  | total loss: 1.34522 | time: 20.504s
| RMSProp | epoch: 065 | loss: 1.34522 - acc: 0.5019 | val_loss: 1.63617 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3696  | total loss: 1.30848 | time: 20.618s
| RMSProp | epoch: 066 | loss: 1.30848 - acc: 0.5227 | val_loss: 1.61604 - val_acc: 0.4232 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3752  | total loss: 1.37339 | time: 20.715s
| RMSProp | epoch: 067 | loss: 1.37339 - acc: 0.5005 | val_loss: 1.64426 - val_acc: 0.4232 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3808  | total loss: 1.38272 | time: 21.001s
| RMSProp | epoch: 068 | loss: 1.38272 - acc: 0.4937 | val_loss: 1.63688 - val_acc: 0.3962 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3864  | total loss: 1.35514 | time: 20.378s
| RMSProp | epoch: 069 | loss: 1.35514 - acc: 0.4917 | val_loss: 1.64408 - val_acc: 0.4070 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3920  | total loss: 1.40171 | time: 20.909s
| RMSProp | epoch: 070 | loss: 1.40171 - acc: 0.4777 | val_loss: 1.65638 - val_acc: 0.4016 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3976  | total loss: 1.31949 | time: 21.403s
| RMSProp | epoch: 071 | loss: 1.31949 - acc: 0.5256 | val_loss: 1.65284 - val_acc: 0.4178 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4032  | total loss: 1.31970 | time: 20.395s
| RMSProp | epoch: 072 | loss: 1.31970 - acc: 0.5266 | val_loss: 1.69453 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4088  | total loss: 1.27862 | time: 20.574s
| RMSProp | epoch: 073 | loss: 1.27862 - acc: 0.5312 | val_loss: 1.66007 - val_acc: 0.3935 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4144  | total loss: 1.32655 | time: 20.764s
| RMSProp | epoch: 074 | loss: 1.32655 - acc: 0.5380 | val_loss: 1.67137 - val_acc: 0.4043 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4200  | total loss: 1.34459 | time: 20.964s
| RMSProp | epoch: 075 | loss: 1.34459 - acc: 0.5309 | val_loss: 1.68991 - val_acc: 0.3881 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4256  | total loss: 1.34804 | time: 20.566s
| RMSProp | epoch: 076 | loss: 1.34804 - acc: 0.5308 | val_loss: 1.69527 - val_acc: 0.4070 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4312  | total loss: 1.33750 | time: 20.615s
| RMSProp | epoch: 077 | loss: 1.33750 - acc: 0.5104 | val_loss: 1.69699 - val_acc: 0.3962 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4368  | total loss: 1.30112 | time: 20.837s
| RMSProp | epoch: 078 | loss: 1.30112 - acc: 0.5426 | val_loss: 1.71145 - val_acc: 0.3908 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4424  | total loss: 1.32311 | time: 21.334s
| RMSProp | epoch: 079 | loss: 1.32311 - acc: 0.5343 | val_loss: 1.65384 - val_acc: 0.3935 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4480  | total loss: 1.32256 | time: 20.868s
| RMSProp | epoch: 080 | loss: 1.32256 - acc: 0.5373 | val_loss: 1.70048 - val_acc: 0.3989 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4536  | total loss: 1.27543 | time: 21.435s
| RMSProp | epoch: 081 | loss: 1.27543 - acc: 0.5556 | val_loss: 1.70111 - val_acc: 0.3989 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4592  | total loss: 1.32435 | time: 20.937s
| RMSProp | epoch: 082 | loss: 1.32435 - acc: 0.5312 | val_loss: 1.69298 - val_acc: 0.3801 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4648  | total loss: 1.24759 | time: 20.508s
| RMSProp | epoch: 083 | loss: 1.24759 - acc: 0.5628 | val_loss: 1.70549 - val_acc: 0.3881 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4704  | total loss: 1.32853 | time: 21.168s
| RMSProp | epoch: 084 | loss: 1.32853 - acc: 0.5325 | val_loss: 1.70526 - val_acc: 0.4016 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4760  | total loss: 1.27650 | time: 21.073s
| RMSProp | epoch: 085 | loss: 1.27650 - acc: 0.5515 | val_loss: 1.69784 - val_acc: 0.3989 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4816  | total loss: 1.31720 | time: 21.503s
| RMSProp | epoch: 086 | loss: 1.31720 - acc: 0.5439 | val_loss: 1.68504 - val_acc: 0.4124 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4872  | total loss: 1.32144 | time: 21.170s
| RMSProp | epoch: 087 | loss: 1.32144 - acc: 0.5343 | val_loss: 1.69791 - val_acc: 0.4151 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4928  | total loss: 1.36670 | time: 20.753s
| RMSProp | epoch: 088 | loss: 1.36670 - acc: 0.5172 | val_loss: 1.73308 - val_acc: 0.3801 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4984  | total loss: 1.32460 | time: 21.072s
| RMSProp | epoch: 089 | loss: 1.32460 - acc: 0.5289 | val_loss: 1.70576 - val_acc: 0.3989 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5040  | total loss: 1.19631 | time: 21.135s
| RMSProp | epoch: 090 | loss: 1.19631 - acc: 0.5931 | val_loss: 1.75295 - val_acc: 0.3881 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5096  | total loss: 1.26478 | time: 20.272s
| RMSProp | epoch: 091 | loss: 1.26478 - acc: 0.5603 | val_loss: 1.74740 - val_acc: 0.4016 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5152  | total loss: 1.28545 | time: 20.677s
| RMSProp | epoch: 092 | loss: 1.28545 - acc: 0.5398 | val_loss: 1.71901 - val_acc: 0.4259 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5208  | total loss: 1.20650 | time: 20.924s
| RMSProp | epoch: 093 | loss: 1.20650 - acc: 0.5887 | val_loss: 1.74535 - val_acc: 0.4097 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5264  | total loss: 1.23216 | time: 20.692s
| RMSProp | epoch: 094 | loss: 1.23216 - acc: 0.5784 | val_loss: 1.74729 - val_acc: 0.4124 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5320  | total loss: 1.23726 | time: 21.086s
| RMSProp | epoch: 095 | loss: 1.23726 - acc: 0.5665 | val_loss: 1.72649 - val_acc: 0.3989 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5376  | total loss: 1.25256 | time: 20.988s
| RMSProp | epoch: 096 | loss: 1.25256 - acc: 0.5634 | val_loss: 1.72409 - val_acc: 0.4178 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5432  | total loss: 1.16704 | time: 21.183s
| RMSProp | epoch: 097 | loss: 1.16704 - acc: 0.6065 | val_loss: 1.74739 - val_acc: 0.4232 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5488  | total loss: 1.24705 | time: 20.543s
| RMSProp | epoch: 098 | loss: 1.24705 - acc: 0.5713 | val_loss: 1.77846 - val_acc: 0.4043 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5544  | total loss: 1.18997 | time: 20.251s
| RMSProp | epoch: 099 | loss: 1.18997 - acc: 0.5874 | val_loss: 1.72793 - val_acc: 0.4232 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5600  | total loss: 1.21529 | time: 20.445s
| RMSProp | epoch: 100 | loss: 1.21529 - acc: 0.5756 | val_loss: 1.74833 - val_acc: 0.4151 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

