Training Step: 56  | total loss: 1.64419 | time: 56.944s
| Adam | epoch: 001 | loss: 1.64419 - acc: 0.3324 | val_loss: 1.61045 - val_acc: 0.3261 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 112  | total loss: 1.59425 | time: 12.799s
| Adam | epoch: 002 | loss: 1.59425 - acc: 0.3547 | val_loss: 1.61178 - val_acc: 0.3235 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 168  | total loss: 1.61036 | time: 12.815s
| Adam | epoch: 003 | loss: 1.61036 - acc: 0.3831 | val_loss: 1.59821 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 224  | total loss: 1.59479 | time: 12.801s
| Adam | epoch: 004 | loss: 1.59479 - acc: 0.3778 | val_loss: 1.59553 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 280  | total loss: 1.57685 | time: 12.816s
| Adam | epoch: 005 | loss: 1.57685 - acc: 0.3779 | val_loss: 1.59359 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 336  | total loss: 1.58494 | time: 12.823s
| Adam | epoch: 006 | loss: 1.58494 - acc: 0.3806 | val_loss: 1.60383 - val_acc: 0.3639 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 392  | total loss: 1.58289 | time: 12.851s
| Adam | epoch: 007 | loss: 1.58289 - acc: 0.4001 | val_loss: 1.60093 - val_acc: 0.3612 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 448  | total loss: 1.61661 | time: 12.778s
| Adam | epoch: 008 | loss: 1.61661 - acc: 0.3807 | val_loss: 1.60121 - val_acc: 0.3774 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 504  | total loss: 1.58441 | time: 12.835s
| Adam | epoch: 009 | loss: 1.58441 - acc: 0.3880 | val_loss: 1.59567 - val_acc: 0.3720 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 560  | total loss: 1.59265 | time: 12.803s
| Adam | epoch: 010 | loss: 1.59265 - acc: 0.3716 | val_loss: 1.60335 - val_acc: 0.3450 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 616  | total loss: 1.53649 | time: 12.823s
| Adam | epoch: 011 | loss: 1.53649 - acc: 0.4133 | val_loss: 1.60348 - val_acc: 0.3612 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 672  | total loss: 1.53026 | time: 12.857s
| Adam | epoch: 012 | loss: 1.53026 - acc: 0.4079 | val_loss: 1.59114 - val_acc: 0.3666 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 728  | total loss: 1.53617 | time: 12.766s
| Adam | epoch: 013 | loss: 1.53617 - acc: 0.3943 | val_loss: 1.58119 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 784  | total loss: 1.51260 | time: 12.834s
| Adam | epoch: 014 | loss: 1.51260 - acc: 0.4257 | val_loss: 1.52529 - val_acc: 0.4043 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 840  | total loss: 1.55494 | time: 12.802s
| Adam | epoch: 015 | loss: 1.55494 - acc: 0.3842 | val_loss: 1.58322 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 896  | total loss: 1.55253 | time: 12.826s
| Adam | epoch: 016 | loss: 1.55253 - acc: 0.3836 | val_loss: 1.56401 - val_acc: 0.3881 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 952  | total loss: 1.52889 | time: 12.827s
| Adam | epoch: 017 | loss: 1.52889 - acc: 0.4201 | val_loss: 1.57695 - val_acc: 0.3747 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1008  | total loss: 1.53848 | time: 12.826s
| Adam | epoch: 018 | loss: 1.53848 - acc: 0.4042 | val_loss: 1.57192 - val_acc: 0.3801 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1064  | total loss: 1.53185 | time: 12.845s
| Adam | epoch: 019 | loss: 1.53185 - acc: 0.4188 | val_loss: 1.53443 - val_acc: 0.4286 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1120  | total loss: 1.52261 | time: 12.867s
| Adam | epoch: 020 | loss: 1.52261 - acc: 0.3976 | val_loss: 1.54553 - val_acc: 0.3693 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1176  | total loss: 1.55715 | time: 12.831s
| Adam | epoch: 021 | loss: 1.55715 - acc: 0.4100 | val_loss: 1.57348 - val_acc: 0.4097 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1232  | total loss: 1.51162 | time: 12.850s
| Adam | epoch: 022 | loss: 1.51162 - acc: 0.4074 | val_loss: 1.58071 - val_acc: 0.3854 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1288  | total loss: 1.51425 | time: 12.773s
| Adam | epoch: 023 | loss: 1.51425 - acc: 0.3988 | val_loss: 1.51013 - val_acc: 0.4151 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1344  | total loss: 1.52982 | time: 12.808s
| Adam | epoch: 024 | loss: 1.52982 - acc: 0.3765 | val_loss: 1.59080 - val_acc: 0.3827 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1400  | total loss: 1.49892 | time: 12.794s
| Adam | epoch: 025 | loss: 1.49892 - acc: 0.4104 | val_loss: 1.60871 - val_acc: 0.3261 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1456  | total loss: 1.53397 | time: 12.803s
| Adam | epoch: 026 | loss: 1.53397 - acc: 0.4044 | val_loss: 1.57504 - val_acc: 0.3666 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1512  | total loss: 1.54866 | time: 13.001s
| Adam | epoch: 027 | loss: 1.54866 - acc: 0.3849 | val_loss: 1.63570 - val_acc: 0.3396 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1568  | total loss: 1.48756 | time: 13.168s
| Adam | epoch: 028 | loss: 1.48756 - acc: 0.4371 | val_loss: 1.52846 - val_acc: 0.4259 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1624  | total loss: 1.49765 | time: 12.831s
| Adam | epoch: 029 | loss: 1.49765 - acc: 0.4482 | val_loss: 1.54737 - val_acc: 0.4043 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1680  | total loss: 1.50329 | time: 12.813s
| Adam | epoch: 030 | loss: 1.50329 - acc: 0.4048 | val_loss: 1.54951 - val_acc: 0.4070 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1736  | total loss: 1.51597 | time: 12.754s
| Adam | epoch: 031 | loss: 1.51597 - acc: 0.4143 | val_loss: 1.53514 - val_acc: 0.4420 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1792  | total loss: 1.53770 | time: 12.866s
| Adam | epoch: 032 | loss: 1.53770 - acc: 0.4176 | val_loss: 1.51864 - val_acc: 0.4501 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1848  | total loss: 1.45474 | time: 12.813s
| Adam | epoch: 033 | loss: 1.45474 - acc: 0.4356 | val_loss: 1.54237 - val_acc: 0.4447 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1904  | total loss: 1.46906 | time: 12.806s
| Adam | epoch: 034 | loss: 1.46906 - acc: 0.4483 | val_loss: 1.52422 - val_acc: 0.3962 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 1960  | total loss: 1.43293 | time: 12.834s
| Adam | epoch: 035 | loss: 1.43293 - acc: 0.4729 | val_loss: 1.49054 - val_acc: 0.4447 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2016  | total loss: 1.42603 | time: 12.800s
| Adam | epoch: 036 | loss: 1.42603 - acc: 0.4627 | val_loss: 1.49273 - val_acc: 0.4340 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2072  | total loss: 1.42108 | time: 12.754s
| Adam | epoch: 037 | loss: 1.42108 - acc: 0.4794 | val_loss: 1.48435 - val_acc: 0.4582 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2128  | total loss: 1.40237 | time: 12.802s
| Adam | epoch: 038 | loss: 1.40237 - acc: 0.4922 | val_loss: 1.45492 - val_acc: 0.4636 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2184  | total loss: 1.41787 | time: 12.793s
| Adam | epoch: 039 | loss: 1.41787 - acc: 0.4788 | val_loss: 1.47300 - val_acc: 0.4259 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2240  | total loss: 1.35609 | time: 12.828s
| Adam | epoch: 040 | loss: 1.35609 - acc: 0.5044 | val_loss: 1.48538 - val_acc: 0.4555 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2296  | total loss: 1.37632 | time: 12.791s
| Adam | epoch: 041 | loss: 1.37632 - acc: 0.4901 | val_loss: 1.46977 - val_acc: 0.4582 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2352  | total loss: 1.34463 | time: 12.845s
| Adam | epoch: 042 | loss: 1.34463 - acc: 0.5069 | val_loss: 1.45700 - val_acc: 0.4555 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2408  | total loss: 1.38453 | time: 12.782s
| Adam | epoch: 043 | loss: 1.38453 - acc: 0.4792 | val_loss: 1.47625 - val_acc: 0.4447 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2464  | total loss: 1.40406 | time: 12.756s
| Adam | epoch: 044 | loss: 1.40406 - acc: 0.4651 | val_loss: 1.45838 - val_acc: 0.4528 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2520  | total loss: 1.34656 | time: 12.776s
| Adam | epoch: 045 | loss: 1.34656 - acc: 0.5071 | val_loss: 1.44471 - val_acc: 0.4528 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2576  | total loss: 1.38785 | time: 12.810s
| Adam | epoch: 046 | loss: 1.38785 - acc: 0.4775 | val_loss: 1.50143 - val_acc: 0.4582 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2632  | total loss: 1.34578 | time: 12.795s
| Adam | epoch: 047 | loss: 1.34578 - acc: 0.5029 | val_loss: 1.45076 - val_acc: 0.4609 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2688  | total loss: 1.35880 | time: 12.815s
| Adam | epoch: 048 | loss: 1.35880 - acc: 0.4816 | val_loss: 1.44471 - val_acc: 0.4555 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2744  | total loss: 1.32701 | time: 12.807s
| Adam | epoch: 049 | loss: 1.32701 - acc: 0.5052 | val_loss: 1.44806 - val_acc: 0.4555 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2800  | total loss: 1.38350 | time: 12.814s
| Adam | epoch: 050 | loss: 1.38350 - acc: 0.4737 | val_loss: 1.48985 - val_acc: 0.4501 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2856  | total loss: 1.37410 | time: 12.843s
| Adam | epoch: 051 | loss: 1.37410 - acc: 0.4912 | val_loss: 1.46493 - val_acc: 0.4501 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2912  | total loss: 1.35062 | time: 12.802s
| Adam | epoch: 052 | loss: 1.35062 - acc: 0.5084 | val_loss: 1.43659 - val_acc: 0.4717 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 2968  | total loss: 1.27766 | time: 12.767s
| Adam | epoch: 053 | loss: 1.27766 - acc: 0.5255 | val_loss: 1.50882 - val_acc: 0.4663 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3024  | total loss: 1.29330 | time: 12.827s
| Adam | epoch: 054 | loss: 1.29330 - acc: 0.5212 | val_loss: 1.44178 - val_acc: 0.4663 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3080  | total loss: 1.30863 | time: 12.797s
| Adam | epoch: 055 | loss: 1.30863 - acc: 0.5044 | val_loss: 1.46792 - val_acc: 0.4636 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3136  | total loss: 1.33276 | time: 12.849s
| Adam | epoch: 056 | loss: 1.33276 - acc: 0.4982 | val_loss: 1.46749 - val_acc: 0.4690 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3192  | total loss: 1.29546 | time: 12.803s
| Adam | epoch: 057 | loss: 1.29546 - acc: 0.5068 | val_loss: 1.47751 - val_acc: 0.4555 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3248  | total loss: 1.30070 | time: 12.808s
| Adam | epoch: 058 | loss: 1.30070 - acc: 0.5209 | val_loss: 1.48217 - val_acc: 0.4663 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3304  | total loss: 1.28210 | time: 12.812s
| Adam | epoch: 059 | loss: 1.28210 - acc: 0.5131 | val_loss: 1.49922 - val_acc: 0.4367 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3360  | total loss: 1.24299 | time: 12.790s
| Adam | epoch: 060 | loss: 1.24299 - acc: 0.5493 | val_loss: 1.48189 - val_acc: 0.4609 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3416  | total loss: 1.28063 | time: 12.752s
| Adam | epoch: 061 | loss: 1.28063 - acc: 0.5249 | val_loss: 1.48625 - val_acc: 0.4582 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3472  | total loss: 1.30515 | time: 12.821s
| Adam | epoch: 062 | loss: 1.30515 - acc: 0.4930 | val_loss: 1.47946 - val_acc: 0.4420 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3528  | total loss: 1.27333 | time: 12.820s
| Adam | epoch: 063 | loss: 1.27333 - acc: 0.5179 | val_loss: 1.47456 - val_acc: 0.4717 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3584  | total loss: 1.26534 | time: 12.799s
| Adam | epoch: 064 | loss: 1.26534 - acc: 0.5184 | val_loss: 1.48908 - val_acc: 0.4663 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3640  | total loss: 1.32612 | time: 12.831s
| Adam | epoch: 065 | loss: 1.32612 - acc: 0.4987 | val_loss: 1.53476 - val_acc: 0.4582 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3696  | total loss: 1.27267 | time: 12.771s
| Adam | epoch: 066 | loss: 1.27267 - acc: 0.5183 | val_loss: 1.51832 - val_acc: 0.4582 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3752  | total loss: 1.26631 | time: 12.753s
| Adam | epoch: 067 | loss: 1.26631 - acc: 0.5268 | val_loss: 1.51142 - val_acc: 0.4609 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3808  | total loss: 1.29280 | time: 12.823s
| Adam | epoch: 068 | loss: 1.29280 - acc: 0.5054 | val_loss: 1.51637 - val_acc: 0.4636 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3864  | total loss: 1.22682 | time: 12.813s
| Adam | epoch: 069 | loss: 1.22682 - acc: 0.5399 | val_loss: 1.47857 - val_acc: 0.4717 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3920  | total loss: 1.27335 | time: 12.784s
| Adam | epoch: 070 | loss: 1.27335 - acc: 0.5133 | val_loss: 1.50356 - val_acc: 0.4663 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 3976  | total loss: 1.22716 | time: 12.808s
| Adam | epoch: 071 | loss: 1.22716 - acc: 0.5352 | val_loss: 1.52021 - val_acc: 0.4690 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4032  | total loss: 1.25604 | time: 12.803s
| Adam | epoch: 072 | loss: 1.25604 - acc: 0.5213 | val_loss: 1.51849 - val_acc: 0.4609 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4088  | total loss: 1.25652 | time: 12.828s
| Adam | epoch: 073 | loss: 1.25652 - acc: 0.5297 | val_loss: 1.49973 - val_acc: 0.4636 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4144  | total loss: 1.24431 | time: 12.784s
| Adam | epoch: 074 | loss: 1.24431 - acc: 0.5347 | val_loss: 1.50650 - val_acc: 0.4744 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4200  | total loss: 1.26774 | time: 12.830s
| Adam | epoch: 075 | loss: 1.26774 - acc: 0.5371 | val_loss: 1.52210 - val_acc: 0.4609 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4256  | total loss: 1.22387 | time: 12.783s
| Adam | epoch: 076 | loss: 1.22387 - acc: 0.5369 | val_loss: 1.53218 - val_acc: 0.4555 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4312  | total loss: 1.20050 | time: 12.802s
| Adam | epoch: 077 | loss: 1.20050 - acc: 0.5593 | val_loss: 1.55734 - val_acc: 0.4609 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4368  | total loss: 1.22498 | time: 12.800s
| Adam | epoch: 078 | loss: 1.22498 - acc: 0.5415 | val_loss: 1.52185 - val_acc: 0.4447 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4424  | total loss: 1.22149 | time: 12.808s
| Adam | epoch: 079 | loss: 1.22149 - acc: 0.5428 | val_loss: 1.51197 - val_acc: 0.4663 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4480  | total loss: 1.34383 | time: 12.782s
| Adam | epoch: 080 | loss: 1.34383 - acc: 0.4919 | val_loss: 1.54983 - val_acc: 0.4690 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4536  | total loss: 1.22981 | time: 12.761s
| Adam | epoch: 081 | loss: 1.22981 - acc: 0.5448 | val_loss: 1.52416 - val_acc: 0.4555 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4592  | total loss: 1.28453 | time: 12.807s
| Adam | epoch: 082 | loss: 1.28453 - acc: 0.5208 | val_loss: 1.52013 - val_acc: 0.4609 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4648  | total loss: 1.20438 | time: 12.788s
| Adam | epoch: 083 | loss: 1.20438 - acc: 0.5538 | val_loss: 1.52315 - val_acc: 0.4582 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4704  | total loss: 1.24852 | time: 12.782s
| Adam | epoch: 084 | loss: 1.24852 - acc: 0.5388 | val_loss: 1.55139 - val_acc: 0.4717 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4760  | total loss: 1.24367 | time: 12.794s
| Adam | epoch: 085 | loss: 1.24367 - acc: 0.5385 | val_loss: 1.53816 - val_acc: 0.4501 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4816  | total loss: 1.19137 | time: 12.798s
| Adam | epoch: 086 | loss: 1.19137 - acc: 0.5412 | val_loss: 1.53308 - val_acc: 0.4501 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4872  | total loss: 1.23928 | time: 12.766s
| Adam | epoch: 087 | loss: 1.23928 - acc: 0.5388 | val_loss: 1.49462 - val_acc: 0.4555 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4928  | total loss: 1.27423 | time: 12.816s
| Adam | epoch: 088 | loss: 1.27423 - acc: 0.5421 | val_loss: 1.51348 - val_acc: 0.4447 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 4984  | total loss: 1.27869 | time: 12.838s
| Adam | epoch: 089 | loss: 1.27869 - acc: 0.5410 | val_loss: 1.51042 - val_acc: 0.4609 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5040  | total loss: 1.19716 | time: 12.820s
| Adam | epoch: 090 | loss: 1.19716 - acc: 0.5520 | val_loss: 1.52057 - val_acc: 0.4744 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5096  | total loss: 1.17169 | time: 12.805s
| Adam | epoch: 091 | loss: 1.17169 - acc: 0.5438 | val_loss: 1.53074 - val_acc: 0.4798 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5152  | total loss: 1.11835 | time: 12.835s
| Adam | epoch: 092 | loss: 1.11835 - acc: 0.5857 | val_loss: 1.56286 - val_acc: 0.4825 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5208  | total loss: 1.13002 | time: 12.770s
| Adam | epoch: 093 | loss: 1.13002 - acc: 0.5625 | val_loss: 1.58816 - val_acc: 0.4636 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5264  | total loss: 1.13483 | time: 12.794s
| Adam | epoch: 094 | loss: 1.13483 - acc: 0.5804 | val_loss: 1.52562 - val_acc: 0.4717 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5320  | total loss: 1.14199 | time: 12.790s
| Adam | epoch: 095 | loss: 1.14199 - acc: 0.5633 | val_loss: 1.57419 - val_acc: 0.4906 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5376  | total loss: 1.16988 | time: 12.788s
| Adam | epoch: 096 | loss: 1.16988 - acc: 0.5417 | val_loss: 1.60178 - val_acc: 0.4717 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5432  | total loss: 1.13116 | time: 12.832s
| Adam | epoch: 097 | loss: 1.13116 - acc: 0.5630 | val_loss: 1.54043 - val_acc: 0.4717 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5488  | total loss: 1.10204 | time: 12.801s
| Adam | epoch: 098 | loss: 1.10204 - acc: 0.5748 | val_loss: 1.61472 - val_acc: 0.4852 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5544  | total loss: 1.07354 | time: 12.771s
| Adam | epoch: 099 | loss: 1.07354 - acc: 0.5818 | val_loss: 1.55754 - val_acc: 0.4798 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

Training Step: 5600  | total loss: 1.11947 | time: 12.747s
| Adam | epoch: 100 | loss: 1.11947 - acc: 0.5666 | val_loss: 1.56289 - val_acc: 0.4825 -- iter: 3341/3341 

 -------------------------------------------------------------------------------- 

