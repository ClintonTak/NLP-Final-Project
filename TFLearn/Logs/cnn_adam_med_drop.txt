Training Step: 108  | total loss: 1.59053 | time: 5.096s
| Adam | epoch: 001 | loss: 1.59053 - acc: 0.3437 | val_loss: 1.58657 - val_acc: 0.3596 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 216  | total loss: 1.59445 | time: 3.991s
| Adam | epoch: 002 | loss: 1.59445 - acc: 0.3939 | val_loss: 1.56445 - val_acc: 0.3596 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 324  | total loss: 1.52295 | time: 3.914s
| Adam | epoch: 003 | loss: 1.52295 - acc: 0.4013 | val_loss: 1.49461 - val_acc: 0.4593 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 432  | total loss: 1.47986 | time: 3.878s
| Adam | epoch: 004 | loss: 1.47986 - acc: 0.4133 | val_loss: 1.43102 - val_acc: 0.4856 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 540  | total loss: 1.36285 | time: 3.899s
| Adam | epoch: 005 | loss: 1.36285 - acc: 0.4876 | val_loss: 1.35342 - val_acc: 0.5171 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 648  | total loss: 1.29006 | time: 4.106s
| Adam | epoch: 006 | loss: 1.29006 - acc: 0.5189 | val_loss: 1.27309 - val_acc: 0.5669 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 756  | total loss: 1.20888 | time: 3.948s
| Adam | epoch: 007 | loss: 1.20888 - acc: 0.5729 | val_loss: 1.22311 - val_acc: 0.5696 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 864  | total loss: 1.20702 | time: 4.102s
| Adam | epoch: 008 | loss: 1.20702 - acc: 0.5730 | val_loss: 1.19633 - val_acc: 0.5879 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 972  | total loss: 1.09037 | time: 4.165s
| Adam | epoch: 009 | loss: 1.09037 - acc: 0.6286 | val_loss: 1.16837 - val_acc: 0.5801 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1080  | total loss: 1.08565 | time: 4.122s
| Adam | epoch: 010 | loss: 1.08565 - acc: 0.6005 | val_loss: 1.15089 - val_acc: 0.5984 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1188  | total loss: 1.02163 | time: 4.033s
| Adam | epoch: 011 | loss: 1.02163 - acc: 0.6345 | val_loss: 1.17005 - val_acc: 0.5958 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1296  | total loss: 1.00762 | time: 3.979s
| Adam | epoch: 012 | loss: 1.00762 - acc: 0.6243 | val_loss: 1.11721 - val_acc: 0.6220 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1404  | total loss: 0.96912 | time: 4.038s
| Adam | epoch: 013 | loss: 0.96912 - acc: 0.6617 | val_loss: 1.13221 - val_acc: 0.6273 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1512  | total loss: 0.92915 | time: 4.018s
| Adam | epoch: 014 | loss: 0.92915 - acc: 0.6596 | val_loss: 1.10580 - val_acc: 0.6037 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1620  | total loss: 0.90933 | time: 3.962s
| Adam | epoch: 015 | loss: 0.90933 - acc: 0.6815 | val_loss: 1.13594 - val_acc: 0.6299 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1728  | total loss: 0.78944 | time: 3.915s
| Adam | epoch: 016 | loss: 0.78944 - acc: 0.7327 | val_loss: 1.07991 - val_acc: 0.6299 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1836  | total loss: 0.92009 | time: 3.914s
| Adam | epoch: 017 | loss: 0.92009 - acc: 0.6632 | val_loss: 1.10142 - val_acc: 0.6352 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 1944  | total loss: 0.83805 | time: 3.888s
| Adam | epoch: 018 | loss: 0.83805 - acc: 0.7332 | val_loss: 1.08152 - val_acc: 0.6220 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2052  | total loss: 0.80318 | time: 3.925s
| Adam | epoch: 019 | loss: 0.80318 - acc: 0.7253 | val_loss: 1.10319 - val_acc: 0.6299 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2160  | total loss: 0.85046 | time: 3.900s
| Adam | epoch: 020 | loss: 0.85046 - acc: 0.6933 | val_loss: 1.12484 - val_acc: 0.6168 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2268  | total loss: 0.72249 | time: 3.916s
| Adam | epoch: 021 | loss: 0.72249 - acc: 0.7420 | val_loss: 1.08883 - val_acc: 0.6299 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2376  | total loss: 0.72151 | time: 3.892s
| Adam | epoch: 022 | loss: 0.72151 - acc: 0.7454 | val_loss: 1.07487 - val_acc: 0.6483 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2484  | total loss: 0.75000 | time: 3.912s
| Adam | epoch: 023 | loss: 0.75000 - acc: 0.7402 | val_loss: 1.08482 - val_acc: 0.6325 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2592  | total loss: 0.71787 | time: 3.884s
| Adam | epoch: 024 | loss: 0.71787 - acc: 0.7613 | val_loss: 1.08879 - val_acc: 0.6378 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2700  | total loss: 0.68414 | time: 3.908s
| Adam | epoch: 025 | loss: 0.68414 - acc: 0.7655 | val_loss: 1.08496 - val_acc: 0.6325 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2808  | total loss: 0.66089 | time: 3.893s
| Adam | epoch: 026 | loss: 0.66089 - acc: 0.7672 | val_loss: 1.11434 - val_acc: 0.6247 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 2916  | total loss: 0.63215 | time: 3.889s
| Adam | epoch: 027 | loss: 0.63215 - acc: 0.7717 | val_loss: 1.09078 - val_acc: 0.6430 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3024  | total loss: 0.63301 | time: 3.904s
| Adam | epoch: 028 | loss: 0.63301 - acc: 0.7791 | val_loss: 1.13856 - val_acc: 0.6457 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3132  | total loss: 0.94862 | time: 3.891s
| Adam | epoch: 029 | loss: 0.94862 - acc: 0.7444 | val_loss: 1.11197 - val_acc: 0.6378 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3240  | total loss: 0.63409 | time: 4.044s
| Adam | epoch: 030 | loss: 0.63409 - acc: 0.7962 | val_loss: 1.13534 - val_acc: 0.6247 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3348  | total loss: 0.65422 | time: 4.174s
| Adam | epoch: 031 | loss: 0.65422 - acc: 0.7645 | val_loss: 1.09435 - val_acc: 0.6404 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3456  | total loss: 0.60043 | time: 3.993s
| Adam | epoch: 032 | loss: 0.60043 - acc: 0.7765 | val_loss: 1.10457 - val_acc: 0.6430 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3564  | total loss: 0.58759 | time: 4.014s
| Adam | epoch: 033 | loss: 0.58759 - acc: 0.7985 | val_loss: 1.11462 - val_acc: 0.6378 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3672  | total loss: 0.53467 | time: 4.011s
| Adam | epoch: 034 | loss: 0.53467 - acc: 0.8120 | val_loss: 1.13822 - val_acc: 0.6325 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3780  | total loss: 0.57958 | time: 3.940s
| Adam | epoch: 035 | loss: 0.57958 - acc: 0.8079 | val_loss: 1.17535 - val_acc: 0.6194 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3888  | total loss: 0.53631 | time: 3.905s
| Adam | epoch: 036 | loss: 0.53631 - acc: 0.8178 | val_loss: 1.18772 - val_acc: 0.6404 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 3996  | total loss: 0.54516 | time: 3.976s
| Adam | epoch: 037 | loss: 0.54516 - acc: 0.8169 | val_loss: 1.15088 - val_acc: 0.6299 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4104  | total loss: 0.55585 | time: 3.906s
| Adam | epoch: 038 | loss: 0.55585 - acc: 0.8022 | val_loss: 1.18965 - val_acc: 0.6194 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4212  | total loss: 0.51687 | time: 3.873s
| Adam | epoch: 039 | loss: 0.51687 - acc: 0.8299 | val_loss: 1.15978 - val_acc: 0.6325 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4320  | total loss: 0.49893 | time: 3.922s
| Adam | epoch: 040 | loss: 0.49893 - acc: 0.8275 | val_loss: 1.15592 - val_acc: 0.6404 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4428  | total loss: 0.49617 | time: 3.926s
| Adam | epoch: 041 | loss: 0.49617 - acc: 0.8254 | val_loss: 1.22280 - val_acc: 0.6220 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4536  | total loss: 0.48351 | time: 3.910s
| Adam | epoch: 042 | loss: 0.48351 - acc: 0.8396 | val_loss: 1.18202 - val_acc: 0.6378 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4644  | total loss: 0.46048 | time: 3.924s
| Adam | epoch: 043 | loss: 0.46048 - acc: 0.8557 | val_loss: 1.19225 - val_acc: 0.6325 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4752  | total loss: 0.45804 | time: 3.903s
| Adam | epoch: 044 | loss: 0.45804 - acc: 0.8730 | val_loss: 1.19761 - val_acc: 0.6247 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4860  | total loss: 0.50494 | time: 3.894s
| Adam | epoch: 045 | loss: 0.50494 - acc: 0.8428 | val_loss: 1.24965 - val_acc: 0.6220 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 4968  | total loss: 0.44234 | time: 3.924s
| Adam | epoch: 046 | loss: 0.44234 - acc: 0.8417 | val_loss: 1.21332 - val_acc: 0.6299 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5076  | total loss: 0.45853 | time: 3.900s
| Adam | epoch: 047 | loss: 0.45853 - acc: 0.8571 | val_loss: 1.28088 - val_acc: 0.6247 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5184  | total loss: 0.45706 | time: 3.937s
| Adam | epoch: 048 | loss: 0.45706 - acc: 0.8476 | val_loss: 1.26033 - val_acc: 0.6325 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5292  | total loss: 0.40123 | time: 3.918s
| Adam | epoch: 049 | loss: 0.40123 - acc: 0.8838 | val_loss: 1.29319 - val_acc: 0.6247 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5400  | total loss: 0.43093 | time: 3.906s
| Adam | epoch: 050 | loss: 0.43093 - acc: 0.8651 | val_loss: 1.29172 - val_acc: 0.6299 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5508  | total loss: 0.39442 | time: 3.894s
| Adam | epoch: 051 | loss: 0.39442 - acc: 0.8642 | val_loss: 1.30105 - val_acc: 0.6194 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5616  | total loss: 0.41094 | time: 3.970s
| Adam | epoch: 052 | loss: 0.41094 - acc: 0.8756 | val_loss: 1.29843 - val_acc: 0.6299 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5724  | total loss: 0.38113 | time: 3.915s
| Adam | epoch: 053 | loss: 0.38113 - acc: 0.8590 | val_loss: 1.34101 - val_acc: 0.6194 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5832  | total loss: 0.36966 | time: 3.888s
| Adam | epoch: 054 | loss: 0.36966 - acc: 0.8830 | val_loss: 1.32802 - val_acc: 0.6220 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 5940  | total loss: 0.42642 | time: 3.980s
| Adam | epoch: 055 | loss: 0.42642 - acc: 0.8515 | val_loss: 1.32921 - val_acc: 0.6142 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6048  | total loss: 0.38385 | time: 4.100s
| Adam | epoch: 056 | loss: 0.38385 - acc: 0.8656 | val_loss: 1.34744 - val_acc: 0.5984 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6156  | total loss: 0.38287 | time: 3.917s
| Adam | epoch: 057 | loss: 0.38287 - acc: 0.8670 | val_loss: 1.36120 - val_acc: 0.6089 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6264  | total loss: 0.37667 | time: 3.939s
| Adam | epoch: 058 | loss: 0.37667 - acc: 0.8682 | val_loss: 1.36814 - val_acc: 0.6273 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6372  | total loss: 0.35179 | time: 3.921s
| Adam | epoch: 059 | loss: 0.35179 - acc: 0.8854 | val_loss: 1.31111 - val_acc: 0.6325 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6480  | total loss: 0.34031 | time: 3.912s
| Adam | epoch: 060 | loss: 0.34031 - acc: 0.8888 | val_loss: 1.33126 - val_acc: 0.6430 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6588  | total loss: 0.36290 | time: 3.903s
| Adam | epoch: 061 | loss: 0.36290 - acc: 0.8621 | val_loss: 1.35616 - val_acc: 0.6273 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6696  | total loss: 0.42108 | time: 4.014s
| Adam | epoch: 062 | loss: 0.42108 - acc: 0.8640 | val_loss: 1.36555 - val_acc: 0.6194 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6804  | total loss: 0.38770 | time: 4.120s
| Adam | epoch: 063 | loss: 0.38770 - acc: 0.8670 | val_loss: 1.32127 - val_acc: 0.6378 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 6912  | total loss: 0.30852 | time: 4.184s
| Adam | epoch: 064 | loss: 0.30852 - acc: 0.9029 | val_loss: 1.34171 - val_acc: 0.6378 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7020  | total loss: 0.30742 | time: 4.035s
| Adam | epoch: 065 | loss: 0.30742 - acc: 0.8894 | val_loss: 1.40045 - val_acc: 0.6142 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7128  | total loss: 0.33980 | time: 4.020s
| Adam | epoch: 066 | loss: 0.33980 - acc: 0.8778 | val_loss: 1.39997 - val_acc: 0.6037 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7236  | total loss: 0.32801 | time: 3.886s
| Adam | epoch: 067 | loss: 0.32801 - acc: 0.8937 | val_loss: 1.36519 - val_acc: 0.6325 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7344  | total loss: 0.33961 | time: 4.051s
| Adam | epoch: 068 | loss: 0.33961 - acc: 0.8879 | val_loss: 1.41872 - val_acc: 0.6168 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7452  | total loss: 0.29656 | time: 4.130s
| Adam | epoch: 069 | loss: 0.29656 - acc: 0.9001 | val_loss: 1.41473 - val_acc: 0.6220 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7560  | total loss: 0.32509 | time: 3.959s
| Adam | epoch: 070 | loss: 0.32509 - acc: 0.8920 | val_loss: 1.44212 - val_acc: 0.6220 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7668  | total loss: 0.28943 | time: 3.899s
| Adam | epoch: 071 | loss: 0.28943 - acc: 0.9127 | val_loss: 1.43671 - val_acc: 0.6089 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7776  | total loss: 0.32228 | time: 3.917s
| Adam | epoch: 072 | loss: 0.32228 - acc: 0.8957 | val_loss: 1.43062 - val_acc: 0.6325 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7884  | total loss: 0.30034 | time: 3.927s
| Adam | epoch: 073 | loss: 0.30034 - acc: 0.9041 | val_loss: 1.42297 - val_acc: 0.6352 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 7992  | total loss: 0.27208 | time: 3.879s
| Adam | epoch: 074 | loss: 0.27208 - acc: 0.9066 | val_loss: 1.45222 - val_acc: 0.6378 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8100  | total loss: 0.30491 | time: 3.899s
| Adam | epoch: 075 | loss: 0.30491 - acc: 0.9080 | val_loss: 1.46922 - val_acc: 0.6273 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8208  | total loss: 0.26269 | time: 3.891s
| Adam | epoch: 076 | loss: 0.26269 - acc: 0.9171 | val_loss: 1.41981 - val_acc: 0.6352 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8316  | total loss: 0.27989 | time: 3.915s
| Adam | epoch: 077 | loss: 0.27989 - acc: 0.9124 | val_loss: 1.44833 - val_acc: 0.6220 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8424  | total loss: 0.30261 | time: 3.958s
| Adam | epoch: 078 | loss: 0.30261 - acc: 0.8895 | val_loss: 1.47160 - val_acc: 0.6168 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8532  | total loss: 0.24644 | time: 3.911s
| Adam | epoch: 079 | loss: 0.24644 - acc: 0.9225 | val_loss: 1.45539 - val_acc: 0.6220 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8640  | total loss: 0.28448 | time: 3.933s
| Adam | epoch: 080 | loss: 0.28448 - acc: 0.9132 | val_loss: 1.46689 - val_acc: 0.6142 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8748  | total loss: 0.22833 | time: 3.925s
| Adam | epoch: 081 | loss: 0.22833 - acc: 0.9334 | val_loss: 1.54869 - val_acc: 0.6168 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8856  | total loss: 0.24647 | time: 3.916s
| Adam | epoch: 082 | loss: 0.24647 - acc: 0.9129 | val_loss: 1.48623 - val_acc: 0.6299 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 8964  | total loss: 0.28267 | time: 3.897s
| Adam | epoch: 083 | loss: 0.28267 - acc: 0.9062 | val_loss: 1.57143 - val_acc: 0.6037 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9072  | total loss: 0.28323 | time: 3.934s
| Adam | epoch: 084 | loss: 0.28323 - acc: 0.8947 | val_loss: 1.54223 - val_acc: 0.6142 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9180  | total loss: 0.20603 | time: 3.912s
| Adam | epoch: 085 | loss: 0.20603 - acc: 0.9403 | val_loss: 1.60437 - val_acc: 0.6063 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9288  | total loss: 0.26791 | time: 3.916s
| Adam | epoch: 086 | loss: 0.26791 - acc: 0.9178 | val_loss: 1.52527 - val_acc: 0.6247 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9396  | total loss: 0.25461 | time: 3.924s
| Adam | epoch: 087 | loss: 0.25461 - acc: 0.9194 | val_loss: 1.51514 - val_acc: 0.6220 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9504  | total loss: 0.24466 | time: 3.919s
| Adam | epoch: 088 | loss: 0.24466 - acc: 0.9128 | val_loss: 1.58560 - val_acc: 0.5958 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9612  | total loss: 0.26065 | time: 3.898s
| Adam | epoch: 089 | loss: 0.26065 - acc: 0.9215 | val_loss: 1.58025 - val_acc: 0.6168 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9720  | total loss: 0.19425 | time: 4.191s
| Adam | epoch: 090 | loss: 0.19425 - acc: 0.9407 | val_loss: 1.60476 - val_acc: 0.6142 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9828  | total loss: 0.24957 | time: 4.113s
| Adam | epoch: 091 | loss: 0.24957 - acc: 0.9116 | val_loss: 1.61469 - val_acc: 0.6168 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 9936  | total loss: 0.25811 | time: 3.932s
| Adam | epoch: 092 | loss: 0.25811 - acc: 0.9123 | val_loss: 1.59901 - val_acc: 0.6010 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10044  | total loss: 0.22741 | time: 3.994s
| Adam | epoch: 093 | loss: 0.22741 - acc: 0.9246 | val_loss: 1.64607 - val_acc: 0.6010 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10152  | total loss: 0.20110 | time: 4.008s
| Adam | epoch: 094 | loss: 0.20110 - acc: 0.9321 | val_loss: 1.67142 - val_acc: 0.6168 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10260  | total loss: 0.25291 | time: 4.067s
| Adam | epoch: 095 | loss: 0.25291 - acc: 0.9193 | val_loss: 1.64368 - val_acc: 0.6037 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10368  | total loss: 0.20323 | time: 4.130s
| Adam | epoch: 096 | loss: 0.20323 - acc: 0.9376 | val_loss: 1.70091 - val_acc: 0.6142 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10476  | total loss: 0.17905 | time: 4.046s
| Adam | epoch: 097 | loss: 0.17905 - acc: 0.9458 | val_loss: 1.63251 - val_acc: 0.6194 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10584  | total loss: 0.21711 | time: 3.873s
| Adam | epoch: 098 | loss: 0.21711 - acc: 0.9257 | val_loss: 1.63015 - val_acc: 0.6220 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10692  | total loss: 0.20027 | time: 4.055s
| Adam | epoch: 099 | loss: 0.20027 - acc: 0.9340 | val_loss: 1.70168 - val_acc: 0.6220 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

Training Step: 10800  | total loss: 0.23859 | time: 3.947s
| Adam | epoch: 100 | loss: 0.23859 - acc: 0.9152 | val_loss: 1.62378 - val_acc: 0.6168 -- iter: 3431/3431 

 -------------------------------------------------------------------------------- 

